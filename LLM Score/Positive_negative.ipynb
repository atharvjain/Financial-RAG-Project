{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809c5731-e1c5-4960-86d4-864d29797af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:29:39,552 - INFO - === Probability-Based Positive/Negative Allocation ===\n",
      "2025-08-25 20:29:39,553 - INFO - Input file: /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-25 20:29:39,553 - INFO - Output file: /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\n",
      "2025-08-25 20:29:39,554 - INFO - Configuration: {'num_neg_samples': 15, 'min_candidates': 2, 'adaptive_neg_samples': True, 'probability_threshold': 0.0, 'processing_mode': 'single_file'}\n",
      "2025-08-25 20:29:39,555 - INFO - \n",
      "=== Analyzing Input Data ===\n",
      "2025-08-25 20:29:39,555 - INFO - === Analyzing Probability Distribution ===\n",
      "2025-08-25 20:29:39,572 - INFO - Loading /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-25 20:29:40,058 - INFO - Loaded 13210 records from /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-25 20:29:40,077 - INFO - Overall probability statistics:\n",
      "2025-08-25 20:29:40,078 - INFO -   - Total candidates analyzed: 132100\n",
      "2025-08-25 20:29:40,087 - INFO -   - Mean probability: 0.3358\n",
      "2025-08-25 20:29:40,097 - INFO -   - Std deviation: 0.3368\n",
      "2025-08-25 20:29:40,101 - INFO -   - Min probability: 0.0100\n",
      "2025-08-25 20:29:40,105 - INFO -   - Max probability: 0.9761\n",
      "2025-08-25 20:29:40,116 - INFO -   - 25th percentile: 0.0100\n",
      "2025-08-25 20:29:40,127 - INFO -   - 50th percentile (median): 0.0100\n",
      "2025-08-25 20:29:40,138 - INFO -   - 75th percentile: 0.6503\n",
      "2025-08-25 20:29:40,139 - INFO - Per-query statistics:\n",
      "2025-08-25 20:29:40,143 - INFO -   - Average max probability per query: 0.4816\n",
      "2025-08-25 20:29:40,145 - INFO -   - Average min probability per query: 0.1977\n",
      "2025-08-25 20:29:40,148 - INFO -   - Average separation per query: 0.2839\n",
      "2025-08-25 20:29:40,189 - INFO - \n",
      "=== Processing Probability-Based Data ===\n",
      "2025-08-25 20:29:40,190 - INFO - === Processing Probability-Based LLM Scored Dataset ===\n",
      "2025-08-25 20:29:40,190 - INFO - Input: /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-25 20:29:40,191 - INFO - Output: /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\n",
      "2025-08-25 20:29:40,192 - INFO - Target negative samples per query: 15\n",
      "2025-08-25 20:29:40,192 - INFO - Minimum candidates required: 2\n",
      "2025-08-25 20:29:40,193 - INFO - Probability threshold: 0.0\n",
      "2025-08-25 20:29:40,195 - INFO - Loading /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-25 20:29:40,527 - INFO - Loaded 13210 records from /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-25 20:29:40,528 - INFO - Loaded 13210 sequences from input file\n",
      "2025-08-25 20:29:40,528 - INFO - Data format validation passed\n",
      "2025-08-25 20:29:40,529 - INFO - Processing 13210 sequences with probability-based allocation\n",
      "2025-08-25 20:29:41,086 - INFO - Candidate statistics:\n",
      "2025-08-25 20:29:41,086 - INFO -   - Average candidates per sequence: 10.0\n",
      "2025-08-25 20:29:41,087 - INFO -   - Min candidates found: 10\n",
      "2025-08-25 20:29:41,087 - INFO -   - Max candidates found: 10\n",
      "2025-08-25 20:29:41,088 - INFO -   - Sequences with < 16 candidates: 13210\n",
      "2025-08-25 20:29:41,088 - INFO - Probability statistics:\n",
      "2025-08-25 20:29:41,096 - INFO -   - Average probability: 0.3358\n",
      "2025-08-25 20:29:41,099 - INFO -   - Min probability: 0.0100\n",
      "2025-08-25 20:29:41,101 - INFO -   - Max probability: 0.9761\n",
      "2025-08-25 20:29:41,107 - INFO -   - Std deviation: 0.3368\n",
      "2025-08-25 20:29:41,112 - INFO - Pos/Neg separation statistics:\n",
      "2025-08-25 20:29:41,113 - INFO -   - Average positive probability: 0.4816\n",
      "2025-08-25 20:29:41,114 - INFO -   - Average negative probability: 0.3197\n",
      "2025-08-25 20:29:41,115 - INFO -   - Average separation (pos - neg): 0.1619\n",
      "2025-08-25 20:29:41,115 - INFO -   - Min separation: 0.0000\n",
      "2025-08-25 20:29:41,116 - INFO -   - Max separation: 0.7409\n",
      "2025-08-25 20:29:41,116 - INFO - Successfully processed: 13210 sequences\n",
      "2025-08-25 20:29:41,116 - INFO - Skipped: 0 sequences\n",
      "2025-08-25 20:29:41,723 - INFO - Saved 13210 sequences to /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\n",
      "2025-08-25 20:29:41,725 - INFO - Processing complete!\n",
      "2025-08-25 20:29:41,725 - INFO - Statistics saved to: /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg_processing_stats.json\n",
      "2025-08-25 20:29:41,792 - INFO - \n",
      "Processing complete! Total sequences processed: 13210\n",
      "2025-08-25 20:29:41,793 - INFO - \n",
      "Output format:\n",
      "2025-08-25 20:29:41,793 - INFO -   - query: Query string\n",
      "2025-08-25 20:29:41,794 - INFO -   - pos: [highest_probability_candidate]\n",
      "2025-08-25 20:29:41,794 - INFO -   - neg: [lowest_probability_candidates...]\n",
      "2025-08-25 20:29:41,794 - INFO -   - teacher_scores: [pos_prob, neg_prob1, neg_prob2, ...]\n",
      "2025-08-25 20:29:41,795 - INFO - \n",
      "Ready for FinSeer retriever training!\n",
      "2025-08-25 20:29:41,795 - INFO - \n",
      "Files generated:\n",
      "2025-08-25 20:29:41,796 - INFO -   - Training data: /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\n",
      "2025-08-25 20:29:41,796 - INFO -   - Statistics: /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg_processing_stats.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_json_files_from_folder(folder_path):\n",
    "    \"\"\"Load all JSON lines files from a folder\"\"\"\n",
    "    json_data = []\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        logger.error(\"Error: Folder path does not exist.\")\n",
    "        return json_data\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "    logger.info(f\"Found {len(files)} files in {folder_path}\")\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            logger.info(f\"Loading {file_name}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    for line_num, line in enumerate(f, 1):\n",
    "                        try:\n",
    "                            json_data.append(json.loads(line.strip()))\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            logger.warning(f\"Skipping invalid JSON at line {line_num} in {file_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(json_data)} total records\")\n",
    "    return json_data\n",
    "\n",
    "def load_single_json_file(file_path):\n",
    "    \"\"\"Load a single JSON lines file\"\"\"\n",
    "    json_data = []\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"Error: File {file_path} does not exist.\")\n",
    "        return json_data\n",
    "\n",
    "    logger.info(f\"Loading {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    json_data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Skipping invalid JSON at line {line_num}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {file_path}: {e}\")\n",
    "        return json_data\n",
    "\n",
    "    logger.info(f\"Loaded {len(json_data)} records from {file_path}\")\n",
    "    return json_data\n",
    "\n",
    "def allocate_pos_and_neg_from_probabilities(all_data, num_neg_samples=15, min_candidates=2, probability_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Allocate positive and negative candidates based on probability scores\n",
    "    \n",
    "    Args:\n",
    "        all_data: List of sequences with candidate probabilities\n",
    "        num_neg_samples: Target number of negative samples per query\n",
    "        min_candidates: Minimum candidates required per query\n",
    "        probability_threshold: Threshold for considering candidates (optional filtering)\n",
    "    \"\"\"\n",
    "    all_json_list = []\n",
    "    skipped_sequences = 0\n",
    "    candidate_stats = []\n",
    "    probability_stats = []\n",
    "\n",
    "    logger.info(f\"Processing {len(all_data)} sequences with probability-based allocation\")\n",
    "\n",
    "    for i, sequence in enumerate(all_data):\n",
    "        try:\n",
    "            # Extract required fields from NEW format\n",
    "            candidate_list = sequence.get(\"candidates\", [])\n",
    "            candidate_index_list = sequence.get(\"candidate_indices\", [])\n",
    "            candidate_probabilities = sequence.get(\"candidate_probabilities\", [])\n",
    "\n",
    "            # Validate data integrity\n",
    "            if not candidate_list or not candidate_probabilities:\n",
    "                logger.warning(f\"Skipping sequence {i}: Missing candidates or probabilities\")\n",
    "                skipped_sequences += 1\n",
    "                continue\n",
    "\n",
    "            if len(candidate_list) != len(candidate_probabilities):\n",
    "                logger.warning(f\"Skipping sequence {i}: Mismatch in candidates ({len(candidate_list)}) and probabilities ({len(candidate_probabilities)})\")\n",
    "                skipped_sequences += 1\n",
    "                continue\n",
    "\n",
    "            if candidate_index_list and len(candidate_index_list) != len(candidate_probabilities):\n",
    "                logger.warning(f\"Skipping sequence {i}: Mismatch in indices ({len(candidate_index_list)}) and probabilities ({len(candidate_probabilities)})\")\n",
    "                skipped_sequences += 1\n",
    "                continue\n",
    "\n",
    "            num_candidates = len(candidate_probabilities)\n",
    "            candidate_stats.append(num_candidates)\n",
    "            probability_stats.extend(candidate_probabilities)\n",
    "\n",
    "            # Skip if we don't have minimum candidates (need at least 1 pos + 1 neg)\n",
    "            if num_candidates < min_candidates:\n",
    "                logger.warning(f\"Skipping sequence {i}: Not enough candidates ({num_candidates}) for minimum requirement ({min_candidates})\")\n",
    "                skipped_sequences += 1\n",
    "                continue\n",
    "\n",
    "            # Convert probabilities to numpy array for easier processing\n",
    "            probs_array = np.array(candidate_probabilities)\n",
    "            \n",
    "            # Sort probabilities in descending order (highest first) and get original indices\n",
    "            sorted_indices = np.argsort(probs_array)[::-1]\n",
    "            sorted_probabilities = probs_array[sorted_indices]\n",
    "\n",
    "            # Adaptively adjust number of negative samples based on available candidates\n",
    "            # Reserve 1 for positive, rest can be negative\n",
    "            max_possible_neg = num_candidates - 1\n",
    "            actual_neg_samples = min(num_neg_samples, max_possible_neg)\n",
    "\n",
    "            if actual_neg_samples != num_neg_samples:\n",
    "                logger.debug(f\"Sequence {i}: Adjusting neg samples from {num_neg_samples} to {actual_neg_samples} (only {num_candidates} candidates available)\")\n",
    "\n",
    "            # Get the highest scoring candidate (positive example)\n",
    "            pos_idx = sorted_indices[0]\n",
    "            pos_candidate = candidate_list[pos_idx]\n",
    "            pos_probability = float(sorted_probabilities[0])\n",
    "            pos_index = candidate_index_list[pos_idx] if candidate_index_list else pos_idx\n",
    "\n",
    "            # Get the lowest scoring candidates (negative examples)\n",
    "            neg_candidates = []\n",
    "            neg_indices = []\n",
    "            neg_probabilities = []\n",
    "\n",
    "            # Take the lowest scoring candidates as negatives\n",
    "            neg_start_idx = max(1, num_candidates - actual_neg_samples)\n",
    "            for idx in sorted_indices[neg_start_idx:]:\n",
    "                neg_candidates.append(candidate_list[idx])\n",
    "                neg_indices.append(candidate_index_list[idx] if candidate_index_list else idx)\n",
    "                neg_probabilities.append(float(probs_array[idx]))\n",
    "\n",
    "            # If we don't have enough low-scoring candidates, take from the middle\n",
    "            while len(neg_candidates) < actual_neg_samples and len(neg_candidates) < num_candidates - 1:\n",
    "                remaining_idx = len(neg_candidates) + 1  # Skip the positive candidate\n",
    "                if remaining_idx < len(sorted_indices):\n",
    "                    idx = sorted_indices[remaining_idx]\n",
    "                    neg_candidates.append(candidate_list[idx])\n",
    "                    neg_indices.append(candidate_index_list[idx] if candidate_index_list else idx)\n",
    "                    neg_probabilities.append(float(probs_array[idx]))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Create teacher score list: [positive_probability, negative_probabilities...]\n",
    "            teacher_score_list = [pos_probability] + neg_probabilities\n",
    "\n",
    "            # Create the new sequence with pos/neg allocation\n",
    "            sequence_with_score = {\n",
    "                \"query_id\": sequence.get(\"query_id\", f\"query_{i}\"),\n",
    "                \"query\": sequence.get(\"query\", \"\"),\n",
    "                \"pos\": [pos_candidate],\n",
    "                \"pos_index\": [pos_index],\n",
    "                \"neg\": neg_candidates,\n",
    "                \"neg_index\": neg_indices,\n",
    "                \"teacher_scores\": teacher_score_list,\n",
    "                \"answers\": sequence.get(\"answers\", [sequence.get(\"correct_answer\", \"\")]),\n",
    "                \"task\": sequence.get(\"task\", \"icl\"),\n",
    "                \"num_candidates_available\": num_candidates,\n",
    "                \"neg_samples_used\": len(neg_candidates),\n",
    "                \"pos_probability\": pos_probability,\n",
    "                \"avg_neg_probability\": np.mean(neg_probabilities) if neg_probabilities else 0.0,\n",
    "                \"probability_separation\": pos_probability - (np.mean(neg_probabilities) if neg_probabilities else 0.0)\n",
    "            }\n",
    "\n",
    "            # Add optional fields if they exist\n",
    "            for optional_field in [\"query_date\", \"query_stock\", \"movement\", \"correct_answer\"]:\n",
    "                if optional_field in sequence:\n",
    "                    sequence_with_score[optional_field] = sequence[optional_field]\n",
    "\n",
    "            all_json_list.append(sequence_with_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing sequence {i}: {e}\")\n",
    "            skipped_sequences += 1\n",
    "            continue\n",
    "\n",
    "    # Calculate and log statistics\n",
    "    if candidate_stats:\n",
    "        avg_candidates = np.mean(candidate_stats)\n",
    "        min_candidates_found = min(candidate_stats)\n",
    "        max_candidates_found = max(candidate_stats)\n",
    "\n",
    "        logger.info(f\"Candidate statistics:\")\n",
    "        logger.info(f\"  - Average candidates per sequence: {avg_candidates:.1f}\")\n",
    "        logger.info(f\"  - Min candidates found: {min_candidates_found}\")\n",
    "        logger.info(f\"  - Max candidates found: {max_candidates_found}\")\n",
    "        logger.info(f\"  - Sequences with < {num_neg_samples + 1} candidates: {sum(1 for x in candidate_stats if x < num_neg_samples + 1)}\")\n",
    "\n",
    "    if probability_stats:\n",
    "        logger.info(f\"Probability statistics:\")\n",
    "        logger.info(f\"  - Average probability: {np.mean(probability_stats):.4f}\")\n",
    "        logger.info(f\"  - Min probability: {min(probability_stats):.4f}\")\n",
    "        logger.info(f\"  - Max probability: {max(probability_stats):.4f}\")\n",
    "        logger.info(f\"  - Std deviation: {np.std(probability_stats):.4f}\")\n",
    "\n",
    "    # Calculate separation statistics for processed sequences\n",
    "    if all_json_list:\n",
    "        separations = [seq.get('probability_separation', 0) for seq in all_json_list]\n",
    "        pos_probs = [seq.get('pos_probability', 0) for seq in all_json_list]\n",
    "        avg_neg_probs = [seq.get('avg_neg_probability', 0) for seq in all_json_list]\n",
    "        \n",
    "        logger.info(f\"Pos/Neg separation statistics:\")\n",
    "        logger.info(f\"  - Average positive probability: {np.mean(pos_probs):.4f}\")\n",
    "        logger.info(f\"  - Average negative probability: {np.mean(avg_neg_probs):.4f}\")\n",
    "        logger.info(f\"  - Average separation (pos - neg): {np.mean(separations):.4f}\")\n",
    "        logger.info(f\"  - Min separation: {min(separations):.4f}\")\n",
    "        logger.info(f\"  - Max separation: {max(separations):.4f}\")\n",
    "\n",
    "    logger.info(f\"Successfully processed: {len(all_json_list)} sequences\")\n",
    "    logger.info(f\"Skipped: {skipped_sequences} sequences\")\n",
    "\n",
    "    return all_json_list\n",
    "\n",
    "def save_json_lines(data, output_file):\n",
    "    \"\"\"Save data as JSON lines format\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "        for obj in data:\n",
    "            json_str = json.dumps(obj, ensure_ascii=False)\n",
    "            file.write(json_str + \"\\n\")\n",
    "\n",
    "    logger.info(f\"Saved {len(data)} sequences to {output_file}\")\n",
    "\n",
    "def process_probability_based_dataset(input_file, output_file, num_neg_samples=15, min_candidates=2, \n",
    "                                     adaptive_neg_samples=True, probability_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Process LLM scored dataset with candidate probabilities\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to scored data with candidate probabilities\n",
    "        output_file: Path to save pos/neg allocated data\n",
    "        num_neg_samples: Target number of negative samples per query\n",
    "        min_candidates: Minimum candidates required per query\n",
    "        adaptive_neg_samples: Whether to adaptively reduce neg samples when not enough candidates\n",
    "        probability_threshold: Minimum probability threshold for candidates (0.0 = include all)\n",
    "    \"\"\"\n",
    "    logger.info(f\"=== Processing Probability-Based LLM Scored Dataset ===\")\n",
    "    logger.info(f\"Input: {input_file}\")\n",
    "    logger.info(f\"Output: {output_file}\")\n",
    "    logger.info(f\"Target negative samples per query: {num_neg_samples}\")\n",
    "    logger.info(f\"Minimum candidates required: {min_candidates}\")\n",
    "    logger.info(f\"Probability threshold: {probability_threshold}\")\n",
    "\n",
    "    # Load the scored data\n",
    "    data = load_single_json_file(input_file)\n",
    "\n",
    "    if not data:\n",
    "        logger.error(\"No data loaded, exiting\")\n",
    "        return 0\n",
    "\n",
    "    logger.info(f\"Loaded {len(data)} sequences from input file\")\n",
    "\n",
    "    # Validate data format\n",
    "    sample_item = data[0] if data else {}\n",
    "    required_fields = [\"candidates\", \"candidate_probabilities\"]\n",
    "    missing_fields = [field for field in required_fields if field not in sample_item]\n",
    "    \n",
    "    if missing_fields:\n",
    "        logger.error(f\"Input data missing required fields: {missing_fields}\")\n",
    "        logger.error(\"Expected format: {'candidates': [...], 'candidate_probabilities': [...]}\")\n",
    "        logger.error(f\"Found fields: {list(sample_item.keys())}\")\n",
    "        return 0\n",
    "\n",
    "    logger.info(\"Data format validation passed\")\n",
    "\n",
    "    # Allocate positive and negative examples based on probabilities\n",
    "    processed_data = allocate_pos_and_neg_from_probabilities(\n",
    "        data, \n",
    "        num_neg_samples=num_neg_samples,\n",
    "        min_candidates=min_candidates,\n",
    "        probability_threshold=probability_threshold\n",
    "    )\n",
    "\n",
    "    if not processed_data:\n",
    "        logger.error(\"No data processed successfully, exiting\")\n",
    "        return 0\n",
    "\n",
    "    # Save the processed data\n",
    "    save_json_lines(processed_data, output_file)\n",
    "\n",
    "    # Generate statistics\n",
    "    stats = {\n",
    "        \"input_file\": input_file,\n",
    "        \"output_file\": output_file,\n",
    "        \"original_sequences\": len(data),\n",
    "        \"processed_sequences\": len(processed_data),\n",
    "        \"processing_rate\": len(processed_data) / len(data) if data else 0,\n",
    "        \"negative_samples_per_query\": num_neg_samples,\n",
    "        \"probability_threshold\": probability_threshold,\n",
    "        \"processing_method\": \"probability_based_allocation\"\n",
    "    }\n",
    "\n",
    "    # Save statistics\n",
    "    stats_file = output_file.replace('.json', '_processing_stats.json')\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Processing complete!\")\n",
    "    logger.info(f\"Statistics saved to: {stats_file}\")\n",
    "\n",
    "    return len(processed_data)\n",
    "\n",
    "def analyze_probability_distribution(input_file):\n",
    "    \"\"\"Analyze the probability distribution in the scored data\"\"\"\n",
    "    logger.info(f\"=== Analyzing Probability Distribution ===\")\n",
    "    \n",
    "    data = load_single_json_file(input_file)\n",
    "    if not data:\n",
    "        logger.error(\"No data to analyze\")\n",
    "        return\n",
    "\n",
    "    all_probabilities = []\n",
    "    query_max_probs = []\n",
    "    query_min_probs = []\n",
    "\n",
    "    for sequence in data:\n",
    "        probs = sequence.get(\"candidate_probabilities\", [])\n",
    "        if probs:\n",
    "            all_probabilities.extend(probs)\n",
    "            query_max_probs.append(max(probs))\n",
    "            query_min_probs.append(min(probs))\n",
    "\n",
    "    if all_probabilities:\n",
    "        logger.info(f\"Overall probability statistics:\")\n",
    "        logger.info(f\"  - Total candidates analyzed: {len(all_probabilities)}\")\n",
    "        logger.info(f\"  - Mean probability: {np.mean(all_probabilities):.4f}\")\n",
    "        logger.info(f\"  - Std deviation: {np.std(all_probabilities):.4f}\")\n",
    "        logger.info(f\"  - Min probability: {min(all_probabilities):.4f}\")\n",
    "        logger.info(f\"  - Max probability: {max(all_probabilities):.4f}\")\n",
    "        logger.info(f\"  - 25th percentile: {np.percentile(all_probabilities, 25):.4f}\")\n",
    "        logger.info(f\"  - 50th percentile (median): {np.percentile(all_probabilities, 50):.4f}\")\n",
    "        logger.info(f\"  - 75th percentile: {np.percentile(all_probabilities, 75):.4f}\")\n",
    "\n",
    "        logger.info(f\"Per-query statistics:\")\n",
    "        logger.info(f\"  - Average max probability per query: {np.mean(query_max_probs):.4f}\")\n",
    "        logger.info(f\"  - Average min probability per query: {np.mean(query_min_probs):.4f}\")\n",
    "        logger.info(f\"  - Average separation per query: {np.mean(query_max_probs) - np.mean(query_min_probs):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=== Probability-Based Positive/Negative Allocation ===\")\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"num_neg_samples\": 15,  # Target number of negative samples per query\n",
    "        \"min_candidates\": 2,    # Minimum candidates required (1 pos + 1 neg)\n",
    "        \"adaptive_neg_samples\": True,  # Adapt neg samples when not enough candidates\n",
    "        \"probability_threshold\": 0.0,  # Include all candidates (no threshold filtering)\n",
    "        \"processing_mode\": \"single_file\"  # Options: \"single_file\", \"multiple_files\", \"folder\"\n",
    "    }\n",
    "\n",
    "    # File paths - UPDATE THESE TO MATCH YOUR NEW LLM SCORING OUTPUT\n",
    "    input_file_path = \"/root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\"\n",
    "    output_file_path = \"/root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\"\n",
    "\n",
    "    logger.info(f\"Input file: {input_file_path}\")\n",
    "    logger.info(f\"Output file: {output_file_path}\")\n",
    "    logger.info(f\"Configuration: {config}\")\n",
    "\n",
    "    # First, analyze the probability distribution\n",
    "    logger.info(\"\\n=== Analyzing Input Data ===\")\n",
    "    analyze_probability_distribution(input_file_path)\n",
    "\n",
    "    # Process the probability-based dataset\n",
    "    logger.info(f\"\\n=== Processing Probability-Based Data ===\")\n",
    "    total_processed = process_probability_based_dataset(\n",
    "        input_file=input_file_path,\n",
    "        output_file=output_file_path,\n",
    "        num_neg_samples=config[\"num_neg_samples\"],\n",
    "        min_candidates=config[\"min_candidates\"],\n",
    "        adaptive_neg_samples=config[\"adaptive_neg_samples\"],\n",
    "        probability_threshold=config[\"probability_threshold\"]\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nProcessing complete! Total sequences processed: {total_processed}\")\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        logger.info(f\"\\nOutput format:\")\n",
    "        logger.info(f\"  - query: Query string\")\n",
    "        logger.info(f\"  - pos: [highest_probability_candidate]\")\n",
    "        logger.info(f\"  - neg: [lowest_probability_candidates...]\")\n",
    "        logger.info(f\"  - teacher_scores: [pos_prob, neg_prob1, neg_prob2, ...]\")\n",
    "        logger.info(f\"\\nReady for FinSeer retriever training!\")\n",
    "    else:\n",
    "        logger.error(\"No sequences were processed. Please check your input data format.\")\n",
    "\n",
    "    logger.info(f\"\\nFiles generated:\")\n",
    "    logger.info(f\"  - Training data: {output_file_path}\")\n",
    "    logger.info(f\"  - Statistics: {output_file_path.replace('.json', '_processing_stats.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bbd65-ad3c-49a4-bbbe-ce84b8a1aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408e093-3d3c-43d2-afb7-a79cf26b8730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
