{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba82ea3-ed48-4aa5-b2ab-9509d202f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:50:41,426 - INFO - === LLM Scoring for RISE/FALL Queries (Candidates with Probabilities Only) ===\n",
      "2025-08-26 13:50:41,428 - INFO - Query file: /root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_qlist.json\n",
      "2025-08-26 13:50:41,429 - INFO - Candidate file: /root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_clist.json\n",
      "2025-08-26 13:50:41,430 - INFO - Output file: /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-26 13:50:41,431 - INFO - Configuration: {'model_name': 'ElsaShaw/StockLLM', 'retrieve_number': 20, 'batch_size': 50, 'max_queries': None}\n",
      "2025-08-26 13:50:41,432 - INFO - Processing Mode: RISE/FALL ONLY - Candidates with Probabilities\n",
      "2025-08-26 13:50:41,432 - INFO - \n",
      "=== Testing model outputs ===\n",
      "2025-08-26 13:50:41,433 - INFO - === Testing Model Predictions (Rise/Fall Only) ===\n",
      "2025-08-26 13:50:43,507 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c440f8fea14c72b8c476cf1f801543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:51:11,161 - INFO - Query movement distribution:\n",
      "2025-08-26 13:51:11,163 - INFO -   - Rise: 6747\n",
      "2025-08-26 13:51:11,164 - INFO -   - Fall: 6484\n",
      "2025-08-26 13:51:11,165 - INFO -   - Freeze (skipped): 5319\n",
      "2025-08-26 13:51:11,166 - INFO -   - Total processed: 13231\n",
      "2025-08-26 13:51:14,873 - INFO - Loaded 18550 total queries, filtered to 13231 rise/fall queries\n",
      "2025-08-26 13:51:14,875 - INFO - Loaded 203275 candidates\n",
      "2025-08-26 13:51:15,299 - INFO - Testing with 13231 rise/fall queries\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1 ---\n",
      "Stock: AAPL\n",
      "Date: 2022-01-19\n",
      "Expected: fall\n",
      "Candidates from: 2020-07-19 onwards (225 total)\n",
      "Calculated probability: 0.7955255508422852\n",
      "\n",
      "--- Test 2 ---\n",
      "Stock: ADBE\n",
      "Date: 2022-01-19\n",
      "Expected: rise\n",
      "Candidates from: 2020-07-19 onwards (225 total)\n",
      "Calculated probability: 0.01\n",
      "\n",
      "--- Test 3 ---\n",
      "Stock: AAPL\n",
      "Date: 2022-01-20\n",
      "Expected: fall\n",
      "Candidates from: 2020-07-20 onwards (450 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:51:17,148 - INFO - === Testing Complete ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated probability: 0.619976282119751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:51:17,743 - INFO - \n",
      "=== Processing RISE/FALL queries with candidate probabilities ===\n",
      "2025-08-26 13:51:17,746 - INFO - === Processing Combined Dataset (RISE/FALL QUERIES ONLY) ===\n",
      "2025-08-26 13:51:17,748 - INFO - Query file: /root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_qlist.json\n",
      "2025-08-26 13:51:17,750 - INFO - Candidate file: /root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_clist.json\n",
      "2025-08-26 13:51:17,753 - INFO - Output file: /root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\n",
      "2025-08-26 13:51:17,754 - INFO - Loading model ElsaShaw/StockLLM on cuda\n",
      "2025-08-26 13:51:17,919 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba406f150124474b7892ffb3ab5e2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:51:20,249 - INFO - Model loaded successfully\n",
      "2025-08-26 13:51:20,465 - INFO - Query movement distribution:\n",
      "2025-08-26 13:51:20,466 - INFO -   - Rise: 6747\n",
      "2025-08-26 13:51:20,468 - INFO -   - Fall: 6484\n",
      "2025-08-26 13:51:20,469 - INFO -   - Freeze (skipped): 5319\n",
      "2025-08-26 13:51:20,469 - INFO -   - Total processed: 13231\n",
      "2025-08-26 13:51:24,372 - INFO - Loaded 18550 total queries, filtered to 13231 rise/fall queries\n",
      "2025-08-26 13:51:24,375 - INFO - Loaded 203275 candidates\n",
      "2025-08-26 13:51:24,646 - INFO - Rise/Fall query statistics: {'rise': 6747, 'fall': 6484, 'total': 13231}\n",
      "2025-08-26 13:51:24,647 - INFO - Total rise/fall queries available: 13231\n",
      "2025-08-26 13:51:24,648 - INFO - Queries to process: 13231\n",
      "Processing RISE/FALL queries:   0%|          | 0/13231 [00:00<?, ?it/s]2025-08-26 13:51:24,659 - WARNING - Insufficient candidates (0) for 2022-01-18 (18-month cutoff: 2020-07-18)\n",
      "2025-08-26 13:51:24,667 - INFO - Query 2022-01-19: 225 candidates from 2020-07-19 to 2022-01-19\n",
      "Processing RISE/FALL queries:  10%|â–ˆ         | 1363/13231 [43:27<6:24:02,  1.94s/it]"
     ]
    }
   ],
   "source": [
    "# Section 4: Fixed LLM Scoring - Only Probabilities for Candidates\n",
    "\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import groupby\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DATASTORE:\n",
    "    \"\"\"Class to manage and process the query and candidate data\"\"\"\n",
    "    def __init__(self, query_file_path, candidate_file_path):\n",
    "        self.query_file_path = query_file_path\n",
    "        self.candidate_file_path = candidate_file_path\n",
    "\n",
    "        # Validate files exist\n",
    "        if not os.path.exists(query_file_path):\n",
    "            raise FileNotFoundError(f\"Query file not found: {query_file_path}\")\n",
    "        if not os.path.exists(candidate_file_path):\n",
    "            raise FileNotFoundError(f\"Candidate file not found: {candidate_file_path}\")\n",
    "\n",
    "        # Load and filter query data to only include rise/fall movements\n",
    "        all_queries = self.load_json_lines(query_file_path)\n",
    "        self.query_sequence_list = self.filter_rise_fall_queries(all_queries)\n",
    "        self.candidate_sequence_list = self.load_json_lines(candidate_file_path)\n",
    "\n",
    "        logger.info(f\"Loaded {len(all_queries)} total queries, filtered to {len(self.query_sequence_list)} rise/fall queries\")\n",
    "        logger.info(f\"Loaded {len(self.candidate_sequence_list)} candidates\")\n",
    "\n",
    "    def load_json_lines(self, file_path):\n",
    "        \"\"\"Load JSON lines with error handling\"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    try:\n",
    "                        data.append(json.loads(line.strip()))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.warning(f\"Skipping invalid JSON at line {line_num} in {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "            raise\n",
    "        return data\n",
    "\n",
    "    def filter_rise_fall_queries(self, query_list):\n",
    "        \"\"\"Filter queries to only include those with 'rise' or 'fall' movements\"\"\"\n",
    "        filtered_queries = []\n",
    "        freeze_count = 0\n",
    "        rise_count = 0\n",
    "        fall_count = 0\n",
    "\n",
    "        for query in query_list:\n",
    "            movement = query.get('movement', '').lower()\n",
    "            if movement == 'rise':\n",
    "                filtered_queries.append(query)\n",
    "                rise_count += 1\n",
    "            elif movement == 'fall':\n",
    "                filtered_queries.append(query)\n",
    "                fall_count += 1\n",
    "            elif movement == 'freeze':\n",
    "                freeze_count += 1\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Query movement distribution:\")\n",
    "        logger.info(f\"  - Rise: {rise_count}\")\n",
    "        logger.info(f\"  - Fall: {fall_count}\")\n",
    "        logger.info(f\"  - Freeze (skipped): {freeze_count}\")\n",
    "        logger.info(f\"  - Total processed: {rise_count + fall_count}\")\n",
    "\n",
    "        return filtered_queries\n",
    "\n",
    "    def get_query_amount(self):\n",
    "        return len(self.query_sequence_list)\n",
    "\n",
    "    def get_rise_fall_stats(self):\n",
    "        \"\"\"Get statistics about rise/fall distribution\"\"\"\n",
    "        rise_count = sum(1 for q in self.query_sequence_list if q.get('movement', '').lower() == 'rise')\n",
    "        fall_count = sum(1 for q in self.query_sequence_list if q.get('movement', '').lower() == 'fall')\n",
    "        return {\"rise\": rise_count, \"fall\": fall_count, \"total\": rise_count + fall_count}\n",
    "\n",
    "    def group_query_by_date(self):\n",
    "        \"\"\"Group queries by date with error handling\"\"\"\n",
    "        try:\n",
    "            self.query_sequence_list.sort(key=lambda x: x[\"query_date\"])\n",
    "            processed_qlist_by_date = {\n",
    "                date: list(items)\n",
    "                for date, items in groupby(self.query_sequence_list, key=lambda x: x[\"query_date\"])\n",
    "            }\n",
    "            return processed_qlist_by_date\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Missing required field in query data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def group_candidate_by_date(self):\n",
    "        \"\"\"Group candidates by date with error handling\"\"\"\n",
    "        try:\n",
    "            self.candidate_sequence_list.sort(key=lambda x: x[\"candidate_date\"])\n",
    "            processed_clist_by_date = {\n",
    "                date: list(items)\n",
    "                for date, items in groupby(self.candidate_sequence_list, key=lambda x: x[\"candidate_date\"])\n",
    "            }\n",
    "            return processed_clist_by_date\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Missing required field in candidate data: {e}\")\n",
    "            raise\n",
    "\n",
    "def get_embedding_sequence_str(sequence, query_or_candidate):\n",
    "    \"\"\"Convert sequence to string representation with improved error handling\"\"\"\n",
    "    try:\n",
    "        if query_or_candidate == 'query':\n",
    "            seq1 = {\n",
    "                'query_stock': sequence.get('query_stock', 'Unknown'),\n",
    "                'query_date': sequence.get('query_date', 'Unknown'),\n",
    "                'recent_date_list': sequence.get('recent_date_list', []),\n",
    "                'adjusted_close_list': sequence.get('adjusted_close_list', []),\n",
    "            }\n",
    "        elif query_or_candidate == 'candidate':\n",
    "            # Find the indicator key (the one ending with '_list' that's not in the standard keys)\n",
    "            standard_keys = {'data_index', 'candidate_stock', 'candidate_date', 'candidate_movement', 'recent_date_list', 'indicator_name'}\n",
    "            indicator_key = None\n",
    "            indicator_values = []\n",
    "\n",
    "            for key in sequence.keys():\n",
    "                if key.endswith('_list') and key not in standard_keys:\n",
    "                    indicator_key = key\n",
    "                    indicator_values = sequence.get(key, [])\n",
    "                    break\n",
    "\n",
    "            if indicator_key is None:\n",
    "                # Fallback to available indicators\n",
    "                available_indicators = ['adj_close_list', 'close_list', 'volume_list', 'Returns_list']\n",
    "                for ind in available_indicators:\n",
    "                    if ind in sequence:\n",
    "                        indicator_key = ind\n",
    "                        indicator_values = sequence.get(ind, [])\n",
    "                        break\n",
    "\n",
    "                if indicator_key is None:\n",
    "                    indicator_key = 'values_list'\n",
    "                    indicator_values = []\n",
    "\n",
    "            seq1 = {\n",
    "                'candidate_stock': sequence.get('candidate_stock', 'Unknown'),\n",
    "                'candidate_date': sequence.get('candidate_date', 'Unknown'),\n",
    "                'recent_date_list': sequence.get('recent_date_list', []),\n",
    "                indicator_key: indicator_values\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid query_or_candidate value: {query_or_candidate}\")\n",
    "\n",
    "        return str(seq1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_embedding_sequence_str: {e}\")\n",
    "        return str({})\n",
    "\n",
    "def generate_candidate_prompt_for_prob(query_sequence, candidate_list, retrieve_number):\n",
    "    \"\"\"Generate prompts for the LLM to predict stock movement with improved sampling\"\"\"\n",
    "    try:\n",
    "        # Early validation - skip if not rise/fall\n",
    "        movement = query_sequence.get('movement', '').lower()\n",
    "        if movement not in ['rise', 'fall']:\n",
    "            logger.debug(f\"Skipping query with movement '{movement}' (not rise/fall)\")\n",
    "            return [], [], [], \"\"\n",
    "\n",
    "        query_date = query_sequence.get('query_date', 'Unknown')\n",
    "        query_stock = query_sequence.get('query_stock', 'Unknown')\n",
    "        query_sequence_str = get_embedding_sequence_str(sequence=query_sequence, query_or_candidate='query')\n",
    "\n",
    "        instruction = (\"Based on the following information, predict stock movement by filling in the [blank] with 'rise' or 'fall'. Just fill in the blank, do not explain.\\n\")\n",
    "        query_inst = f'\\nQuery: On {query_date}, the movement of ${query_stock} is [blank].\\n'\n",
    "        retrieve_prompt = 'These are sequences that may affect this stock\\'s price recently:\\n'\n",
    "        query_prompt = 'This is the query sequence:\\n'\n",
    "\n",
    "        # Safely sample candidates\n",
    "        actual_retrieve_number = min(retrieve_number, len(candidate_list))\n",
    "        if actual_retrieve_number == 0:\n",
    "            logger.warning(f\"No candidates available for query on {query_date}\")\n",
    "            return [], [], [], query_sequence_str\n",
    "\n",
    "        retrieve_result = random.sample(candidate_list, actual_retrieve_number)\n",
    "\n",
    "        prompt_list = []\n",
    "        candidate_index_list = []\n",
    "        candidate_str_list = []\n",
    "\n",
    "        for candidate_sequence in retrieve_result:\n",
    "            candidate_index_list.append(candidate_sequence.get('data_index', 0))\n",
    "            candidate_prompt = str({\n",
    "                'candidate_sequence': get_embedding_sequence_str(sequence=candidate_sequence, query_or_candidate='candidate')\n",
    "            })\n",
    "            candidate_str_list.append(candidate_prompt)\n",
    "            prompt = instruction + retrieve_prompt + candidate_prompt + '\\n' + query_prompt + query_sequence_str + '\\n' + query_inst\n",
    "            prompt_list.append(prompt)\n",
    "\n",
    "        return candidate_index_list, candidate_str_list, prompt_list, query_sequence_str\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating candidate prompt: {e}\")\n",
    "        return [], [], [], \"\"\n",
    "\n",
    "def get_probability_one_sequence(prompt, model, tokenizer, answer, device):\n",
    "    \"\"\"Get probability of correct answer from LLM with improved debugging and calculation\"\"\"\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a stock analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        # Apply chat template safely\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Handle tokenization with proper padding\n",
    "        input_ids = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate response with more deterministic parameters\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,  # Reduced - we only need \"rise\" or \"fall\"\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.1,  # Lower temperature for more deterministic output\n",
    "                do_sample=False,  # Use greedy decoding\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        # Get the generated text for debugging\n",
    "        generated_ids = outputs.sequences[0][input_ids.shape[1]:]  # Only new tokens\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip().lower()\n",
    "\n",
    "        # Debug: log what was generated\n",
    "        logger.debug(f\"Generated text: '{generated_text}' for answer: '{answer}'\")\n",
    "\n",
    "        # Method 1: Direct text matching (most reliable)\n",
    "        prob = 0.0\n",
    "        if answer.lower() in generated_text:\n",
    "            # If the correct answer appears in generated text, use transition scores\n",
    "            if hasattr(outputs, 'scores') and outputs.scores:\n",
    "                # Get probabilities for the first generated token\n",
    "                first_token_logits = outputs.scores[0][0]  # Shape: [vocab_size]\n",
    "                first_token_probs = torch.softmax(first_token_logits, dim=-1)\n",
    "\n",
    "                # Check both direct token and variations\n",
    "                answer_variations = [answer.lower(), answer.upper(), answer.capitalize()]\n",
    "                for variation in answer_variations:\n",
    "                    token_id = tokenizer.encode(variation, add_special_tokens=False)\n",
    "                    if token_id:  # If encoding successful\n",
    "                        token_id = token_id[0] if isinstance(token_id, list) else token_id\n",
    "                        if token_id < len(first_token_probs):\n",
    "                            candidate_prob = first_token_probs[token_id].item()\n",
    "                            prob = max(prob, candidate_prob)\n",
    "\n",
    "                # Alternative: check for partial matches in generated tokens\n",
    "                if prob == 0.0:\n",
    "                    for i, score in enumerate(outputs.scores[:3]):  # Check first 3 tokens\n",
    "                        if i >= len(generated_ids):\n",
    "                            break\n",
    "                        token_probs = torch.softmax(score[0], dim=-1)\n",
    "                        generated_token_id = generated_ids[i]\n",
    "                        generated_token = tokenizer.decode(generated_token_id).strip().lower()\n",
    "\n",
    "                        if answer.lower() in generated_token or generated_token in answer.lower():\n",
    "                            prob = token_probs[generated_token_id].item()\n",
    "                            break\n",
    "            else:\n",
    "                # Fallback: assign a reasonable probability if text matches\n",
    "                prob = 0.5\n",
    "\n",
    "        # Ensure we return a reasonable minimum probability\n",
    "        if prob == 0.0:\n",
    "            prob = 0.01  # Very small but non-zero\n",
    "\n",
    "        logger.debug(f\"Final probability: {prob} for answer '{answer}' with generated '{generated_text}'\")\n",
    "\n",
    "        # Clean up GPU memory\n",
    "        if hasattr(outputs, 'sequences'):\n",
    "            del outputs.sequences\n",
    "        if hasattr(outputs, 'scores'):\n",
    "            del outputs.scores\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        return float(prob)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in probability calculation: {e}\")\n",
    "        logger.error(f\"Prompt length: {len(prompt) if prompt else 0}\")\n",
    "        logger.error(f\"Answer: {answer}\")\n",
    "        return 0.01  # Return small non-zero value instead of 0\n",
    "\n",
    "def process_combined_dataset_scoring(query_file, candidate_file, output_file,\n",
    "                                   model_name=\"ElsaShaw/StockLLM\", retrieve_number=20,\n",
    "                                   batch_size=100, max_queries=None):\n",
    "    \"\"\"Process ONLY RISE/FALL queries and return candidates with probabilities only\"\"\"\n",
    "    \n",
    "    logger.info(f\"=== Processing Combined Dataset (RISE/FALL QUERIES ONLY) ===\")\n",
    "    logger.info(f\"Query file: {query_file}\")\n",
    "    logger.info(f\"Candidate file: {candidate_file}\")\n",
    "    logger.info(f\"Output file: {output_file}\")\n",
    "\n",
    "    # Load model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"Loading model {model_name} on {device}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        logger.info(f\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        # Load data (filtering happens in DATASTORE initialization)\n",
    "        datastore = DATASTORE(query_file, candidate_file)\n",
    "        qlist_by_date = datastore.group_query_by_date()\n",
    "        clist_by_date = datastore.group_candidate_by_date()\n",
    "\n",
    "        # Get statistics about filtered data\n",
    "        rise_fall_stats = datastore.get_rise_fall_stats()\n",
    "        total_queries_available = datastore.get_query_amount()\n",
    "        queries_to_process = total_queries_available if max_queries is None else min(max_queries, total_queries_available)\n",
    "\n",
    "        logger.info(f\"Rise/Fall query statistics: {rise_fall_stats}\")\n",
    "        logger.info(f\"Total rise/fall queries available: {total_queries_available}\")\n",
    "        logger.info(f\"Queries to process: {queries_to_process}\")\n",
    "\n",
    "        # Create output directory\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        processed_queries = 0\n",
    "        successful_queries = 0\n",
    "        skipped_queries = 0\n",
    "        all_results = []\n",
    "\n",
    "        # Create progress bar for all queries\n",
    "        pbar = tqdm(total=queries_to_process, desc=\"Processing RISE/FALL queries\")\n",
    "\n",
    "        # Process each query date\n",
    "        for query_date in sorted(qlist_by_date.keys()):\n",
    "            if max_queries and processed_queries >= max_queries:\n",
    "                break\n",
    "\n",
    "            query_sequence_list = qlist_by_date[query_date]\n",
    "\n",
    "            try:\n",
    "                query_date_dt = datetime.strptime(query_date, \"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                logger.warning(f\"Invalid date format: {query_date}\")\n",
    "                continue\n",
    "\n",
    "            # Use 18-month window for better relevance\n",
    "            cutoff_date = query_date_dt - pd.DateOffset(months=18)\n",
    "            \n",
    "            qualified_candidate_list = []\n",
    "            for candidate_date, candidate_sequence_list in clist_by_date.items():\n",
    "                try:\n",
    "                    candidate_date_dt = datetime.strptime(candidate_date, \"%Y-%m-%d\")\n",
    "                    if (query_date_dt > candidate_date_dt) and (candidate_date_dt >= cutoff_date):\n",
    "                        qualified_candidate_list.extend(candidate_sequence_list)\n",
    "                except ValueError:\n",
    "                    logger.warning(f\"Invalid candidate date format: {candidate_date}\")\n",
    "                    continue\n",
    "\n",
    "            if len(qualified_candidate_list) < 100:  # Minimum threshold\n",
    "                logger.warning(f\"Insufficient candidates ({len(qualified_candidate_list)}) for {query_date} \"\n",
    "                             f\"(18-month cutoff: {cutoff_date.strftime('%Y-%m-%d')})\")\n",
    "                continue\n",
    "\n",
    "            # Optional: Log candidate pool info for monitoring\n",
    "            if processed_queries < 5:  # Log details for first 5 queries\n",
    "                logger.info(f\"Query {query_date}: {len(qualified_candidate_list)} candidates \"\n",
    "                           f\"from {cutoff_date.strftime('%Y-%m-%d')} to {query_date}\")\n",
    "\n",
    "            # Process each query in this date\n",
    "            for query_sequence in query_sequence_list:\n",
    "                if max_queries and processed_queries >= max_queries:\n",
    "                    break\n",
    "\n",
    "                query_id = query_sequence.get('data_index', processed_queries)\n",
    "                reference_answer = query_sequence.get('movement', 'freeze')\n",
    "\n",
    "                # Double-check: Only process rise/fall movements (should already be filtered)\n",
    "                if reference_answer not in ['rise', 'fall']:\n",
    "                    logger.debug(f\"Skipping query {query_id} with movement '{reference_answer}'\")\n",
    "                    skipped_queries += 1\n",
    "                    processed_queries += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Generate prompts\n",
    "                    candidate_index_list, candidate_str_list, prompt_list, query_sequence_str = generate_candidate_prompt_for_prob(\n",
    "                        query_sequence, qualified_candidate_list, retrieve_number)\n",
    "\n",
    "                    if not prompt_list:\n",
    "                        processed_queries += 1\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    # Score with LLM (limit prompts for efficiency)\n",
    "                    candidate_probabilities = []\n",
    "                    max_prompts = min(10, len(prompt_list))  # Process up to 10 prompts per query\n",
    "\n",
    "                    for prompt in prompt_list[:max_prompts]:\n",
    "                        probability = get_probability_one_sequence(prompt, model, tokenizer, reference_answer, device)\n",
    "                        candidate_probabilities.append(probability)\n",
    "\n",
    "                    # Create result entry - FIXED FORMAT\n",
    "                    if candidate_probabilities:\n",
    "                        query_stock = query_sequence.get('query_stock', 'Unknown')\n",
    "                        query_result = {\n",
    "                            \"query_id\": f\"{query_stock}_{query_id}\",\n",
    "                            \"query\": query_sequence_str,\n",
    "                            \"candidates\": candidate_str_list[:max_prompts],  # All candidates here\n",
    "                            \"candidate_indices\": candidate_index_list[:max_prompts],  # All indices here\n",
    "                            \"candidate_probabilities\": candidate_probabilities,  # Probabilities for each candidate\n",
    "                            \"correct_answer\": reference_answer,  # What the correct answer should be\n",
    "                            \"query_date\": query_date,\n",
    "                            \"query_stock\": query_stock,\n",
    "                            \"movement\": reference_answer\n",
    "                        }\n",
    "                        all_results.append(query_result)\n",
    "                        successful_queries += 1\n",
    "\n",
    "                    processed_queries += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Save in batches to avoid memory issues\n",
    "                    if len(all_results) >= batch_size:\n",
    "                        save_batch_results(all_results, output_file, append=successful_queries > batch_size)\n",
    "                        all_results = []\n",
    "\n",
    "                        # Force garbage collection\n",
    "                        gc.collect()\n",
    "                        if device == \"cuda\":\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing query {query_id}: {e}\")\n",
    "                    processed_queries += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "        # Save remaining results\n",
    "        if all_results:\n",
    "            save_batch_results(all_results, output_file, append=successful_queries > len(all_results))\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        # Generate final statistics\n",
    "        stats = {\n",
    "            \"total_rise_fall_queries\": total_queries_available,\n",
    "            \"queries_processed\": processed_queries,\n",
    "            \"queries_successfully_scored\": successful_queries,\n",
    "            \"queries_skipped\": skipped_queries,\n",
    "            \"rise_fall_distribution\": rise_fall_stats,\n",
    "            \"success_rate\": successful_queries / processed_queries if processed_queries > 0 else 0,\n",
    "            \"output_file\": output_file,\n",
    "            \"model_used\": model_name,\n",
    "            \"retrieve_number\": retrieve_number,\n",
    "            \"processing_mode\": \"rise_fall_only_candidates_with_probabilities\"\n",
    "        }\n",
    "\n",
    "        # Save statistics\n",
    "        stats_file = output_file.replace('.json', '_stats.json')\n",
    "        with open(stats_file, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "\n",
    "        logger.info(f\"\\nProcessing Complete!\")\n",
    "        logger.info(f\"Statistics:\")\n",
    "        logger.info(f\"  - Total rise/fall queries available: {total_queries_available}\")\n",
    "        logger.info(f\"  - Rise queries: {rise_fall_stats['rise']}\")\n",
    "        logger.info(f\"  - Fall queries: {rise_fall_stats['fall']}\")\n",
    "        logger.info(f\"  - Queries processed: {processed_queries}\")\n",
    "        logger.info(f\"  - Queries successfully scored: {successful_queries}\")\n",
    "        logger.info(f\"  - Queries skipped: {skipped_queries}\")\n",
    "        logger.info(f\"  - Success rate: {stats['success_rate']:.2%}\")\n",
    "        logger.info(f\"  - Results saved to: {output_file}\")\n",
    "        logger.info(f\"  - Statistics saved to: {stats_file}\")\n",
    "\n",
    "        return successful_queries\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in combined dataset processing: {e}\")\n",
    "        return 0\n",
    "\n",
    "def save_batch_results(results, output_file, append=False):\n",
    "    \"\"\"Save results in batches to manage memory\"\"\"\n",
    "    mode = 'a' if append else 'w'\n",
    "    with open(output_file, mode, encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            json_str = json.dumps(result)\n",
    "            f.write(json_str + '\\n')\n",
    "\n",
    "def test_model_predictions(query_file, candidate_file, model_name=\"ElsaShaw/StockLLM\", num_tests=5):\n",
    "    \"\"\"Test a few predictions to see what the model generates\"\"\"\n",
    "    logger.info(\"=== Testing Model Predictions (Rise/Fall Only) ===\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    try:\n",
    "        datastore = DATASTORE(query_file, candidate_file)\n",
    "        qlist_by_date = datastore.group_query_by_date()\n",
    "        clist_by_date = datastore.group_candidate_by_date()\n",
    "\n",
    "        logger.info(f\"Testing with {datastore.get_query_amount()} rise/fall queries\")\n",
    "\n",
    "        test_count = 0\n",
    "        for query_date, query_sequence_list in qlist_by_date.items():\n",
    "            if test_count >= num_tests:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                query_date_dt = datetime.strptime(query_date, \"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            # Apply 18-month window in testing too\n",
    "            cutoff_date = query_date_dt - pd.DateOffset(months=18)\n",
    "            qualified_candidate_list = []\n",
    "\n",
    "            for candidate_date, candidate_sequence_list in clist_by_date.items():\n",
    "                try:\n",
    "                    candidate_date_dt = datetime.strptime(candidate_date, \"%Y-%m-%d\")\n",
    "                    if (query_date_dt > candidate_date_dt) and (candidate_date_dt >= cutoff_date):\n",
    "                        qualified_candidate_list.extend(candidate_sequence_list)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            for query_sequence in query_sequence_list[:2]:  # Test first 2 queries per date\n",
    "                if test_count >= num_tests:\n",
    "                    break\n",
    "\n",
    "                reference_answer = query_sequence.get('movement', 'freeze')\n",
    "                # Since datastore already filtered, this should only be rise/fall\n",
    "                if reference_answer not in ['rise', 'fall'] or not qualified_candidate_list:\n",
    "                    continue\n",
    "\n",
    "                candidate_index_list, candidate_str_list, prompt_list, query_sequence_str = generate_candidate_prompt_for_prob(\n",
    "                    query_sequence, qualified_candidate_list, 3)\n",
    "\n",
    "                if prompt_list:\n",
    "                    print(f\"\\n--- Test {test_count + 1} ---\")\n",
    "                    print(f\"Stock: {query_sequence.get('query_stock', 'Unknown')}\")\n",
    "                    print(f\"Date: {query_sequence.get('query_date', 'Unknown')}\")\n",
    "                    print(f\"Expected: {reference_answer}\")\n",
    "                    print(f\"Candidates from: {cutoff_date.strftime('%Y-%m-%d')} onwards ({len(qualified_candidate_list)} total)\")\n",
    "\n",
    "                    # Test first prompt\n",
    "                    prob = get_probability_one_sequence(prompt_list[0], model, tokenizer, reference_answer, device)\n",
    "\n",
    "                    print(f\"Calculated probability: {prob}\")\n",
    "                    test_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in testing: {e}\")\n",
    "\n",
    "    logger.info(\"=== Testing Complete ===\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=== LLM Scoring for RISE/FALL Queries (Candidates with Probabilities Only) ===\")\n",
    "\n",
    "    # Your specific file paths\n",
    "    query_file_path = \"/root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_qlist.json\"\n",
    "    candidate_file_path = \"/root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_clist.json\"\n",
    "    output_file_path = \"/root/nfs/AJ FinRag/LLM Scores/llm_data/all_companies_scored_candidates_probabilities.json\"\n",
    "\n",
    "    # Configuration - Process only rise/fall queries with 18-month window\n",
    "    config = {\n",
    "        \"model_name\": \"ElsaShaw/StockLLM\",\n",
    "        \"retrieve_number\": 20,  # Number of candidates per query\n",
    "        \"batch_size\": 50,       # Process in batches for memory management\n",
    "        \"max_queries\": None     # None = process ALL rise/fall queries (no limit)\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Query file: {query_file_path}\")\n",
    "    logger.info(f\"Candidate file: {candidate_file_path}\")\n",
    "    logger.info(f\"Output file: {output_file_path}\")\n",
    "    logger.info(f\"Configuration: {config}\")\n",
    "    logger.info(f\"Processing Mode: RISE/FALL ONLY - Candidates with Probabilities\")\n",
    "\n",
    "    # Test the model first (optional - comment out if you want to skip)\n",
    "    logger.info(\"\\n=== Testing model outputs ===\")\n",
    "    test_model_predictions(\n",
    "        query_file_path,\n",
    "        candidate_file_path,\n",
    "        config[\"model_name\"],\n",
    "        num_tests=3\n",
    "    )\n",
    "\n",
    "    # Process ONLY rise/fall queries in the combined dataset with 18-month window\n",
    "    logger.info(f\"\\n=== Processing RISE/FALL queries with candidate probabilities ===\")\n",
    "    total_queries = process_combined_dataset_scoring(\n",
    "        query_file=query_file_path,\n",
    "        candidate_file=candidate_file_path,\n",
    "        output_file=output_file_path,\n",
    "        model_name=config[\"model_name\"],\n",
    "        retrieve_number=config[\"retrieve_number\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_queries=config[\"max_queries\"]\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nRise/Fall scoring complete! Total queries successfully processed: {total_queries}\")\n",
    "    logger.info(f\"Output format: Candidates with probabilities (ready for pos/neg allocation)\")\n",
    "    \n",
    "    # Show sample output format\n",
    "    logger.info(\"\\nSample output format:\")\n",
    "    logger.info('''{\n",
    "  \"query\": \"query_string\",\n",
    "  \"candidates\": [\"candidate_1\", \"candidate_2\", \"candidate_3\"],\n",
    "  \"candidate_probabilities\": [0.85, 0.23, 0.67],\n",
    "  \"correct_answer\": \"rise\"\n",
    "}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4c655-9b66-4643-b605-d5e9ef7ab84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
