{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59479aca-eddb-4ee2-85d9-a4934bc3a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training FinQuest Retriever on device: cuda\n",
      "INFO:__main__:Loading pos/neg allocated data from: /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\n",
      "INFO:__main__:Processing 1 pos/neg allocated data files...\n",
      "INFO:__main__:Loading /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json...\n",
      "INFO:__main__:Loaded 13210 examples from /root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\n",
      "INFO:__main__:Loaded 13210 training examples for FinQuest\n",
      "INFO:__main__:FinQuest training data analysis:\n",
      "INFO:__main__:  Total examples: 13210\n",
      "INFO:__main__:  Valid examples (positive score > 0): 13210\n",
      "INFO:__main__:  Examples with zero positive scores: 0\n",
      "INFO:__main__:  Positive score range: 0.0100 - 0.9761\n",
      "INFO:__main__:  Average positive score: 0.4816\n",
      "INFO:__main__:  Negative score range: 0.0100 - 0.9571\n",
      "INFO:__main__:  Average negative score: 0.3197\n",
      "INFO:__main__:  Average pos-neg separation: 0.1619\n",
      "INFO:__main__:  Separation std: 0.1819\n",
      "INFO:__main__:FinQuest dataset loaded successfully with 13210 examples\n",
      "INFO:__main__:FinQuest Retriever initialized with 22,861,056 parameters\n",
      "INFO:__main__:Using batch size: 12 (GPU memory: 25.4GB)\n",
      "INFO:__main__:Starting FinQuest training:\n",
      "INFO:__main__:  Training examples: 13,210\n",
      "INFO:__main__:  Model parameters: 22,861,056\n",
      "INFO:__main__:  Estimated memory usage: ~0.01GB\n",
      "/tmp/ipykernel_759/1241941847.py:396: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "FinQuest Epoch 1/20:   0%|          | 0/1100 [00:00<?, ?it/s]/tmp/ipykernel_759/1241941847.py:429: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_759/1241941847.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_759/1241941847.py:443: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "FinQuest Epoch 1/20: 100%|██████████| 1100/1100 [02:12<00:00,  8.31it/s, Loss=1.0505, CL=2.099, KD=0.003, Div=0.000, GN=1.84, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 1 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.1413 (CL: 2.280, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4993 - 0.6112\n",
      "INFO:__main__:    Avg similarity: 0.5527 (±0.0355)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.1413\n",
      "FinQuest Epoch 2/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.42it/s, Loss=1.1013, CL=2.200, KD=0.003, Div=0.000, GN=2.02, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 2 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.1191 (CL: 2.236, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.1191\n",
      "FinQuest Epoch 3/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.39it/s, Loss=1.1090, CL=2.216, KD=0.003, Div=0.000, GN=1.92, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 3 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.1061 (CL: 2.210, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4954 - 0.5850\n",
      "INFO:__main__:    Avg similarity: 0.5352 (±0.0235)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.1061\n",
      "FinQuest Epoch 4/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.40it/s, Loss=1.2454, CL=2.488, KD=0.003, Div=0.000, GN=2.72, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 4 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0929 (CL: 2.183, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0929\n",
      "INFO:__main__:Saved checkpoint: epoch 4\n",
      "FinQuest Epoch 5/20: 100%|██████████| 1100/1100 [02:12<00:00,  8.33it/s, Loss=1.1523, CL=2.302, KD=0.003, Div=0.000, GN=2.45, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 5 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0803 (CL: 2.158, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4417 - 0.5455\n",
      "INFO:__main__:    Avg similarity: 0.4990 (±0.0337)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0803\n",
      "FinQuest Epoch 6/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.37it/s, Loss=1.1028, CL=2.203, KD=0.003, Div=0.000, GN=3.00, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 6 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0653 (CL: 2.128, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0653\n",
      "FinQuest Epoch 7/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.38it/s, Loss=1.0622, CL=2.122, KD=0.003, Div=0.000, GN=3.40, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 7 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0513 (CL: 2.100, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4251 - 0.5549\n",
      "INFO:__main__:    Avg similarity: 0.4921 (±0.0356)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0513\n",
      "FinQuest Epoch 8/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.40it/s, Loss=0.9693, CL=1.936, KD=0.004, Div=0.000, GN=3.79, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 8 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0393 (CL: 2.076, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0393\n",
      "INFO:__main__:Saved checkpoint: epoch 8\n",
      "FinQuest Epoch 9/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.38it/s, Loss=0.9829, CL=1.964, KD=0.002, Div=0.000, GN=5.65, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 9 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0249 (CL: 2.047, KD: 0.003, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4666 - 0.5812\n",
      "INFO:__main__:    Avg similarity: 0.5117 (±0.0351)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0249\n",
      "FinQuest Epoch 10/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.38it/s, Loss=0.9475, CL=1.893, KD=0.003, Div=0.000, GN=4.12, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 10 Summary:\n",
      "INFO:__main__:  Avg Loss: 1.0105 (CL: 2.018, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 1.0105\n",
      "FinQuest Epoch 11/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.36it/s, Loss=1.0456, CL=2.086, KD=0.007, Div=0.000, GN=4.56, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 11 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.9945 (CL: 1.986, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4234 - 0.5776\n",
      "INFO:__main__:    Avg similarity: 0.4980 (±0.0452)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.9945\n",
      "FinQuest Epoch 12/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.39it/s, Loss=0.8363, CL=1.669, KD=0.005, Div=0.000, GN=4.61, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 12 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.9790 (CL: 1.955, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.9790\n",
      "INFO:__main__:Saved checkpoint: epoch 12\n",
      "FinQuest Epoch 13/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.39it/s, Loss=0.8797, CL=1.755, KD=0.005, Div=0.000, GN=6.47, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 13 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.9612 (CL: 1.919, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4096 - 0.5852\n",
      "INFO:__main__:    Avg similarity: 0.4905 (±0.0594)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.9612\n",
      "FinQuest Epoch 14/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.41it/s, Loss=0.8402, CL=1.678, KD=0.003, Div=0.000, GN=4.89, Skip=0]\n",
      "INFO:__main__:FinQuest Epoch 14 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.9452 (CL: 1.887, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.9452\n",
      "FinQuest Epoch 15/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.43it/s, Loss=0.8186, CL=1.634, KD=0.004, Div=0.000, GN=5.36, Skip=0] \n",
      "INFO:__main__:FinQuest Epoch 15 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.9297 (CL: 1.856, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.3872 - 0.5837\n",
      "INFO:__main__:    Avg similarity: 0.4913 (±0.0654)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.9297\n",
      "FinQuest Epoch 16/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.44it/s, Loss=0.6934, CL=1.384, KD=0.004, Div=0.000, GN=5.14, Skip=0] \n",
      "INFO:__main__:FinQuest Epoch 16 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.9127 (CL: 1.822, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.9127\n",
      "INFO:__main__:Saved checkpoint: epoch 16\n",
      "FinQuest Epoch 17/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.40it/s, Loss=0.9552, CL=1.907, KD=0.005, Div=0.000, GN=6.49, Skip=0] \n",
      "INFO:__main__:FinQuest Epoch 17 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.8929 (CL: 1.783, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.3911 - 0.6251\n",
      "INFO:__main__:    Avg similarity: 0.5038 (±0.0871)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.8929\n",
      "FinQuest Epoch 18/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.40it/s, Loss=0.8985, CL=1.794, KD=0.004, Div=0.000, GN=6.65, Skip=0] \n",
      "INFO:__main__:FinQuest Epoch 18 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.8758 (CL: 1.748, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.8758\n",
      "FinQuest Epoch 19/20: 100%|██████████| 1100/1100 [02:11<00:00,  8.37it/s, Loss=1.0072, CL=2.011, KD=0.004, Div=0.000, GN=9.82, Skip=0] \n",
      "INFO:__main__:FinQuest Epoch 19 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.8576 (CL: 1.712, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:  Embedding Quality Check:\n",
      "INFO:__main__:    Similarity range: 0.4053 - 0.6697\n",
      "INFO:__main__:    Avg similarity: 0.5332 (±0.0957)\n",
      "INFO:__main__:  ✅ Healthy embedding diversity\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.8576\n",
      "FinQuest Epoch 20/20: 100%|██████████| 1100/1100 [02:10<00:00,  8.41it/s, Loss=0.7729, CL=1.543, KD=0.004, Div=0.000, GN=7.10, Skip=0] \n",
      "INFO:__main__:FinQuest Epoch 20 Summary:\n",
      "INFO:__main__:  Avg Loss: 0.8414 (CL: 1.680, KD: 0.004, Div: 0.000)\n",
      "INFO:__main__:  Processed: 1100 batches, Skipped: 0\n",
      "INFO:__main__:Saved best FinQuest model with loss 0.8414\n",
      "INFO:__main__:Saved checkpoint: epoch 20\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:FinQuest Training Completed Successfully!\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Saved models:\n",
      "INFO:__main__:  - finquest_models/finquest_retriever_best.pth (best validation loss)\n",
      "INFO:__main__:  - finquest_models/finquest_retriever_final.pth (final model)\n",
      "INFO:__main__:  - finquest_models/finquest_retriever_epoch_X.pth (checkpoints)\n",
      "INFO:__main__:Final training loss: 0.8414\n",
      "INFO:__main__:Best training loss: 0.8414\n",
      "INFO:__main__:\n",
      "FinQuest is ready for financial pattern retrieval!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FinQuestDataset(Dataset):\n",
    "    \"\"\"Dataset for training FinQuest - Financial Pattern Retriever\"\"\"\n",
    "    \n",
    "    def __init__(self, pos_neg_data_paths, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = self.load_multiple_pos_neg_files(pos_neg_data_paths)\n",
    "        logger.info(f\"Loaded {len(self.data)} training examples for FinQuest\")\n",
    "\n",
    "        # Analyze score distribution\n",
    "        self.analyze_teacher_scores()\n",
    "\n",
    "    def analyze_teacher_scores(self):\n",
    "        \"\"\"Analyze the distribution of teacher probability scores\"\"\"\n",
    "        all_pos_scores = []\n",
    "        all_neg_scores = []\n",
    "        separations = []\n",
    "        zero_score_count = 0\n",
    "        valid_examples = 0\n",
    "\n",
    "        for item in self.data:\n",
    "            scores = item.get('teacher_scores', [])\n",
    "            if len(scores) == 0:\n",
    "                continue\n",
    "                \n",
    "            pos_score = scores[0] if len(scores) > 0 else 0\n",
    "            neg_scores = scores[1:] if len(scores) > 1 else []\n",
    "            \n",
    "            if pos_score > 0:\n",
    "                all_pos_scores.append(pos_score)\n",
    "                valid_examples += 1\n",
    "                \n",
    "                if neg_scores:\n",
    "                    all_neg_scores.extend(neg_scores)\n",
    "                    avg_neg = np.mean(neg_scores)\n",
    "                    separations.append(pos_score - avg_neg)\n",
    "            else:\n",
    "                zero_score_count += 1\n",
    "\n",
    "        logger.info(f\"FinQuest training data analysis:\")\n",
    "        logger.info(f\"  Total examples: {len(self.data)}\")\n",
    "        logger.info(f\"  Valid examples (positive score > 0): {valid_examples}\")\n",
    "        logger.info(f\"  Examples with zero positive scores: {zero_score_count}\")\n",
    "        \n",
    "        if all_pos_scores:\n",
    "            logger.info(f\"  Positive score range: {min(all_pos_scores):.4f} - {max(all_pos_scores):.4f}\")\n",
    "            logger.info(f\"  Average positive score: {np.mean(all_pos_scores):.4f}\")\n",
    "            \n",
    "        if all_neg_scores:\n",
    "            logger.info(f\"  Negative score range: {min(all_neg_scores):.4f} - {max(all_neg_scores):.4f}\")\n",
    "            logger.info(f\"  Average negative score: {np.mean(all_neg_scores):.4f}\")\n",
    "            \n",
    "        if separations:\n",
    "            logger.info(f\"  Average pos-neg separation: {np.mean(separations):.4f}\")\n",
    "            logger.info(f\"  Separation std: {np.std(separations):.4f}\")\n",
    "\n",
    "    def load_multiple_pos_neg_files(self, paths):\n",
    "        \"\"\"Load and combine pos/neg allocated data from multiple files\"\"\"\n",
    "        all_data = []\n",
    "\n",
    "        # Handle different input types\n",
    "        if isinstance(paths, str):\n",
    "            if paths.endswith('.json'):\n",
    "                paths = [paths]\n",
    "            else:\n",
    "                paths = glob.glob(paths)\n",
    "        elif isinstance(paths, (list, tuple)):\n",
    "            expanded_paths = []\n",
    "            for path in paths:\n",
    "                if '*' in path:\n",
    "                    expanded_paths.extend(glob.glob(path))\n",
    "                else:\n",
    "                    expanded_paths.append(path)\n",
    "            paths = expanded_paths\n",
    "\n",
    "        logger.info(f\"Processing {len(paths)} pos/neg allocated data files...\")\n",
    "\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path):\n",
    "                logger.warning(f\"File not found: {path}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"Loading {path}...\")\n",
    "            file_data = self.load_pos_neg_data(path)\n",
    "\n",
    "            # Add file identifier for tracking\n",
    "            file_name = Path(path).stem\n",
    "            for item in file_data:\n",
    "                item['source_file'] = file_name\n",
    "\n",
    "            all_data.extend(file_data)\n",
    "            logger.info(f\"Loaded {len(file_data)} examples from {path}\")\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def load_pos_neg_data(self, path):\n",
    "        \"\"\"Load pos/neg allocated data with better error handling\"\"\"\n",
    "        data = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        item = json.loads(line)\n",
    "                        if self.validate_item(item):\n",
    "                            data.append(item)\n",
    "                        else:\n",
    "                            logger.warning(f\"Invalid item structure at {path}:{line_num}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Skipping malformed JSON at {path}:{line_num}: {e}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Unexpected error at {path}:{line_num}: {e}\")\n",
    "        return data\n",
    "\n",
    "    def validate_item(self, item):\n",
    "        \"\"\"Validate that item has required fields for pos/neg training\"\"\"\n",
    "        required_fields = ['query', 'pos', 'neg', 'teacher_scores']\n",
    "        has_required = all(field in item for field in required_fields)\n",
    "        \n",
    "        if has_required:\n",
    "            # Additional validation\n",
    "            pos_list = item.get('pos', [])\n",
    "            neg_list = item.get('neg', [])\n",
    "            scores = item.get('teacher_scores', [])\n",
    "            \n",
    "            # Check if we have at least one positive and one negative\n",
    "            if len(pos_list) > 0 and len(neg_list) > 0 and len(scores) >= 2:\n",
    "                return True\n",
    "            else:\n",
    "                logger.debug(f\"Item failed validation: pos={len(pos_list)}, neg={len(neg_list)}, scores={len(scores)}\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.data[idx]\n",
    "\n",
    "            # Extract data from pos/neg format\n",
    "            query = item['query']\n",
    "            pos_candidates = item['pos']  # Should be a list with 1 item\n",
    "            neg_candidates = item['neg']  # Should be a list with multiple items\n",
    "            teacher_scores = item['teacher_scores']  # [pos_score, neg_score1, neg_score2, ...]\n",
    "\n",
    "            # Validate lengths\n",
    "            expected_scores = len(pos_candidates) + len(neg_candidates)\n",
    "            if len(teacher_scores) != expected_scores:\n",
    "                logger.warning(f\"Score mismatch at idx {idx}: expected {expected_scores}, got {len(teacher_scores)}\")\n",
    "                return None\n",
    "\n",
    "            # Skip if no positive or negative candidates\n",
    "            if len(pos_candidates) == 0 or len(neg_candidates) == 0:\n",
    "                return None\n",
    "\n",
    "            # Skip examples with zero positive score\n",
    "            pos_score = teacher_scores[0]\n",
    "            if pos_score <= 0.0:\n",
    "                return None\n",
    "\n",
    "            # Get the positive candidate (should be just one)\n",
    "            positive_candidate = pos_candidates[0]\n",
    "            \n",
    "            # Get negative candidates and their scores\n",
    "            neg_scores = teacher_scores[1:len(neg_candidates)+1]\n",
    "\n",
    "            return {\n",
    "                'query': query,\n",
    "                'positive': positive_candidate,\n",
    "                'negatives': neg_candidates,\n",
    "                'teacher_scores_selected': teacher_scores,  # Keep all scores\n",
    "                'pos_score': float(pos_score),\n",
    "                'neg_scores': [float(s) for s in neg_scores],\n",
    "                'source_file': item.get('source_file', 'unknown'),\n",
    "                'query_stock': item.get('query_stock', 'unknown'),\n",
    "                'query_date': item.get('query_date', 'unknown')\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing item at index {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for FinQuest training\"\"\"\n",
    "        batch = [item for item in batch if item is not None]\n",
    "        if not batch:\n",
    "            return None\n",
    "\n",
    "        if len(batch) == 1:\n",
    "            logger.warning(\"Batch size of 1 may cause training instability\")\n",
    "\n",
    "        queries = [item['query'] for item in batch]\n",
    "        positives = [item['positive'] for item in batch]\n",
    "        source_files = [item['source_file'] for item in batch]\n",
    "        query_stocks = [item['query_stock'] for item in batch]\n",
    "        query_dates = [item['query_date'] for item in batch]\n",
    "\n",
    "        # Handle variable number of negatives\n",
    "        max_num_negatives = max(len(item['negatives']) for item in batch)\n",
    "\n",
    "        negatives_padded = []\n",
    "        negatives_mask = []\n",
    "        teacher_scores_padded = []\n",
    "\n",
    "        for item in batch:\n",
    "            num_negatives = len(item['negatives'])\n",
    "            \n",
    "            # Pad negatives\n",
    "            padded_negs = item['negatives'] + [''] * (max_num_negatives - num_negatives)\n",
    "            mask = [1] * num_negatives + [0] * (max_num_negatives - num_negatives)\n",
    "\n",
    "            # Pad teacher scores\n",
    "            # Format: [pos_score, neg_score1, neg_score2, ..., padding_zeros]\n",
    "            scores = item['teacher_scores_selected']\n",
    "            padded_scores = scores + [0.0] * (max_num_negatives - (len(scores) - 1))\n",
    "\n",
    "            negatives_padded.append(padded_negs)\n",
    "            negatives_mask.append(mask)\n",
    "            teacher_scores_padded.append(padded_scores)\n",
    "\n",
    "        return {\n",
    "            'query': queries,\n",
    "            'positive': positives,\n",
    "            'negatives_padded': negatives_padded,\n",
    "            'negatives_mask': torch.tensor(negatives_mask, dtype=torch.bool),\n",
    "            'teacher_scores_padded_tensor': torch.tensor(teacher_scores_padded, dtype=torch.float32),\n",
    "            'source_files': source_files,\n",
    "            'query_stocks': query_stocks,\n",
    "            'query_dates': query_dates\n",
    "        }\n",
    "\n",
    "class FinQuestRetriever(nn.Module):\n",
    "    \"\"\"FinQuest - Advanced Financial Pattern Retriever\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2', hidden_size=384, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Base encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Financial domain adaptation layers - SIMPLIFIED TO PREVENT COLLAPSE\n",
    "        encoder_dim = self.encoder.config.hidden_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        logger.info(f\"FinQuest Retriever initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
    "\n",
    "    def encode_sequence(self, sequences):\n",
    "        \"\"\"Encode financial sequences with domain adaptation\"\"\"\n",
    "        if not sequences or all(not seq.strip() for seq in sequences):\n",
    "            # Handle empty sequences\n",
    "            return torch.zeros(len(sequences), self.hidden_size).to(next(self.parameters()).device)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            sequences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(next(self.parameters()).device)\n",
    "\n",
    "        # Use mixed precision for encoding\n",
    "        with autocast():\n",
    "            # Get base embeddings\n",
    "            outputs = self.encoder(**inputs)\n",
    "            # Use mean pooling instead of just CLS token\n",
    "            embeddings = self.mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "        # Project to financial domain in full precision\n",
    "        projected = self.projection(embeddings.float())\n",
    "        \n",
    "        # Apply dropout only during training\n",
    "        if self.training:\n",
    "            projected = self.dropout(projected)\n",
    "        \n",
    "        # DON'T NORMALIZE HERE - let loss functions handle it\n",
    "        return projected\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Mean pooling with attention mask\"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "class FinancialContrastiveLoss(nn.Module):\n",
    "    \"\"\"Contrastive loss optimized for financial patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.2):  # INCREASED TEMPERATURE\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, query_emb, pos_emb, neg_embs, neg_mask):\n",
    "        # NORMALIZE HERE\n",
    "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "        pos_emb = F.normalize(pos_emb, p=2, dim=1)\n",
    "        neg_embs = F.normalize(neg_embs, p=2, dim=2)\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = torch.sum(query_emb * pos_emb, dim=1) / self.temperature\n",
    "        neg_sims = torch.bmm(query_emb.unsqueeze(1), neg_embs.transpose(1, 2)).squeeze(1) / self.temperature\n",
    "\n",
    "        # Mask out padded negatives\n",
    "        neg_sims = neg_sims.masked_fill(~neg_mask, -1e9)\n",
    "\n",
    "        # Concatenate positive and negative similarities\n",
    "        logits = torch.cat([pos_sim.unsqueeze(1), neg_sims], dim=1)\n",
    "\n",
    "        # Positive examples should have label 0 (first position)\n",
    "        targets = torch.zeros(logits.size(0), dtype=torch.long).to(logits.device)\n",
    "\n",
    "        return F.cross_entropy(logits, targets)\n",
    "\n",
    "class FinancialKnowledgeDistillation(nn.Module):\n",
    "    \"\"\"Knowledge distillation loss for financial relevance scores\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=2.0):  # REDUCED TEMPERATURE\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, query_emb, candidate_embs, teacher_scores_padded, candidate_mask):\n",
    "        # NORMALIZE HERE TOO\n",
    "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "        candidate_embs = F.normalize(candidate_embs, p=2, dim=2)\n",
    "        \n",
    "        # Student similarities\n",
    "        student_sims = torch.bmm(query_emb.unsqueeze(1), candidate_embs.transpose(1, 2)).squeeze(1) / self.temperature\n",
    "        student_sims = student_sims.masked_fill(~candidate_mask, -1e9)\n",
    "\n",
    "        # Better teacher score handling\n",
    "        teacher_scores_clamped = torch.clamp(teacher_scores_padded, min=0.01, max=0.99)\n",
    "        teacher_probs = F.softmax(teacher_scores_clamped / self.temperature, dim=1)\n",
    "        \n",
    "        # Mask teacher probs and renormalize\n",
    "        teacher_probs = teacher_probs * candidate_mask.float()\n",
    "        teacher_probs = teacher_probs / (teacher_probs.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        student_log_probs = F.log_softmax(student_sims, dim=1)\n",
    "\n",
    "        # KL divergence with better stability\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "\n",
    "def compute_embedding_diversity_loss(embeddings, min_distance=0.1):\n",
    "    \"\"\"Encourage embedding diversity to prevent collapse\"\"\"\n",
    "    embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    batch_size = embeddings_norm.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = torch.mm(embeddings_norm, embeddings_norm.t())\n",
    "    \n",
    "    # Remove diagonal (self-similarities)\n",
    "    mask = ~torch.eye(batch_size).bool().to(similarities.device)\n",
    "    off_diagonal_sims = similarities[mask]\n",
    "    \n",
    "    # Encourage low similarities (diverse directions)\n",
    "    diversity_loss = torch.mean(torch.relu(off_diagonal_sims - (1.0 - min_distance)))\n",
    "    \n",
    "    return diversity_loss\n",
    "\n",
    "def train_finquest(model, dataloader, optimizer, device, num_epochs=15, save_every=3, save_dir=\"finquest_models\"):\n",
    "    \"\"\"Train FinQuest retriever with advanced loss combination\"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    contrastive_loss_fn = FinancialContrastiveLoss(temperature=0.2)  # FIXED TEMPERATURE\n",
    "    kd_loss_fn = FinancialKnowledgeDistillation(temperature=2.0)  # FIXED TEMPERATURE\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    training_history = []\n",
    "\n",
    "    # Advanced gradient clipping\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = total_cl = total_kd = total_div = 0\n",
    "        num_batches = 0\n",
    "        skipped_batches = 0\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f'FinQuest Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress):\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            try:\n",
    "                # Validate teacher scores\n",
    "                teacher_scores = batch['teacher_scores_padded_tensor'].to(device)\n",
    "                if torch.any(torch.isnan(teacher_scores)) or torch.any(torch.isinf(teacher_scores)):\n",
    "                    logger.warning(f\"Skipping batch {batch_idx}: NaN/Inf in teacher scores\")\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "\n",
    "                # Robust score clamping for financial data\n",
    "                teacher_scores = torch.clamp(teacher_scores, min=0.0, max=1.0)\n",
    "\n",
    "                # Forward pass with mixed precision\n",
    "                with autocast():\n",
    "                    # Encode query and positive\n",
    "                    q_emb = model.encode_sequence(batch['query'])\n",
    "                    p_emb = model.encode_sequence(batch['positive'])\n",
    "\n",
    "                    # Flatten and encode all negatives\n",
    "                    all_neg_seqs = [neg for negs in batch['negatives_padded'] for neg in negs if neg.strip()]\n",
    "                    if not all_neg_seqs:  # Handle edge case\n",
    "                        skipped_batches += 1\n",
    "                        continue\n",
    "                        \n",
    "                    n_embs = model.encode_sequence(all_neg_seqs).view(len(batch['query']), -1, q_emb.size(-1))\n",
    "\n",
    "                # Compute losses in full precision\n",
    "                with torch.cuda.amp.autocast(enabled=False):\n",
    "                    # Convert to float32 for stable loss computation\n",
    "                    q_emb_fp32 = q_emb.float()\n",
    "                    p_emb_fp32 = p_emb.float()\n",
    "                    n_embs_fp32 = n_embs.float()\n",
    "\n",
    "                    # Financial contrastive loss\n",
    "                    cl_loss = contrastive_loss_fn(q_emb_fp32, p_emb_fp32, n_embs_fp32, batch['negatives_mask'].to(device))\n",
    "\n",
    "                    # Financial knowledge distillation loss\n",
    "                    candidate_embs_fp32 = torch.cat([p_emb_fp32.unsqueeze(1), n_embs_fp32], dim=1)\n",
    "                    candidate_mask = torch.cat([\n",
    "                        torch.ones(len(batch['query']), 1, dtype=torch.bool).to(device),\n",
    "                        batch['negatives_mask'].to(device)\n",
    "                    ], dim=1)\n",
    "\n",
    "                    kd_loss = kd_loss_fn(\n",
    "                        q_emb_fp32,\n",
    "                        candidate_embs_fp32,\n",
    "                        teacher_scores,\n",
    "                        candidate_mask\n",
    "                    )\n",
    "\n",
    "                    # DIVERSITY LOSS TO PREVENT COLLAPSE\n",
    "                    all_embeddings = torch.cat([q_emb_fp32, p_emb_fp32], dim=0)\n",
    "                    diversity_loss = compute_embedding_diversity_loss(all_embeddings, min_distance=0.15)\n",
    "\n",
    "                    # BALANCED LOSS COMBINATION\n",
    "                    cl_loss = torch.clamp(cl_loss, max=10.0)\n",
    "                    kd_loss = torch.clamp(kd_loss, max=10.0)\n",
    "                    diversity_loss = torch.clamp(diversity_loss, max=2.0)\n",
    "                    \n",
    "                    # FIXED WEIGHTS - NO ADAPTIVE\n",
    "                    loss = 0.5 * cl_loss + 0.4 * kd_loss + 0.1 * diversity_loss\n",
    "\n",
    "                # Validate loss before backward pass\n",
    "                if torch.isnan(loss) or torch.isinf(loss) or loss > 100:\n",
    "                    logger.warning(f\"Skipping batch {batch_idx}: Invalid loss {loss.item()}\")\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "\n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Advanced gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # Update metrics\n",
    "                total_loss += loss.item()\n",
    "                total_cl += cl_loss.item()\n",
    "                total_kd += kd_loss.item()\n",
    "                total_div += diversity_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Enhanced progress bar\n",
    "                progress.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'CL': f'{cl_loss.item():.3f}',\n",
    "                    'KD': f'{kd_loss.item():.3f}',\n",
    "                    'Div': f'{diversity_loss.item():.3f}',\n",
    "                    'GN': f'{grad_norm:.2f}',\n",
    "                    'Skip': skipped_batches\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch {batch_idx}: {e}\")\n",
    "                skipped_batches += 1\n",
    "                continue\n",
    "\n",
    "        # Epoch summary with EMBEDDING QUALITY CHECK\n",
    "        if num_batches > 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            avg_cl = total_cl / num_batches\n",
    "            avg_kd = total_kd / num_batches\n",
    "            avg_div = total_div / num_batches\n",
    "\n",
    "            epoch_stats = {\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_loss': avg_loss,\n",
    "                'avg_contrastive': avg_cl,\n",
    "                'avg_kd': avg_kd,\n",
    "                'avg_diversity': avg_div,\n",
    "                'skipped_batches': skipped_batches,\n",
    "                'processed_batches': num_batches\n",
    "            }\n",
    "            training_history.append(epoch_stats)\n",
    "\n",
    "            logger.info(f'FinQuest Epoch {epoch+1} Summary:')\n",
    "            logger.info(f'  Avg Loss: {avg_loss:.4f} (CL: {avg_cl:.3f}, KD: {avg_kd:.3f}, Div: {avg_div:.3f})')\n",
    "            logger.info(f'  Processed: {num_batches} batches, Skipped: {skipped_batches}')\n",
    "\n",
    "            # EMBEDDING QUALITY CHECK EVERY 2 EPOCHS\n",
    "            if epoch % 2 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_seqs = [\n",
    "                        \"Apple financial data analysis\",\n",
    "                        \"Tesla stock performance metrics\",\n",
    "                        \"Google revenue growth patterns\",\n",
    "                        \"Microsoft quarterly earnings\",\n",
    "                        \"Amazon business indicators\"\n",
    "                    ]\n",
    "                    test_embs = model.encode_sequence(test_seqs)\n",
    "                    test_embs_norm = F.normalize(test_embs, p=2, dim=1)\n",
    "                    \n",
    "                    # Compute similarities\n",
    "                    sims = torch.mm(test_embs_norm, test_embs_norm.t())\n",
    "                    mask = ~torch.eye(len(test_seqs)).bool()\n",
    "                    off_diagonal = sims[mask]\n",
    "                    \n",
    "                    min_sim = torch.min(off_diagonal).item()\n",
    "                    max_sim = torch.max(off_diagonal).item()\n",
    "                    avg_sim = torch.mean(off_diagonal).item()\n",
    "                    std_sim = torch.std(off_diagonal).item()\n",
    "                    \n",
    "                    logger.info(f\"  Embedding Quality Check:\")\n",
    "                    logger.info(f\"    Similarity range: {min_sim:.4f} - {max_sim:.4f}\")\n",
    "                    logger.info(f\"    Avg similarity: {avg_sim:.4f} (±{std_sim:.4f})\")\n",
    "                    \n",
    "                    # COLLAPSE DETECTION\n",
    "                    if avg_sim > 0.9:\n",
    "                        logger.warning(\"  ⚠️  HIGH COLLAPSE RISK!\")\n",
    "                    elif avg_sim > 0.8:\n",
    "                        logger.warning(\"  ⚠️  Moderate collapse risk\")\n",
    "                    else:\n",
    "                        logger.info(\"  ✅ Healthy embedding diversity\")\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "            # Save best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_model_path = os.path.join(save_dir, 'finquest_retriever_best.pth')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': avg_loss,\n",
    "                    'training_history': training_history,\n",
    "                    'model_config': {\n",
    "                        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                        'hidden_size': model.hidden_size,\n",
    "                        'retriever_type': 'FinQuest'\n",
    "                    }\n",
    "                }, best_model_path)\n",
    "                logger.info(f\"Saved best FinQuest model with loss {avg_loss:.4f}\")\n",
    "\n",
    "            # Regular checkpoint\n",
    "            if (epoch + 1) % save_every == 0:\n",
    "                checkpoint_path = os.path.join(save_dir, f'finquest_retriever_epoch_{epoch+1}.pth')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': avg_loss,\n",
    "                    'training_history': training_history\n",
    "                }, checkpoint_path)\n",
    "                logger.info(f\"Saved checkpoint: epoch {epoch+1}\")\n",
    "        else:\n",
    "            logger.warning(f\"Epoch {epoch+1}: No valid batches processed!\")\n",
    "\n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return training_history\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Training FinQuest Retriever on device: {device}\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # Your pos/neg allocated data file path\n",
    "    pos_neg_data_path = \"/root/nfs/AJ FinRag/Training Data/llm_data/all_companies_train_pos_neg.json\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(pos_neg_data_path):\n",
    "        logger.error(f\"Pos/neg data file not found: {pos_neg_data_path}\")\n",
    "        logger.error(\"Please ensure your pos/neg allocation step completed successfully\")\n",
    "        logger.error(\"Expected format: {'query': '...', 'pos': [...], 'neg': [...], 'teacher_scores': [...]}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Loading pos/neg allocated data from: {pos_neg_data_path}\")\n",
    "\n",
    "    # Initialize FinQuest dataset\n",
    "    dataset = FinQuestDataset(pos_neg_data_path, tokenizer)\n",
    "\n",
    "    # Validate dataset\n",
    "    if len(dataset) == 0:\n",
    "        logger.error(\"FinQuest dataset is empty! Please check your pos/neg data file.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"FinQuest dataset loaded successfully with {len(dataset)} examples\")\n",
    "\n",
    "    # Initialize FinQuest model\n",
    "    model = FinQuestRetriever(hidden_size=384, dropout_rate=0.1).to(device)\n",
    "\n",
    "    # Dynamic batch size based on GPU memory\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
    "            if gpu_memory < 6:\n",
    "                batch_size = 2\n",
    "            elif gpu_memory < 12:\n",
    "                batch_size = 4\n",
    "            elif gpu_memory < 24:\n",
    "                batch_size = 8\n",
    "            else:\n",
    "                batch_size = 12\n",
    "        else:\n",
    "            batch_size = 2\n",
    "    except:\n",
    "        batch_size = 4  # Conservative default\n",
    "\n",
    "    logger.info(f\"Using batch size: {batch_size} \" + \n",
    "               (f\"(GPU memory: {gpu_memory:.1f}GB)\" if torch.cuda.is_available() else \"(CPU)\"))\n",
    "\n",
    "    # Create optimized dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=0,  # Keep at 0 for stability\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    # BETTER OPTIMIZER SETTINGS\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,  # Slightly higher LR\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.7, patience=3, verbose=True, min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Starting FinQuest training:\")\n",
    "    logger.info(f\"  Training examples: {len(dataset):,}\")\n",
    "    logger.info(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    logger.info(f\"  Estimated memory usage: ~{batch_size * 512 * 384 * 4 / 1e9:.2f}GB\")\n",
    "\n",
    "    # Train FinQuest\n",
    "    training_history = train_finquest(\n",
    "        model=model, \n",
    "        dataloader=dataloader, \n",
    "        optimizer=optimizer, \n",
    "        device=device, \n",
    "        num_epochs=20,  # More epochs for financial domain\n",
    "        save_every=4,\n",
    "        save_dir=\"finquest_models\"\n",
    "    )\n",
    "\n",
    "    # Final model save\n",
    "    final_model_path = \"finquest_models/finquest_retriever_final.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            'hidden_size': model.hidden_size,\n",
    "            'retriever_type': 'FinQuest',\n",
    "            'training_examples': len(dataset),\n",
    "            'final_epoch': len(training_history)\n",
    "        },\n",
    "        'training_history': training_history\n",
    "    }, final_model_path)\n",
    "\n",
    "    # Training completion summary\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"FinQuest Training Completed Successfully!\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"Saved models:\")\n",
    "    logger.info(\"  - finquest_models/finquest_retriever_best.pth (best validation loss)\")\n",
    "    logger.info(\"  - finquest_models/finquest_retriever_final.pth (final model)\")\n",
    "    logger.info(\"  - finquest_models/finquest_retriever_epoch_X.pth (checkpoints)\")\n",
    "    if training_history:\n",
    "        logger.info(f\"Final training loss: {training_history[-1]['avg_loss']:.4f}\")\n",
    "        logger.info(f\"Best training loss: {min(h['avg_loss'] for h in training_history):.4f}\")\n",
    "    logger.info(\"\\nFinQuest is ready for financial pattern retrieval!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bb00e-3cfe-4ca3-9dd8-d12e6eb8c66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
