{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b1f476-14a5-4f31-861f-00b4d96f1cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting technical indicator processing...\n",
      "Input file: /root/nfs/AJ FinRag/Company Raw Data/all_companies.csv\n",
      "Output directory: /root/nfs/AJ FinRag/Company Processed Data\n",
      "Loading data from: /root/nfs/AJ FinRag/Company Raw Data/all_companies.csv\n",
      "Processing 25 companies...\n",
      "\n",
      "Processing indicators for AAPL...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/AAPL_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/AAPL_processed.json\n",
      "\n",
      "Processing indicators for MSFT...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/MSFT_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/MSFT_processed.json\n",
      "\n",
      "Processing indicators for GOOGL...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/GOOGL_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/GOOGL_processed.json\n",
      "\n",
      "Processing indicators for AMZN...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/AMZN_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/AMZN_processed.json\n",
      "\n",
      "Processing indicators for META...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/META_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/META_processed.json\n",
      "\n",
      "Processing indicators for TSLA...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/TSLA_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/TSLA_processed.json\n",
      "\n",
      "Processing indicators for NVDA...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/NVDA_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/NVDA_processed.json\n",
      "\n",
      "Processing indicators for JPM...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/JPM_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/JPM_processed.json\n",
      "\n",
      "Processing indicators for V...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/V_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/V_processed.json\n",
      "\n",
      "Processing indicators for MA...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/MA_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/MA_processed.json\n",
      "\n",
      "Processing indicators for BAC...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/BAC_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/BAC_processed.json\n",
      "\n",
      "Processing indicators for WMT...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/WMT_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/WMT_processed.json\n",
      "\n",
      "Processing indicators for HD...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/HD_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/HD_processed.json\n",
      "\n",
      "Processing indicators for MCD...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/MCD_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/MCD_processed.json\n",
      "\n",
      "Processing indicators for NKE...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/NKE_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/NKE_processed.json\n",
      "\n",
      "Processing indicators for JNJ...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/JNJ_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/JNJ_processed.json\n",
      "\n",
      "Processing indicators for PFE...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/PFE_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/PFE_processed.json\n",
      "\n",
      "Processing indicators for UNH...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/UNH_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/UNH_processed.json\n",
      "\n",
      "Processing indicators for XOM...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/XOM_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/XOM_processed.json\n",
      "\n",
      "Processing indicators for CVX...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/CVX_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/CVX_processed.json\n",
      "\n",
      "Processing indicators for INTC...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/INTC_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/INTC_processed.json\n",
      "\n",
      "Processing indicators for AMD...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/AMD_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/AMD_processed.json\n",
      "\n",
      "Processing indicators for ADBE...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/ADBE_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/ADBE_processed.json\n",
      "\n",
      "Processing indicators for CRM...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/CRM_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/CRM_processed.json\n",
      "\n",
      "Processing indicators for NFLX...\n",
      "Saved CSV: /root/nfs/AJ FinRag/Company Processed Data/NFLX_processed.csv\n",
      "Saved JSON: /root/nfs/AJ FinRag/Company Processed Data/NFLX_processed.json\n",
      "\n",
      "Successfully saved combined CSV to /root/nfs/AJ FinRag/Company Processed Data/all_companies_processed.csv\n",
      "Successfully saved combined JSON to /root/nfs/AJ FinRag/Company Processed Data/all_companies_processed.json\n",
      "\n",
      "Processing complete! Sample of processed data:\n",
      "Shape: (18800, 19)\n",
      "\n",
      "Columns: ['date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close', 'MACD_Histogram', 'macd_crossover', 'bollinger_bands', 'exceeding_upper', 'exceeding_lower', 'overbought_and_oversold_conditions', 'kdj_crossover', 'Returns', 'VWAP', 'alpha_smr', 'alpha_mom']\n",
      "\n",
      "Sample data:\n",
      "        date ticker        open        high         low       close  \\\n",
      "0 2022-01-03   AAPL  177.830002  182.880005  177.710007  182.009995   \n",
      "1 2022-01-04   AAPL  182.630005  182.940002  179.119995  179.699997   \n",
      "2 2022-01-05   AAPL  179.610001  180.169998  174.639999  174.919998   \n",
      "3 2022-01-06   AAPL  172.699997  175.300003  171.639999  172.000000   \n",
      "4 2022-01-07   AAPL  172.889999  174.139999  171.029999  172.169998   \n",
      "\n",
      "      volume   adj_close  MACD_Histogram  macd_crossover  bollinger_bands  \\\n",
      "0  104487900  178.443115        0.000000               0              NaN   \n",
      "1   99310400  176.178406       -0.023034              -1              NaN   \n",
      "2   94537600  171.492081       -0.109459               0              NaN   \n",
      "3   96904000  168.629288       -0.187472               0              NaN   \n",
      "4   86709100  168.795975       -0.187652               0              NaN   \n",
      "\n",
      "   exceeding_upper  exceeding_lower  overbought_and_oversold_conditions  \\\n",
      "0              NaN              NaN                                 NaN   \n",
      "1              NaN              NaN                                 NaN   \n",
      "2              NaN              NaN                                 NaN   \n",
      "3              NaN              NaN                                 NaN   \n",
      "4              NaN              NaN                                 NaN   \n",
      "\n",
      "   kdj_crossover   Returns        VWAP  alpha_smr  alpha_mom  \n",
      "0            NaN       NaN  180.866669        NaN        NaN  \n",
      "1            NaN -0.012691  180.730224        NaN        NaN  \n",
      "2            NaN -0.026600  179.414031        NaN        NaN  \n",
      "3            NaN -0.016693  177.836551        NaN        NaN  \n",
      "4            NaN  0.000988  176.866838        NaN        NaN  \n",
      "\n",
      "Data summary:\n",
      "Date range: 2022-01-03 00:00:00 to 2024-12-30 00:00:00\n",
      "Companies: 25\n",
      "Total records: 18800\n",
      "\n",
      "NaN values in each column:\n",
      "date                                      0\n",
      "ticker                                    0\n",
      "open                                      0\n",
      "high                                      0\n",
      "low                                       0\n",
      "close                                     0\n",
      "volume                                    0\n",
      "adj_close                                 0\n",
      "MACD_Histogram                            0\n",
      "macd_crossover                            0\n",
      "bollinger_bands                       16862\n",
      "exceeding_upper                       17764\n",
      "exceeding_lower                       17898\n",
      "overbought_and_oversold_conditions    12856\n",
      "kdj_crossover                         11279\n",
      "Returns                                  25\n",
      "VWAP                                      0\n",
      "alpha_smr                               475\n",
      "alpha_mom                               475\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths - using absolute paths for reliability\n",
    "raw_data_path = os.path.abspath(\"/root/nfs/AJ FinRag/Company Raw Data/all_companies.csv\")\n",
    "processed_data_dir = os.path.abspath(\"/root/nfs/AJ FinRag/Company Processed Data\")\n",
    "\n",
    "def verify_file_exists(file_path):\n",
    "    \"\"\"Check if file exists and is accessible\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found at: {file_path}\")\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise ValueError(f\"Path exists but is not a file: {file_path}\")\n",
    "    if not os.access(file_path, os.R_OK):\n",
    "        raise PermissionError(f\"Cannot read file: {file_path}\")\n",
    "\n",
    "def clean_and_drop_nan(df):\n",
    "    \"\"\"Clean and drop NaN values while preserving data structure\"\"\"\n",
    "    # First replace inf/-inf with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # For time series data, we don't want to drop all rows with any NaN\n",
    "    # Instead, we'll handle different columns differently\n",
    "\n",
    "    # For indicator columns, we can safely drop rows where all indicator values are NaN\n",
    "    indicator_cols = ['MACD_Histogram', 'macd_crossover', 'bollinger_bands',\n",
    "                     'exceeding_upper', 'exceeding_lower',\n",
    "                     'overbought_and_oversold_conditions', 'kdj_crossover',\n",
    "                     'Returns', 'VWAP', 'alpha_smr', 'alpha_mom']\n",
    "\n",
    "    # For price/volume data, we should keep even if indicators are NaN\n",
    "    price_cols = ['open', 'high', 'low', 'close', 'volume', 'adj_close']\n",
    "\n",
    "    # Keep rows where we have at least one indicator value or all price data\n",
    "    df = df.dropna(subset=price_cols, how='all')\n",
    "\n",
    "    # For indicators, only drop rows where all indicators are NaN\n",
    "    if len(indicator_cols) > 0:\n",
    "        df = df.dropna(subset=indicator_cols, how='all')\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_ema(data, period):\n",
    "    \"\"\"Calculate Exponential Moving Average\"\"\"\n",
    "    return data.ewm(span=period).mean()\n",
    "\n",
    "def calculate_sma(data, period):\n",
    "    \"\"\"Calculate Simple Moving Average\"\"\"\n",
    "    return data.rolling(window=period).mean()\n",
    "\n",
    "def calculate_macd(df, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD indicator\"\"\"\n",
    "    try:\n",
    "        # Calculate EMAs\n",
    "        ema_fast = calculate_ema(df['close'], fast)\n",
    "        ema_slow = calculate_ema(df['close'], slow)\n",
    "        \n",
    "        # MACD line\n",
    "        macd_line = ema_fast - ema_slow\n",
    "        \n",
    "        # Signal line\n",
    "        signal_line = calculate_ema(macd_line, signal)\n",
    "        \n",
    "        # MACD Histogram\n",
    "        df['MACD_Histogram'] = macd_line - signal_line\n",
    "        \n",
    "        # MACD Crossover (1 for bullish, -1 for bearish, 0 for no signal)\n",
    "        macd_crossover = np.where(\n",
    "            (macd_line.shift(1) <= signal_line.shift(1)) & (macd_line > signal_line), 1,\n",
    "            np.where((macd_line.shift(1) >= signal_line.shift(1)) & (macd_line < signal_line), -1, 0)\n",
    "        )\n",
    "        df['macd_crossover'] = macd_crossover\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['MACD_Histogram'] = 0.0\n",
    "        df['macd_crossover'] = None\n",
    "        return df\n",
    "\n",
    "def calculate_bollinger_bands(df, period=20, std_dev=2):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    try:\n",
    "        # Calculate moving average and standard deviation\n",
    "        sma = calculate_sma(df['close'], period)\n",
    "        std = df['close'].rolling(window=period).std()\n",
    "        \n",
    "        # Calculate bands\n",
    "        upper_band = sma + (std * std_dev)\n",
    "        lower_band = sma - (std * std_dev)\n",
    "        \n",
    "        # Bollinger Band signals\n",
    "        df['bollinger_bands'] = np.where(df['close'] > upper_band, 1,\n",
    "                                np.where(df['close'] < lower_band, -1, None))\n",
    "        \n",
    "        # Exceeding bands\n",
    "        df['exceeding_upper'] = np.where(df['close'] > upper_band, 1, None)\n",
    "        df['exceeding_lower'] = np.where(df['close'] < lower_band, 1, None)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['bollinger_bands'] = None\n",
    "        df['exceeding_upper'] = None\n",
    "        df['exceeding_lower'] = None\n",
    "        return df\n",
    "\n",
    "def calculate_kdj(df, k_period=9, d_period=3, j_period=3):\n",
    "    \"\"\"Calculate KDJ indicator\"\"\"\n",
    "    try:\n",
    "        # Calculate %K\n",
    "        low_min = df['low'].rolling(window=k_period).min()\n",
    "        high_max = df['high'].rolling(window=k_period).max()\n",
    "        \n",
    "        k_percent = 100 * (df['close'] - low_min) / (high_max - low_min)\n",
    "        \n",
    "        # Calculate %D (smoothed %K)\n",
    "        d_percent = k_percent.rolling(window=d_period).mean()\n",
    "        \n",
    "        # Calculate %J\n",
    "        j_percent = 3 * k_percent - 2 * d_percent\n",
    "        \n",
    "        # Overbought/Oversold conditions\n",
    "        overbought = (k_percent > 80) & (d_percent > 80)\n",
    "        oversold = (k_percent < 20) & (d_percent < 20)\n",
    "        \n",
    "        df['overbought_and_oversold_conditions'] = np.where(overbought, 1,\n",
    "                                                   np.where(oversold, -1, None))\n",
    "        \n",
    "        # KDJ Crossover\n",
    "        kdj_crossover = np.where(\n",
    "            (k_percent.shift(1) <= d_percent.shift(1)) & (k_percent > d_percent), 1,\n",
    "            np.where((k_percent.shift(1) >= d_percent.shift(1)) & (k_percent < d_percent), -1, None)\n",
    "        )\n",
    "        df['kdj_crossover'] = kdj_crossover\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['overbought_and_oversold_conditions'] = None\n",
    "        df['kdj_crossover'] = None\n",
    "        return df\n",
    "\n",
    "def calculate_returns(df):\n",
    "    \"\"\"Calculate returns\"\"\"\n",
    "    try:\n",
    "        df['Returns'] = df['adj_close'].pct_change()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['Returns'] = None\n",
    "        return df\n",
    "\n",
    "def calculate_vwap(df):\n",
    "    \"\"\"Calculate Volume Weighted Average Price\"\"\"\n",
    "    try:\n",
    "        # Typical price\n",
    "        typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "        \n",
    "        # VWAP calculation\n",
    "        df['VWAP'] = (typical_price * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['VWAP'] = df['close']\n",
    "        return df\n",
    "\n",
    "def add_mean_reversion_alpha(df, lookback=20):\n",
    "    \"\"\"Add mean reversion alpha factor\"\"\"\n",
    "    try:\n",
    "        # Calculate rolling mean and standard deviation\n",
    "        rolling_mean = df['close'].rolling(window=lookback).mean()\n",
    "        rolling_std = df['close'].rolling(window=lookback).std()\n",
    "        \n",
    "        # Z-score (how many standard deviations from mean)\n",
    "        z_score = (df['close'] - rolling_mean) / rolling_std\n",
    "        \n",
    "        # Mean reversion signal (negative z-score indicates potential reversion)\n",
    "        df['alpha_smr'] = -z_score * df['Returns'].shift(1)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['alpha_smr'] = None\n",
    "        return df\n",
    "\n",
    "def add_momentum_alpha(df, short_window=5, long_window=20):\n",
    "    \"\"\"Add momentum alpha factor\"\"\"\n",
    "    try:\n",
    "        # Calculate short and long term moving averages\n",
    "        short_ma = calculate_sma(df['close'], short_window)\n",
    "        long_ma = calculate_sma(df['close'], long_window)\n",
    "        \n",
    "        # Momentum signal\n",
    "        momentum_signal = (short_ma - long_ma) / long_ma\n",
    "        \n",
    "        # Momentum alpha (momentum signal * lagged returns)\n",
    "        df['alpha_mom'] = momentum_signal * df['Returns'].shift(1)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df['alpha_mom'] = None\n",
    "        return df\n",
    "\n",
    "def calculate_all_indicators(df):\n",
    "    \"\"\"Calculate all technical indicators for a dataframe\"\"\"\n",
    "    # Make sure data is sorted by date for each ticker\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "    df = calculate_macd(df)\n",
    "    df = calculate_bollinger_bands(df)\n",
    "    df = calculate_kdj(df)\n",
    "    df = calculate_returns(df)\n",
    "    df = calculate_vwap(df)\n",
    "    df = add_mean_reversion_alpha(df)\n",
    "    df = add_momentum_alpha(df)\n",
    "    return clean_and_drop_nan(df)\n",
    "\n",
    "def process_dataframe_with_indicators(input_path, output_dir):\n",
    "    \"\"\"Process existing dataframe by adding technical indicators\"\"\"\n",
    "    try:\n",
    "        # Verify input file exists\n",
    "        verify_file_exists(input_path)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load the combined data\n",
    "        print(f\"Loading data from: {input_path}\")\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert date column to datetime if it's not already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Process each company separately\n",
    "        processed_dfs = []\n",
    "        tickers = df['ticker'].unique()\n",
    "\n",
    "        print(f\"Processing {len(tickers)} companies...\")\n",
    "\n",
    "        for ticker in tickers:\n",
    "            print(f\"\\nProcessing indicators for {ticker}...\")\n",
    "            try:\n",
    "                # Get data for this ticker\n",
    "                ticker_data = df[df['ticker'] == ticker].copy()\n",
    "\n",
    "                # Sort by date\n",
    "                ticker_data = ticker_data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "                # Calculate indicators\n",
    "                processed_df = calculate_all_indicators(ticker_data)\n",
    "\n",
    "                # Save individual company file (CSV)\n",
    "                output_csv_path = os.path.join(output_dir, f\"{ticker}_processed.csv\")\n",
    "                processed_df.to_csv(output_csv_path, index=False)\n",
    "                print(f\"Saved CSV: {output_csv_path}\")\n",
    "\n",
    "                # Save individual company file (JSON)\n",
    "                output_json_path = os.path.join(output_dir, f\"{ticker}_processed.json\")\n",
    "                processed_df.to_json(output_json_path, orient='records', lines=True)\n",
    "                print(f\"Saved JSON: {output_json_path}\")\n",
    "\n",
    "                processed_dfs.append(processed_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ticker}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Combine all processed data\n",
    "        if processed_dfs:\n",
    "            combined_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "\n",
    "            # Save combined data (CSV)\n",
    "            combined_csv_path = os.path.join(output_dir, \"all_companies_processed.csv\")\n",
    "            combined_df.to_csv(combined_csv_path, index=False)\n",
    "            print(f\"\\nSuccessfully saved combined CSV to {combined_csv_path}\")\n",
    "\n",
    "            # Save combined data (JSON)\n",
    "            combined_json_path = os.path.join(output_dir, \"all_companies_processed.json\")\n",
    "            combined_df.to_json(combined_json_path, orient='records', lines=True)\n",
    "            print(f\"Successfully saved combined JSON to {combined_json_path}\")\n",
    "\n",
    "            return combined_df\n",
    "        else:\n",
    "            print(\"\\nNo data was processed successfully\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error in processing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting technical indicator processing...\")\n",
    "    print(f\"Input file: {raw_data_path}\")\n",
    "    print(f\"Output directory: {processed_data_dir}\")\n",
    "\n",
    "    try:\n",
    "        processed_df = process_dataframe_with_indicators(raw_data_path, processed_data_dir)\n",
    "\n",
    "        if processed_df is not None:\n",
    "            print(\"\\nProcessing complete! Sample of processed data:\")\n",
    "            print(f\"Shape: {processed_df.shape}\")\n",
    "            print(\"\\nColumns:\", processed_df.columns.tolist())\n",
    "            print(\"\\nSample data:\")\n",
    "            print(processed_df.head())\n",
    "\n",
    "            print(\"\\nData summary:\")\n",
    "            print(f\"Date range: {processed_df['date'].min()} to {processed_df['date'].max()}\")\n",
    "            print(f\"Companies: {processed_df['ticker'].nunique()}\")\n",
    "            print(f\"Total records: {len(processed_df)}\")\n",
    "\n",
    "            # Show NaN statistics\n",
    "            print(\"\\nNaN values in each column:\")\n",
    "            print(processed_df.isna().sum())\n",
    "        else:\n",
    "            print(\"\\nProcessing completed with errors\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to run processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf0945-24b1-45e2-87d4-f95ae7563eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
