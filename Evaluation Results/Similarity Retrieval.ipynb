{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5328314-0610-4c39-adc0-776a6597c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading experimental data...\n",
      "INFO:__main__:Loading candidates from 75 files...\n",
      "INFO:__main__:Loaded 2642 test queries\n",
      "INFO:__main__:Loaded 2642 ground truth entries\n",
      "INFO:__main__:Loaded 8151 candidates\n",
      "INFO:__main__:Loaded similarity results for 2642 queries\n",
      "INFO:__main__:ðŸš€ RUNNING MULTI-LLM FINQUEST SIMILARITY EXPERIMENT\n",
      "INFO:__main__:ðŸš€ Starting MULTI-LLM FINQUEST SIMILARITY RETRIEVAL experiment\n",
      "INFO:__main__:Testing LLMs: ['StockLLM', 'Qwen2.5-1.5B', 'Llama3.2-3B']\n",
      "INFO:__main__:Candidates per query: 5\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: StockLLM\n",
      "INFO:__main__:Description: Specialized financial LLM\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:ðŸš€ Starting FINQUEST SIMILARITY RETRIEVAL experiment with StockLLM (k=5)\n",
      "INFO:__main__:Loading StockLLM: ElsaShaw/StockLLM\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba8ecbabad0483f9d14eff42008128d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:âœ… Successfully loaded StockLLM\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy: 0.510 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy: 0.455 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy: 0.470 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy: 0.472 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy: 0.493 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy: 0.496 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy: 0.499 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy: 0.501 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy: 0.488 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy: 0.479 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy: 0.479 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy: 0.478 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy: 0.485 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy: 0.487 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy: 0.481 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy: 0.479 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy: 0.476 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy: 0.472 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy: 0.470 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy: 0.473 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy: 0.475 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy: 0.474 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy: 0.477 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy: 0.476 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy: 0.478 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy: 0.476 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy: 0.479 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy: 0.481 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy: 0.481 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy: 0.483 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy: 0.483 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy: 0.483 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy: 0.485 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy: 0.485 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy: 0.485 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy: 0.487 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy: 0.491 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy: 0.495 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy: 0.496 | Candidate Coverage: 1.000\n",
      "INFO:__main__:======================================================================\n",
      "INFO:__main__:FINQUEST SIMILARITY RETRIEVAL RESULTS - StockLLM\n",
      "INFO:__main__:======================================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.4924\n",
      "INFO:__main__:Rise Accuracy: 0.8191 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.1341 (1260 samples)\n",
      "INFO:__main__:Candidate Coverage: 1.0000 (2642/2642)\n",
      "INFO:__main__:Avg Candidates Used: 5.0\n",
      "INFO:__main__:Avg Similarity Score: 1.0000\n",
      "INFO:__main__:Results saved to: finquest_similarity_experiments/StockLLM_finquest_similarity_k5_results.csv\n",
      "INFO:__main__:======================================================================\n",
      "INFO:__main__:âœ… Completed StockLLM\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: Qwen2.5-1.5B\n",
      "INFO:__main__:Description: Qwen instruction-following model\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:ðŸš€ Starting FINQUEST SIMILARITY RETRIEVAL experiment with Qwen2.5-1.5B (k=5)\n",
      "INFO:__main__:Loading Qwen2.5-1.5B: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:__main__:âœ… Successfully loaded Qwen2.5-1.5B\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy: 0.551 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy: 0.495 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy: 0.497 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy: 0.487 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy: 0.478 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy: 0.477 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy: 0.489 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy: 0.481 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy: 0.481 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy: 0.478 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy: 0.479 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy: 0.483 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy: 0.485 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy: 0.487 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy: 0.481 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy: 0.487 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy: 0.483 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy: 0.486 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy: 0.485 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy: 0.482 | Candidate Coverage: 1.000\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy: 0.484 | Candidate Coverage: 1.000\n"
     ]
    }
   ],
   "source": [
    "# SECTION 3: FINQUEST SIMILARITY RETRIEVAL EXPERIMENT\n",
    "# LLM prediction with FinQuest similarity-based retrieved candidates\n",
    "# Tests the effectiveness of learned financial pattern similarity\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FinQuestSimilarityExperiment:\n",
    "    \"\"\"\n",
    "    FinQuest similarity retrieval experiment: LLM + similarity-based retrieved candidates\n",
    "    Tests the effectiveness of trained FinQuest retriever vs. baselines\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 test_queries_file: str, \n",
    "                 ground_truth_file: str, \n",
    "                 embeddings_dir: str,\n",
    "                 similarity_results_file: str):\n",
    "        \n",
    "        self.test_queries_file = test_queries_file\n",
    "        self.ground_truth_file = ground_truth_file\n",
    "        self.embeddings_dir = embeddings_dir\n",
    "        self.similarity_results_file = similarity_results_file\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load all required data\n",
    "        self._load_data()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.test_queries)} test queries\")\n",
    "        logger.info(f\"Loaded {len(self.ground_truth)} ground truth entries\")\n",
    "        logger.info(f\"Loaded {len(self.candidates)} candidates\")\n",
    "        logger.info(f\"Loaded similarity results for {len(self.similarity_results)} queries\")\n",
    "        \n",
    "        # LLM configurations\n",
    "        self.llm_configs = {\n",
    "            'StockLLM': {\n",
    "                'model_name': 'ElsaShaw/StockLLM',\n",
    "                'description': 'Specialized financial LLM',\n",
    "                'max_length': 1024\n",
    "            },\n",
    "            'Llama3.2-3B': {\n",
    "                'model_name': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "                'description': 'Medium general-purpose LLM',\n",
    "                'max_length': 2048\n",
    "            },\n",
    "            'Qwen2.5-1.5B': {\n",
    "                'model_name': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "                'description': 'Qwen instruction-following model',\n",
    "                'max_length': 2048\n",
    "            },\n",
    "            'Phi3-Mini': {\n",
    "                'model_name': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "                'description': 'Microsoft compact LLM',\n",
    "                'max_length': 4096\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Current LLM state\n",
    "        self.current_llm = None\n",
    "        self.current_tokenizer = None\n",
    "        self.current_llm_name = None\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load all required datasets\"\"\"\n",
    "        logger.info(\"Loading experimental data...\")\n",
    "        \n",
    "        # Load test queries\n",
    "        self.test_queries = self._load_json_file(self.test_queries_file)\n",
    "        \n",
    "        # Load ground truth\n",
    "        self.ground_truth = self._load_json_file(self.ground_truth_file)\n",
    "        self.gt_lookup = {gt['query_id']: gt for gt in self.ground_truth}\n",
    "        \n",
    "        # Load candidates from embeddings\n",
    "        self.candidates = self._load_candidates()\n",
    "        \n",
    "        # Load pre-computed similarity results\n",
    "        if os.path.exists(self.similarity_results_file):\n",
    "            with open(self.similarity_results_file, 'rb') as f:\n",
    "                self.similarity_results = pickle.load(f)\n",
    "            \n",
    "            # Create lookup for fast access\n",
    "            self.similarity_lookup = {}\n",
    "            for result in self.similarity_results:\n",
    "                key = f\"{result['query_stock']}_{result['query_date']}\"\n",
    "                self.similarity_lookup[key] = result\n",
    "            \n",
    "        else:\n",
    "            logger.error(f\"Similarity results file not found: {self.similarity_results_file}\")\n",
    "            raise FileNotFoundError(\"FinQuest similarity results required for this experiment\")\n",
    "    \n",
    "    def _load_json_file(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load JSONL file\"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return data\n",
    "    \n",
    "    def _load_candidates(self) -> Dict:\n",
    "        \"\"\"Load candidates from embedding files\"\"\"\n",
    "        candidates = {}\n",
    "        embedding_files = list(Path(self.embeddings_dir).glob(\"c_*_FinQuest_embeddings_*.pkl\"))\n",
    "        \n",
    "        logger.info(f\"Loading candidates from {len(embedding_files)} files...\")\n",
    "        \n",
    "        for file_path in embedding_files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                embedding_data = pickle.load(f)\n",
    "            \n",
    "            for date_group in embedding_data:\n",
    "                for date, candidates_on_date in date_group.items():\n",
    "                    for candidate_item in candidates_on_date:\n",
    "                        candidate_data = candidate_item['data']\n",
    "                        candidates[candidate_data['data_index']] = candidate_data\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def load_llm(self, llm_name: str):\n",
    "        \"\"\"Load specified LLM for testing\"\"\"\n",
    "        if llm_name not in self.llm_configs:\n",
    "            raise ValueError(f\"Unknown LLM: {llm_name}\")\n",
    "        \n",
    "        # Clear previous model\n",
    "        if self.current_llm is not None:\n",
    "            del self.current_llm\n",
    "            del self.current_tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        config = self.llm_configs[llm_name]\n",
    "        model_name = config['model_name']\n",
    "        \n",
    "        logger.info(f\"Loading {llm_name}: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            self.current_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if self.current_tokenizer.pad_token is None:\n",
    "                self.current_tokenizer.pad_token = self.current_tokenizer.eos_token\n",
    "            \n",
    "            self.current_llm = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16 if self.device.type == 'cuda' else torch.float32,\n",
    "                device_map=\"auto\" if self.device.type == 'cuda' else None,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.current_llm_name = llm_name\n",
    "            logger.info(f\"âœ… Successfully loaded {llm_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to load {llm_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_finquest_candidates(self, query: Dict, k: int = 5) -> Tuple[List[Dict], List[float], List[int]]:\n",
    "        \"\"\"\n",
    "        Get top-k candidates from pre-computed FinQuest similarity results\n",
    "        Returns candidates, similarity scores, and movement distribution\n",
    "        \"\"\"\n",
    "        query_stock = query.get('query_stock', '')\n",
    "        query_date = query.get('query_date', '')\n",
    "        \n",
    "        # Look up pre-computed similarity results\n",
    "        key = f\"{query_stock}_{query_date}\"\n",
    "        \n",
    "        if key not in self.similarity_lookup:\n",
    "            logger.warning(f\"No similarity results found for {query_stock} on {query_date}\")\n",
    "            return [], [], [0, 0]\n",
    "        \n",
    "        similarity_result = self.similarity_lookup[key]\n",
    "        similarity_list = similarity_result.get('similarity_list', [])\n",
    "        \n",
    "        # Extract top-k candidates\n",
    "        top_candidates = []\n",
    "        similarity_scores = []\n",
    "        candidate_movement_count = [0, 0]  # [rise, fall]\n",
    "        \n",
    "        for candidate_info in similarity_list[:k]:\n",
    "            candidate_index = candidate_info['candidate_index']\n",
    "            candidate_score = candidate_info['candidate_score']\n",
    "            \n",
    "            # Get candidate data\n",
    "            if candidate_index in self.candidates:\n",
    "                candidate_data = self.candidates[candidate_index]\n",
    "                top_candidates.append(candidate_data)\n",
    "                similarity_scores.append(candidate_score)\n",
    "                \n",
    "                # Track movement distribution\n",
    "                movement = candidate_data.get('movement', 'unknown')\n",
    "                if movement == 'rise':\n",
    "                    candidate_movement_count[0] += 1\n",
    "                elif movement == 'fall':\n",
    "                    candidate_movement_count[1] += 1\n",
    "        \n",
    "        return top_candidates, similarity_scores, candidate_movement_count\n",
    "    \n",
    "    def generate_finquest_prompt(self, \n",
    "                                query: Dict, \n",
    "                                candidates: List[Dict], \n",
    "                                similarity_scores: List[float]) -> str:\n",
    "        \"\"\"\n",
    "        Generate prompt with FinQuest similarity-retrieved candidates\n",
    "        Uses same format as your working similarity search\n",
    "        \"\"\"\n",
    "        query_stock = query.get('query_stock', 'Unknown')\n",
    "        query_date = query.get('query_date', 'Unknown')\n",
    "        \n",
    "        instruction = (\n",
    "            \"Based on the following information, predict stock movement by filling in the [blank] with 'rise' or 'fall'. \"\n",
    "            \"Just fill in the blank, do not explain.\\n\"\n",
    "        )\n",
    "        \n",
    "        retrieve_prompt = 'These are sequences that may affect this stock\\'s price recently, where similarity score shows the similarity to the query sequence:\\n'\n",
    "        \n",
    "        # Format similarity-retrieved candidates\n",
    "        candidate_text = \"\"\n",
    "        \n",
    "        for candidate, score in zip(candidates, similarity_scores):\n",
    "            # Format candidate sequence (matching your working format)\n",
    "            candidate_sequence = {\n",
    "                'candidate_stock': candidate.get('candidate_stock', 'Unknown'),\n",
    "                'candidate_date': candidate.get('candidate_date', 'Unknown'),\n",
    "                'recent_date_list': candidate.get('recent_date_list', []),\n",
    "                'adjusted_close_list': candidate.get('adjusted_close_list', [])\n",
    "            }\n",
    "            \n",
    "            candidate_text += str({\n",
    "                'candidate_sequence': candidate_sequence,\n",
    "                'similarity_score': round(score, 4)\n",
    "            }) + '\\n'\n",
    "        \n",
    "        # Query section\n",
    "        query_prompt = 'This is the query sequence:\\n'\n",
    "        query_sequence = {\n",
    "            'query_stock': query_stock,\n",
    "            'query_date': query_date,\n",
    "            'recent_date_list': query.get('recent_date_list', []),\n",
    "            'adjusted_close_list': query.get('adjusted_close_list', [])\n",
    "        }\n",
    "        \n",
    "        query_instruction = f'\\nQuery: On {query_date}, the movement of ${query_stock} is [blank].\\n'\n",
    "        \n",
    "        # Combine all parts\n",
    "        full_prompt = instruction + retrieve_prompt + candidate_text + '\\n' + query_prompt + str(query_sequence) + '\\n' + query_instruction\n",
    "        \n",
    "        return full_prompt\n",
    "    \n",
    "    def ask_llm(self, prompt: str) -> str:\n",
    "        \"\"\"Get prediction from current LLM with appropriate context length\"\"\"\n",
    "        if self.current_llm is None:\n",
    "            raise RuntimeError(\"No LLM loaded\")\n",
    "        \n",
    "        config = self.llm_configs[self.current_llm_name]\n",
    "        max_length = config.get('max_length', 1024)\n",
    "        \n",
    "        # Format prompt based on LLM architecture\n",
    "        if 'Llama' in self.current_llm_name:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst. Use the similar historical patterns to predict stock movements accurately.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            formatted_prompt = self.current_tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt = f\"System: You are a financial analyst.\\nUser: {prompt}\\nAssistant:\"\n",
    "        \n",
    "        # Tokenize with appropriate max length\n",
    "        input_ids = self.current_tokenizer.encode(\n",
    "            formatted_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_llm.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.current_tokenizer.eos_token_id,\n",
    "                eos_token_id=self.current_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "        response = self.current_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def extract_prediction(self, response: str, reference: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Extract rise/fall prediction from LLM response\"\"\"\n",
    "        response_clean = response.lower().strip()\n",
    "        reference_clean = reference.lower().strip()\n",
    "        \n",
    "        if 'rise' in response_clean:\n",
    "            prediction = 'rise'\n",
    "        elif 'fall' in response_clean:\n",
    "            prediction = 'fall'\n",
    "        else:\n",
    "            prediction = 'freeze'\n",
    "        \n",
    "        correct = (prediction == reference_clean)\n",
    "        return prediction, correct\n",
    "    \n",
    "    def run_single_llm_experiment(self, llm_name: str, output_dir: str, k_candidates: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Run FinQuest similarity retrieval experiment for a single LLM\"\"\"\n",
    "        logger.info(f\"ðŸš€ Starting FINQUEST SIMILARITY RETRIEVAL experiment with {llm_name} (k={k_candidates})\")\n",
    "        \n",
    "        # Load the specified LLM\n",
    "        self.load_llm(llm_name)\n",
    "        \n",
    "        results = []\n",
    "        processed_count = 0\n",
    "        correct_count = 0\n",
    "        queries_with_candidates = 0\n",
    "        queries_without_candidates = 0\n",
    "        \n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_id = query['query_id']\n",
    "            ground_truth = self.gt_lookup.get(query_id)\n",
    "            \n",
    "            if not ground_truth:\n",
    "                continue\n",
    "            \n",
    "            reference_answer = ground_truth['actual_movement']\n",
    "            \n",
    "            # Skip freeze movements\n",
    "            if reference_answer == 'freeze':\n",
    "                continue\n",
    "            \n",
    "            # Progress logging\n",
    "            if (i + 1) % 50 == 0:\n",
    "                accuracy = correct_count / processed_count if processed_count > 0 else 0\n",
    "                candidate_coverage = queries_with_candidates / (queries_with_candidates + queries_without_candidates) if (queries_with_candidates + queries_without_candidates) > 0 else 0\n",
    "                logger.info(f\"Progress: {i+1}/{len(self.test_queries)} | Accuracy: {accuracy:.3f} | Candidate Coverage: {candidate_coverage:.3f}\")\n",
    "            \n",
    "            try:\n",
    "                # Get FinQuest similarity-based candidates\n",
    "                candidates, similarity_scores, candidate_movement_count = self.get_finquest_candidates(query, k_candidates)\n",
    "                \n",
    "                if len(candidates) == 0:\n",
    "                    logger.warning(f\"No FinQuest candidates found for query {query_id}\")\n",
    "                    queries_without_candidates += 1\n",
    "                    continue\n",
    "                \n",
    "                queries_with_candidates += 1\n",
    "                \n",
    "                # Generate prompt with similarity-retrieved candidates\n",
    "                prompt = self.generate_finquest_prompt(query, candidates, similarity_scores)\n",
    "                \n",
    "                # Get LLM prediction\n",
    "                llm_response = self.ask_llm(prompt)\n",
    "                \n",
    "                # Extract and validate prediction\n",
    "                prediction, correct = self.extract_prediction(llm_response, reference_answer)\n",
    "                \n",
    "                # Track accuracy\n",
    "                processed_count += 1\n",
    "                if correct:\n",
    "                    correct_count += 1\n",
    "                \n",
    "                # Store detailed result\n",
    "                result = {\n",
    "                    'llm_name': llm_name,\n",
    "                    'query_id': query_id,\n",
    "                    'query_stock': query.get('query_stock', ''),\n",
    "                    'query_date': query.get('query_date', ''),\n",
    "                    'method': 'finquest_similarity_retrieval',\n",
    "                    'prompt': prompt,\n",
    "                    'llm_response': llm_response,\n",
    "                    'prediction': prediction,\n",
    "                    'reference': reference_answer,\n",
    "                    'correct': correct,\n",
    "                    'candidate_count': len(candidates),\n",
    "                    'candidate_movement_dist': str(candidate_movement_count),\n",
    "                    'similarity_scores': str([round(s, 4) for s in similarity_scores]),\n",
    "                    'avg_similarity_score': np.mean(similarity_scores) if similarity_scores else 0,\n",
    "                    'max_similarity_score': max(similarity_scores) if similarity_scores else 0,\n",
    "                    'min_similarity_score': min(similarity_scores) if similarity_scores else 0,\n",
    "                    'k_candidates': k_candidates\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing query {query_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save detailed results\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f'{llm_name}_finquest_similarity_k{k_candidates}_results.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        total_predictions = len(df)\n",
    "        overall_accuracy = df['correct'].mean() if total_predictions > 0 else 0\n",
    "        \n",
    "        # Class-specific metrics\n",
    "        rise_df = df[df['reference'] == 'rise']\n",
    "        fall_df = df[df['reference'] == 'fall']\n",
    "        \n",
    "        rise_accuracy = rise_df['correct'].mean() if len(rise_df) > 0 else 0\n",
    "        fall_accuracy = fall_df['correct'].mean() if len(fall_df) > 0 else 0\n",
    "        \n",
    "        # Candidate and similarity statistics\n",
    "        avg_candidates_used = df['candidate_count'].mean() if total_predictions > 0 else 0\n",
    "        avg_similarity = df['avg_similarity_score'].mean() if total_predictions > 0 else 0\n",
    "        \n",
    "        # Coverage statistics\n",
    "        candidate_coverage = queries_with_candidates / (queries_with_candidates + queries_without_candidates) if (queries_with_candidates + queries_without_candidates) > 0 else 0\n",
    "        \n",
    "        # Log comprehensive results\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"FINQUEST SIMILARITY RETRIEVAL RESULTS - {llm_name}\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Total Predictions: {total_predictions}\")\n",
    "        logger.info(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        logger.info(f\"Rise Accuracy: {rise_accuracy:.4f} ({len(rise_df)} samples)\")\n",
    "        logger.info(f\"Fall Accuracy: {fall_accuracy:.4f} ({len(fall_df)} samples)\")\n",
    "        logger.info(f\"Candidate Coverage: {candidate_coverage:.4f} ({queries_with_candidates}/{queries_with_candidates + queries_without_candidates})\")\n",
    "        logger.info(f\"Avg Candidates Used: {avg_candidates_used:.1f}\")\n",
    "        logger.info(f\"Avg Similarity Score: {avg_similarity:.4f}\")\n",
    "        logger.info(f\"Results saved to: {output_file}\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def run_multi_llm_experiment(self, llm_list: List[str], output_dir: str, k_candidates: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Run FinQuest similarity retrieval experiment across multiple LLMs\"\"\"\n",
    "        logger.info(f\"ðŸš€ Starting MULTI-LLM FINQUEST SIMILARITY RETRIEVAL experiment\")\n",
    "        logger.info(f\"Testing LLMs: {llm_list}\")\n",
    "        logger.info(f\"Candidates per query: {k_candidates}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for llm_name in llm_list:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"TESTING LLM: {llm_name}\")\n",
    "            logger.info(f\"Description: {self.llm_configs[llm_name]['description']}\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                # Run experiment for this LLM\n",
    "                llm_results = self.run_single_llm_experiment(llm_name, output_dir, k_candidates)\n",
    "                all_results.append(llm_results)\n",
    "                \n",
    "                logger.info(f\"âœ… Completed {llm_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed {llm_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if all_results:\n",
    "            combined_df = pd.concat(all_results, ignore_index=True)\n",
    "            \n",
    "            # Save combined results\n",
    "            combined_file = os.path.join(output_dir, f'all_llms_finquest_similarity_k{k_candidates}_combined.csv')\n",
    "            combined_df.to_csv(combined_file, index=False)\n",
    "            \n",
    "            # Generate comprehensive comparison report\n",
    "            self._generate_comprehensive_comparison_report(combined_df, output_dir, k_candidates)\n",
    "            \n",
    "            # Analyze similarity score patterns\n",
    "            self._analyze_similarity_patterns(combined_df, output_dir)\n",
    "            \n",
    "            logger.info(f\"\\nâœ… Multi-LLM FinQuest similarity retrieval experiment completed!\")\n",
    "            logger.info(f\"Combined results saved to: {combined_file}\")\n",
    "            \n",
    "            return combined_df\n",
    "        else:\n",
    "            logger.error(\"âŒ No successful experiments!\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def run_k_ablation_study(self, llm_name: str, output_dir: str, k_values: List[int] = [1, 3, 5, 10, 15]) -> pd.DataFrame:\n",
    "        \"\"\"Run ablation study on number of FinQuest similarity candidates\"\"\"\n",
    "        logger.info(f\"ðŸ”¬ Starting FINQUEST K-ABLATION STUDY for {llm_name}\")\n",
    "        logger.info(f\"Testing k values: {k_values}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for k in k_values:\n",
    "            logger.info(f\"\\n{'='*40}\")\n",
    "            logger.info(f\"TESTING k={k} FinQuest candidates\")\n",
    "            logger.info(f\"{'='*40}\")\n",
    "            \n",
    "            try:\n",
    "                # Run experiment with k candidates\n",
    "                k_results = self.run_single_llm_experiment(llm_name, output_dir, k)\n",
    "                all_results.append(k_results)\n",
    "                \n",
    "                logger.info(f\"âœ… Completed k={k}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed k={k}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Analyze k-ablation results\n",
    "        if all_results:\n",
    "            self._analyze_finquest_k_ablation(all_results, llm_name, output_dir, k_values)\n",
    "            \n",
    "            combined_df = pd.concat(all_results, ignore_index=True)\n",
    "            return combined_df\n",
    "        else:\n",
    "            logger.error(\"âŒ FinQuest K-ablation study failed!\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _analyze_finquest_k_ablation(self, results_list: List[pd.DataFrame], llm_name: str, output_dir: str, k_values: List[int]):\n",
    "        \"\"\"Analyze effect of different k values for FinQuest similarity retrieval\"\"\"\n",
    "        \n",
    "        k_analysis = []\n",
    "        \n",
    "        for i, df in enumerate(results_list):\n",
    "            k = k_values[i]\n",
    "            \n",
    "            # Basic accuracy metrics\n",
    "            accuracy = df['correct'].mean()\n",
    "            rise_acc = df[df['reference'] == 'rise']['correct'].mean()\n",
    "            fall_acc = df[df['reference'] == 'fall']['correct'].mean()\n",
    "            \n",
    "            # Similarity-specific metrics\n",
    "            avg_similarity = df['avg_similarity_score'].mean()\n",
    "            max_similarity_avg = df['max_similarity_score'].mean()\n",
    "            \n",
    "            k_analysis.append({\n",
    "                'k_candidates': k,\n",
    "                'overall_accuracy': accuracy,\n",
    "                'rise_accuracy': rise_acc,\n",
    "                'fall_accuracy': fall_acc,\n",
    "                'total_predictions': len(df),\n",
    "                'avg_similarity_score': avg_similarity,\n",
    "                'avg_max_similarity': max_similarity_avg\n",
    "            })\n",
    "        \n",
    "        # Save k-ablation analysis\n",
    "        k_df = pd.DataFrame(k_analysis)\n",
    "        k_file = os.path.join(output_dir, f'{llm_name}_finquest_k_ablation_analysis.csv')\n",
    "        k_df.to_csv(k_file, index=False)\n",
    "        \n",
    "        # Print detailed analysis\n",
    "        logger.info(f\"\\nFINQUEST K-ABLATION ANALYSIS - {llm_name}\")\n",
    "        logger.info(\"=\"*80)\n",
    "        for result in k_analysis:\n",
    "            logger.info(f\"k={result['k_candidates']:2d} | Accuracy: {result['overall_accuracy']:.4f} | \"\n",
    "                       f\"Rise: {result['rise_accuracy']:.3f} | Fall: {result['fall_accuracy']:.3f} | \"\n",
    "                       f\"Avg Sim: {result['avg_similarity_score']:.3f}\")\n",
    "        \n",
    "        logger.info(f\"\\nFinQuest K-ablation analysis saved to: {k_file}\")\n",
    "    \n",
    "    def _analyze_similarity_patterns(self, combined_df: pd.DataFrame, output_dir: str):\n",
    "        \"\"\"Analyze patterns in similarity scores vs. prediction accuracy\"\"\"\n",
    "        \n",
    "        # Similarity score binning analysis\n",
    "        similarity_bins = [0.0, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "        combined_df['similarity_bin'] = pd.cut(combined_df['avg_similarity_score'], \n",
    "                                             bins=similarity_bins, \n",
    "                                             labels=['0.0-0.3', '0.3-0.5', '0.5-0.7', '0.7-0.9', '0.9-1.0'])\n",
    "        \n",
    "        similarity_analysis = []\n",
    "        \n",
    "        for bin_name in ['0.0-0.3', '0.3-0.5', '0.5-0.7', '0.7-0.9', '0.9-1.0']:\n",
    "            bin_df = combined_df[combined_df['similarity_bin'] == bin_name]\n",
    "            \n",
    "            if len(bin_df) > 0:\n",
    "                similarity_analysis.append({\n",
    "                    'similarity_range': bin_name,\n",
    "                    'count': len(bin_df),\n",
    "                    'accuracy': bin_df['correct'].mean(),\n",
    "                    'avg_similarity': bin_df['avg_similarity_score'].mean()\n",
    "                })\n",
    "        \n",
    "        # Save similarity analysis\n",
    "        sim_df = pd.DataFrame(similarity_analysis)\n",
    "        sim_file = os.path.join(output_dir, 'finquest_similarity_score_analysis.csv')\n",
    "        sim_df.to_csv(sim_file, index=False)\n",
    "        \n",
    "        logger.info(\"\\nFINQUEST SIMILARITY SCORE ANALYSIS\")\n",
    "        logger.info(\"=\"*50)\n",
    "        for result in similarity_analysis:\n",
    "            logger.info(f\"Similarity {result['similarity_range']} | Count: {result['count']:4d} | \"\n",
    "                       f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "        \n",
    "        logger.info(f\"\\nSimilarity analysis saved to: {sim_file}\")\n",
    "    \n",
    "    def _generate_comprehensive_comparison_report(self, combined_df: pd.DataFrame, output_dir: str, k_candidates: int):\n",
    "        \"\"\"Generate comprehensive comparison report across LLMs with FinQuest-specific metrics\"\"\"\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        for llm_name in combined_df['llm_name'].unique():\n",
    "            llm_df = combined_df[combined_df['llm_name'] == llm_name]\n",
    "            \n",
    "            # Basic accuracy metrics\n",
    "            total = len(llm_df)\n",
    "            accuracy = llm_df['correct'].mean()\n",
    "            \n",
    "            rise_df = llm_df[llm_df['reference'] == 'rise']\n",
    "            fall_df = llm_df[llm_df['reference'] == 'fall']\n",
    "            \n",
    "            rise_acc = rise_df['correct'].mean() if len(rise_df) > 0 else 0\n",
    "            fall_acc = fall_df['correct'].mean() if len(fall_df) > 0 else 0\n",
    "            \n",
    "            # Prediction distribution\n",
    "            pred_dist = llm_df['prediction'].value_counts()\n",
    "            \n",
    "            # FinQuest-specific metrics\n",
    "            avg_candidates = llm_df['candidate_count'].mean()\n",
    "            avg_similarity = llm_df['avg_similarity_score'].mean()\n",
    "            max_similarity_avg = llm_df['max_similarity_score'].mean()\n",
    "            \n",
    "            # High similarity predictions (>0.7 avg similarity)\n",
    "            high_sim_df = llm_df[llm_df['avg_similarity_score'] > 0.7]\n",
    "            high_sim_accuracy = high_sim_df['correct'].mean() if len(high_sim_df) > 0 else 0\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'llm_name': llm_name,\n",
    "                'model_description': self.llm_configs[llm_name]['description'],\n",
    "                'total_predictions': total,\n",
    "                'overall_accuracy': accuracy,\n",
    "                'rise_accuracy': rise_acc,\n",
    "                'fall_accuracy': fall_acc,\n",
    "                'rise_predictions': len(rise_df),\n",
    "                'fall_predictions': len(fall_df),\n",
    "                'predicted_rise': pred_dist.get('rise', 0),\n",
    "                'predicted_fall': pred_dist.get('fall', 0),\n",
    "                'predicted_freeze': pred_dist.get('freeze', 0),\n",
    "                'avg_candidates_used': avg_candidates,\n",
    "                'avg_similarity_score': avg_similarity,\n",
    "                'avg_max_similarity': max_similarity_avg,\n",
    "                'high_similarity_accuracy': high_sim_accuracy,\n",
    "                'high_similarity_count': len(high_sim_df),\n",
    "                'k_candidates': k_candidates,\n",
    "                'method': 'finquest_similarity_retrieval'\n",
    "            })\n",
    "        \n",
    "        # Save comprehensive comparison\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        comparison_file = os.path.join(output_dir, f'finquest_similarity_k{k_candidates}_comprehensive_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_file, index=False)\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        logger.info(\"\\n\" + \"=\"*100)\n",
    "        logger.info(f\"FINQUEST SIMILARITY RETRIEVAL (k={k_candidates}) - COMPREHENSIVE LLM COMPARISON\")\n",
    "        logger.info(\"=\"*100)\n",
    "        \n",
    "        for result in comparison_results:\n",
    "            logger.info(f\"{result['llm_name']:15} | Accuracy: {result['overall_accuracy']:.4f} | \"\n",
    "                       f\"Rise: {result['rise_accuracy']:.3f} | Fall: {result['fall_accuracy']:.3f} | \"\n",
    "                       f\"AvgSim: {result['avg_similarity_score']:.3f} | \"\n",
    "                       f\"HighSim: {result['high_similarity_accuracy']:.3f}({result['high_similarity_count']})\")\n",
    "        \n",
    "        logger.info(f\"\\nComprehensive comparison saved to: {comparison_file}\")\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "def run_finquest_similarity_experiment():\n",
    "    \"\"\"Main function to run the FinQuest similarity retrieval experiment\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    test_queries_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/test_queries_rise_fall_only.json'\n",
    "    ground_truth_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/ground_truth_rise_fall_only.json'\n",
    "    embeddings_dir = '/root/nfs/AJ FinRag/Embeddings/embeddings/test/FinQuest'\n",
    "    similarity_results_file = 'similar_candidates/test/FinQuest/test_similarity_results_FinQuest.pkl'\n",
    "    output_dir = 'finquest_similarity_experiments'\n",
    "    \n",
    "    # LLMs to test\n",
    "    test_llms = [\n",
    "        'StockLLM',      # Specialized financial model - should work best\n",
    "        'Qwen2.5-1.5B',   # Small general model\n",
    "        'Llama3.2-3B'    # Medium general model - good balance\n",
    "    ]\n",
    "    \n",
    "    # Experimental parameters\n",
    "    k_candidates = 5  # Number of similarity-based candidates to retrieve\n",
    "    \n",
    "    try:\n",
    "        # Initialize experiment\n",
    "        experiment = FinQuestSimilarityExperiment(\n",
    "            test_queries_file, \n",
    "            ground_truth_file, \n",
    "            embeddings_dir,\n",
    "            similarity_results_file\n",
    "        )\n",
    "        \n",
    "        # Option 1: Run multi-LLM experiment with fixed k\n",
    "        logger.info(\"ðŸš€ RUNNING MULTI-LLM FINQUEST SIMILARITY EXPERIMENT\")\n",
    "        results = experiment.run_multi_llm_experiment(test_llms, output_dir, k_candidates)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            logger.info(\"ðŸŽ‰ FINQUEST SIMILARITY EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "            logger.info(f\"ðŸ“Š Total results: {len(results)} predictions across {len(test_llms)} LLMs\")\n",
    "        \n",
    "        # Option 2: Run k-ablation study (uncomment to run)\n",
    "        # logger.info(\"ðŸ”¬ RUNNING FINQUEST K-ABLATION STUDY\")\n",
    "        # ablation_results = experiment.run_k_ablation_study('StockLLM', output_dir, [1, 3, 5, 10, 15])\n",
    "        \n",
    "        # Option 3: Deep analysis of best performing LLM (uncomment to run)\n",
    "        # best_llm = 'StockLLM'  # Choose based on initial results\n",
    "        # logger.info(f\"ðŸ” RUNNING DETAILED ANALYSIS FOR {best_llm}\")\n",
    "        # detailed_results = experiment.run_single_llm_experiment(best_llm, output_dir, 10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_finquest_similarity_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2d582a-a0ed-46ff-b670-743002c416de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” ANALYZING STOCKLLM RESPONSE PATTERNS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š NO_RETRIEVAL\n",
      "----------------------------------------\n",
      "Total predictions: 2642\n",
      "Prediction distribution: {'rise': 2640, 'fall': 2}\n",
      "Reference distribution: {'rise': 1382, 'fall': 1260}\n",
      "\n",
      "Sample LLM responses:\n",
      "  1. 'rise.' -> rise (ref: rise)\n",
      "  2. 'rise.' -> rise (ref: fall)\n",
      "  3. 'rise.' -> rise (ref: rise)\n",
      "  4. 'rise.' -> rise (ref: rise)\n",
      "  5. 'rise.' -> rise (ref: fall)\n",
      "\n",
      "Unique responses: 8/2642 (0.3%)\n",
      "\n",
      "Most common responses:\n",
      "  'rise.': 2175 times (82.3%)\n",
      "  'rise\n",
      "\n",
      "Query: On 2025-02': 165 times (6.2%)\n",
      "  'rise\n",
      "\n",
      "Query: On 2025-04': 125 times (4.7%)\n",
      "  'rise\n",
      "\n",
      "Query: On 2025-03': 120 times (4.5%)\n",
      "  'rise\n",
      "\n",
      "Query: On 2025-01': 48 times (1.8%)\n",
      "  'rise': 5 times (0.2%)\n",
      "  'rise\n",
      "\n",
      "Query: On 2025-05': 2 times (0.1%)\n",
      "  'fall.': 2 times (0.1%)\n",
      "\n",
      "ðŸ“Š RANDOM_RETRIEVAL\n",
      "----------------------------------------\n",
      "Total predictions: 2642\n",
      "Prediction distribution: {'rise': 2611, 'fall': 31}\n",
      "Reference distribution: {'rise': 1382, 'fall': 1260}\n",
      "\n",
      "Sample LLM responses:\n",
      "  1. 'rise' -> rise (ref: rise)\n",
      "  2. 'rise' -> rise (ref: fall)\n",
      "  3. 'rise' -> rise (ref: rise)\n",
      "  4. 'rise' -> rise (ref: rise)\n",
      "  5. 'rise' -> rise (ref: fall)\n",
      "\n",
      "Unique responses: 4/2642 (0.2%)\n",
      "\n",
      "Most common responses:\n",
      "  'rise': 2557 times (96.8%)\n",
      "  'rise.': 54 times (2.0%)\n",
      "  'fall.': 29 times (1.1%)\n",
      "  'Fall.': 2 times (0.1%)\n",
      "\n",
      "ðŸ“Š FINQUEST_SIMILARITY\n",
      "----------------------------------------\n",
      "Total predictions: 2642\n",
      "Prediction distribution: {'rise': 2210, 'fall': 401, 'freeze': 31}\n",
      "Reference distribution: {'rise': 1382, 'fall': 1260}\n",
      "\n",
      "Sample LLM responses:\n",
      "  1. 'rise.' -> rise (ref: rise)\n",
      "  2. 'rise' -> rise (ref: fall)\n",
      "  3. 'rise' -> rise (ref: rise)\n",
      "  4. 'rise' -> rise (ref: rise)\n",
      "  5. 'rise' -> rise (ref: fall)\n",
      "\n",
      "Unique responses: 44/2642 (1.7%)\n",
      "\n",
      "Most common responses:\n",
      "  'rise': 1472 times (55.7%)\n",
      "  'rise.': 478 times (18.1%)\n",
      "  'Fall.': 279 times (10.6%)\n",
      "  'Rise.': 135 times (5.1%)\n",
      "  'fall.': 72 times (2.7%)\n",
      "  'Rise. The stock price of $HON on': 19 times (0.7%)\n",
      "  'Rise. The stock price of Oracle Corporation (OR': 18 times (0.7%)\n",
      "  'The movement of $NFLX on 2025': 18 times (0.7%)\n",
      "  'Rise. The stock price of $ABT on': 14 times (0.5%)\n",
      "  'The query sequence shows that on 2025-': 13 times (0.5%)\n",
      "\n",
      "ðŸ” DEBUGGING PROMPT QUALITY\n",
      "==================================================\n",
      "\n",
      "Query 1: AAPL on 2025-01-17\n",
      "Top candidates:\n",
      "  1. Index: 44, Score: 0.999997\n",
      "      Stock: XOM, Date: 2022-01-24\n",
      "  2. Index: 45, Score: 0.999997\n",
      "      Stock: XOM, Date: 2022-01-24\n",
      "  3. Index: 46, Score: 0.999997\n",
      "      Stock: XOM, Date: 2022-01-24\n",
      "\n",
      "Query 2: AAPL on 2025-01-21\n",
      "Top candidates:\n",
      "  1. Index: 2750, Score: 0.999997\n",
      "      Stock: XOM, Date: 2023-01-17\n",
      "  2. Index: 2751, Score: 0.999997\n",
      "      Stock: XOM, Date: 2023-01-17\n",
      "  3. Index: 2752, Score: 0.999997\n",
      "      Stock: XOM, Date: 2023-01-17\n",
      "\n",
      "Query 3: AAPL on 2025-01-27\n",
      "Top candidates:\n",
      "  1. Index: 2750, Score: 0.999997\n",
      "      Stock: XOM, Date: 2023-01-17\n",
      "  2. Index: 2751, Score: 0.999997\n",
      "      Stock: XOM, Date: 2023-01-17\n",
      "  3. Index: 2752, Score: 0.999997\n",
      "      Stock: XOM, Date: 2023-01-17\n",
      "\n",
      "Query 4: AAPL on 2025-01-28\n",
      "Top candidates:\n",
      "  1. Index: 44, Score: 0.999996\n",
      "      Stock: XOM, Date: 2022-01-24\n",
      "  2. Index: 45, Score: 0.999996\n",
      "      Stock: XOM, Date: 2022-01-24\n",
      "  3. Index: 46, Score: 0.999996\n",
      "      Stock: XOM, Date: 2022-01-24\n",
      "\n",
      "Query 5: AAPL on 2025-01-30\n",
      "Top candidates:\n",
      "  1. Index: 4862, Score: 0.999996\n",
      "      Stock: XOM, Date: 2023-10-20\n",
      "  2. Index: 4863, Score: 0.999996\n",
      "      Stock: XOM, Date: 2023-10-20\n",
      "  3. Index: 4864, Score: 0.999996\n",
      "      Stock: XOM, Date: 2023-10-20\n"
     ]
    }
   ],
   "source": [
    "# Debug script to investigate StockLLM's response bias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def analyze_stockllm_responses(results_files):\n",
    "    \"\"\"Analyze StockLLM responses across different methods\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” ANALYZING STOCKLLM RESPONSE PATTERNS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    methods = ['no_retrieval', 'random_retrieval', 'finquest_similarity']\n",
    "    \n",
    "    for method in methods:\n",
    "        if method in results_files and os.path.exists(results_files[method]):\n",
    "            print(f\"\\nðŸ“Š {method.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            df = pd.read_csv(results_files[method])\n",
    "            \n",
    "            # Response analysis\n",
    "            response_sample = df['llm_response'].head(20).tolist()\n",
    "            prediction_dist = df['prediction'].value_counts()\n",
    "            reference_dist = df['reference'].value_counts()\n",
    "            \n",
    "            print(f\"Total predictions: {len(df)}\")\n",
    "            print(f\"Prediction distribution: {dict(prediction_dist)}\")\n",
    "            print(f\"Reference distribution: {dict(reference_dist)}\")\n",
    "            \n",
    "            # Sample responses\n",
    "            print(f\"\\nSample LLM responses:\")\n",
    "            for i, response in enumerate(response_sample[:5]):\n",
    "                ref = df.iloc[i]['reference']\n",
    "                pred = df.iloc[i]['prediction']\n",
    "                print(f\"  {i+1}. '{response}' -> {pred} (ref: {ref})\")\n",
    "            \n",
    "            # Check for repetitive responses\n",
    "            unique_responses = df['llm_response'].nunique()\n",
    "            print(f\"\\nUnique responses: {unique_responses}/{len(df)} ({unique_responses/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            # Most common responses\n",
    "            common_responses = df['llm_response'].value_counts().head(10)\n",
    "            print(f\"\\nMost common responses:\")\n",
    "            for response, count in common_responses.items():\n",
    "                print(f\"  '{response}': {count} times ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "def debug_prompt_quality(similarity_results_file, sample_size=5):\n",
    "    \"\"\"Check if prompts contain meaningful candidate differences\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ” DEBUGGING PROMPT QUALITY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load similarity results\n",
    "    with open(similarity_results_file, 'rb') as f:\n",
    "        similarity_results = pickle.load(f)\n",
    "    \n",
    "    for i in range(min(sample_size, len(similarity_results))):\n",
    "        result = similarity_results[i]\n",
    "        query_stock = result['query_stock']\n",
    "        query_date = result['query_date']\n",
    "        candidates = result.get('similarity_list', [])[:3]  # Top 3\n",
    "        \n",
    "        print(f\"\\nQuery {i+1}: {query_stock} on {query_date}\")\n",
    "        \n",
    "        if candidates:\n",
    "            print(\"Top candidates:\")\n",
    "            for j, candidate in enumerate(candidates):\n",
    "                score = candidate.get('candidate_score', 0)\n",
    "                index = candidate.get('candidate_index', 'unknown')\n",
    "                print(f\"  {j+1}. Index: {index}, Score: {score:.6f}\")\n",
    "                \n",
    "                # Check if we have actual candidate data\n",
    "                if 'candidate_data' in candidate:\n",
    "                    cand_stock = candidate['candidate_data'].get('candidate_stock', 'unknown')\n",
    "                    cand_date = candidate['candidate_data'].get('candidate_date', 'unknown')\n",
    "                    print(f\"      Stock: {cand_stock}, Date: {cand_date}\")\n",
    "        else:\n",
    "            print(\"  No candidates found!\")\n",
    "\n",
    "# Usage\n",
    "results_files = {\n",
    "    'no_retrieval': 'no_retrieval_experiments/StockLLM_no_retrieval_results.csv',\n",
    "    'random_retrieval': 'random_retrieval_experiments/StockLLM_random_retrieval_k5_results.csv',\n",
    "    'finquest_similarity': 'finquest_similarity_experiments/StockLLM_finquest_similarity_k5_results.csv'\n",
    "}\n",
    "\n",
    "analyze_stockllm_responses(results_files)\n",
    "debug_prompt_quality('similar_candidates/test/FinQuest/test_similarity_results_FinQuest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c16a91-b894-4eb6-9566-6c27d6763ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TRACING FINQUEST DATA PIPELINE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‚ STEP 1: DATA SOURCES ANALYSIS\n",
      "--------------------------------------------------\n",
      "âœ… Test queries: 2642\n",
      "âœ… Ground truth: 2642\n",
      "Test date range: 2025-01-17 to 2025-04-29\n",
      "Test stocks: 50 unique stocks\n",
      "Sample test stocks: ['PEP', 'CVX', 'JNJ', 'HD', 'AXP', 'MRK', 'COP', 'CAT', 'AMZN', 'INTC']\n",
      "\n",
      "ðŸ“‚ STEP 2: CANDIDATE POOL ANALYSIS\n",
      "--------------------------------------------------\n",
      "Found 75 candidate embedding files\n",
      "âœ… Total candidates: 8151\n",
      "Candidate date range: 2022-01-18 to 2024-12-27\n",
      "Candidate stocks: 50 unique stocks\n",
      "Sample candidate stocks: ['CVX', 'PEP', 'JNJ', 'HD', 'AXP', 'COP', 'MRK', 'CAT', 'AMZN', 'INTC']\n",
      "Stock overlap: 50/50 test stocks also in candidates\n",
      "Date overlap: 0 dates appear in both test and candidate sets\n",
      "\n",
      "ðŸ“‚ STEP 3: SIMILARITY RESULTS ANALYSIS\n",
      "--------------------------------------------------\n",
      "âœ… Similarity results: 2642 queries\n",
      "\n",
      "ðŸ” DETAILED ANALYSIS OF FIRST 5 SIMILARITY RESULTS:\n",
      "\n",
      "--- Query 1: AAPL on 2025-01-17 ---\n",
      "Candidates found: 10\n",
      "  Candidate 1:\n",
      "    Index: 44\n",
      "    Score: 0.9999971986\n",
      "    Stock: XOM\n",
      "    Date: 2022-01-24\n",
      "    Movement: Unknown\n",
      "  Candidate 2:\n",
      "    Index: 45\n",
      "    Score: 0.9999971986\n",
      "    Stock: XOM\n",
      "    Date: 2022-01-24\n",
      "    Movement: Unknown\n",
      "  Candidate 3:\n",
      "    Index: 46\n",
      "    Score: 0.9999971986\n",
      "    Stock: XOM\n",
      "    Date: 2022-01-24\n",
      "    Movement: Unknown\n",
      "\n",
      "--- Query 2: AAPL on 2025-01-21 ---\n",
      "Candidates found: 10\n",
      "  Candidate 1:\n",
      "    Index: 2750\n",
      "    Score: 0.9999966621\n",
      "    Stock: XOM\n",
      "    Date: 2023-01-17\n",
      "    Movement: Unknown\n",
      "  Candidate 2:\n",
      "    Index: 2751\n",
      "    Score: 0.9999966621\n",
      "    Stock: XOM\n",
      "    Date: 2023-01-17\n",
      "    Movement: Unknown\n",
      "  Candidate 3:\n",
      "    Index: 2752\n",
      "    Score: 0.9999966621\n",
      "    Stock: XOM\n",
      "    Date: 2023-01-17\n",
      "    Movement: Unknown\n",
      "\n",
      "--- Query 3: AAPL on 2025-01-27 ---\n",
      "Candidates found: 10\n",
      "  Candidate 1:\n",
      "    Index: 2750\n",
      "    Score: 0.9999974370\n",
      "    Stock: XOM\n",
      "    Date: 2023-01-17\n",
      "    Movement: Unknown\n",
      "  Candidate 2:\n",
      "    Index: 2751\n",
      "    Score: 0.9999974370\n",
      "    Stock: XOM\n",
      "    Date: 2023-01-17\n",
      "    Movement: Unknown\n",
      "  Candidate 3:\n",
      "    Index: 2752\n",
      "    Score: 0.9999974370\n",
      "    Stock: XOM\n",
      "    Date: 2023-01-17\n",
      "    Movement: Unknown\n",
      "\n",
      "--- Query 4: AAPL on 2025-01-28 ---\n",
      "Candidates found: 10\n",
      "  Candidate 1:\n",
      "    Index: 44\n",
      "    Score: 0.9999961257\n",
      "    Stock: XOM\n",
      "    Date: 2022-01-24\n",
      "    Movement: Unknown\n",
      "  Candidate 2:\n",
      "    Index: 45\n",
      "    Score: 0.9999961257\n",
      "    Stock: XOM\n",
      "    Date: 2022-01-24\n",
      "    Movement: Unknown\n",
      "  Candidate 3:\n",
      "    Index: 46\n",
      "    Score: 0.9999961257\n",
      "    Stock: XOM\n",
      "    Date: 2022-01-24\n",
      "    Movement: Unknown\n",
      "\n",
      "--- Query 5: AAPL on 2025-01-30 ---\n",
      "Candidates found: 10\n",
      "  Candidate 1:\n",
      "    Index: 4862\n",
      "    Score: 0.9999963045\n",
      "    Stock: XOM\n",
      "    Date: 2023-10-20\n",
      "    Movement: Unknown\n",
      "  Candidate 2:\n",
      "    Index: 4863\n",
      "    Score: 0.9999963045\n",
      "    Stock: XOM\n",
      "    Date: 2023-10-20\n",
      "    Movement: Unknown\n",
      "  Candidate 3:\n",
      "    Index: 4864\n",
      "    Score: 0.9999963045\n",
      "    Stock: XOM\n",
      "    Date: 2023-10-20\n",
      "    Movement: Unknown\n",
      "\n",
      "ðŸ“Š STEP 4: SIMILARITY SCORE STATISTICS\n",
      "--------------------------------------------------\n",
      "Total similarity scores analyzed: 26420\n",
      "Perfect scores (1.0): 0 (0.00%)\n",
      "Exact stock-date matches: 0\n",
      "Unique score values: 40\n",
      "Score statistics:\n",
      "  Min: 0.9999458790\n",
      "  Max: 0.9999989271\n",
      "  Mean: 0.9999943244\n",
      "  Std: 0.0000049495\n",
      "\n",
      "Most common similarity scores:\n",
      "  0.999997: 6320 times (23.9%)\n",
      "  0.999996: 4880 times (18.5%)\n",
      "  0.999995: 3890 times (14.7%)\n",
      "  0.999998: 2980 times (11.3%)\n",
      "  0.999994: 1940 times (7.3%)\n",
      "  0.999993: 1540 times (5.8%)\n",
      "  0.999992: 1050 times (4.0%)\n",
      "  0.999991: 700 times (2.6%)\n",
      "  0.999990: 600 times (2.3%)\n",
      "  0.999989: 420 times (1.6%)\n",
      "\n",
      "ðŸ” STEP 5: ROOT CAUSE ANALYSIS\n",
      "--------------------------------------------------\n",
      "ðŸ¤” UNCLEAR: Similarity scores look normal but something else is wrong\n",
      "Check your similarity search generation process\n",
      "\n",
      "ðŸ’¡ RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "3. ðŸ”§ VERIFY DATA SPLIT: Ensure temporal separation\n",
      "   - Training data should be from earlier dates\n",
      "   - Test data should be from later dates\n",
      "   - No overlap in stock-date combinations\n",
      "\n",
      "ðŸŽ¯ SUMMARY\n",
      "==================================================\n",
      "test_queries: 2642\n",
      "candidates: 8151\n",
      "similarity_results: 2642\n",
      "perfect_scores: 0\n",
      "exact_matches: 0\n",
      "total_scores: 26420\n",
      "stock_overlap: 50\n",
      "date_overlap: 0\n"
     ]
    }
   ],
   "source": [
    "# Script to trace your FinQuest data pipeline and understand the similarity score issue\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def trace_finquest_data_pipeline():\n",
    "    \"\"\"Trace the complete FinQuest data pipeline to understand similarity scores\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” TRACING FINQUEST DATA PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Understand your data sources\n",
    "    print(\"\\nðŸ“‚ STEP 1: DATA SOURCES ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Test queries (what you're predicting on)\n",
    "    test_queries_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/test_queries_rise_fall_only.json'\n",
    "    ground_truth_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/ground_truth_rise_fall_only.json'\n",
    "    \n",
    "    # Load test queries\n",
    "    test_queries = []\n",
    "    with open(test_queries_file, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                test_queries.append(json.loads(line.strip()))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Load ground truth\n",
    "    ground_truth = []\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                ground_truth.append(json.loads(line.strip()))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"âœ… Test queries: {len(test_queries)}\")\n",
    "    print(f\"âœ… Ground truth: {len(ground_truth)}\")\n",
    "    \n",
    "    # Analyze test query date ranges\n",
    "    test_dates = []\n",
    "    test_stocks = set()\n",
    "    for query in test_queries:\n",
    "        if 'query_date' in query:\n",
    "            test_dates.append(query['query_date'])\n",
    "            test_stocks.add(query.get('query_stock', ''))\n",
    "    \n",
    "    test_dates.sort()\n",
    "    print(f\"Test date range: {test_dates[0] if test_dates else 'None'} to {test_dates[-1] if test_dates else 'None'}\")\n",
    "    print(f\"Test stocks: {len(test_stocks)} unique stocks\")\n",
    "    print(f\"Sample test stocks: {list(test_stocks)[:10]}\")\n",
    "    \n",
    "    # Step 2: Analyze candidate pool (training data embeddings)\n",
    "    print(\"\\nðŸ“‚ STEP 2: CANDIDATE POOL ANALYSIS\")  \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    embeddings_dir = '/root/nfs/AJ FinRag/Embeddings/embeddings/test/FinQuest'\n",
    "    \n",
    "    # Load candidates\n",
    "    candidates = {}\n",
    "    candidate_dates = []\n",
    "    candidate_stocks = set()\n",
    "    \n",
    "    embedding_files = list(Path(embeddings_dir).glob(\"c_*_FinQuest_embeddings_*.pkl\"))\n",
    "    print(f\"Found {len(embedding_files)} candidate embedding files\")\n",
    "    \n",
    "    for file_path in embedding_files:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            embedding_data = pickle.load(f)\n",
    "        \n",
    "        for date_group in embedding_data:\n",
    "            for date, candidates_on_date in date_group.items():\n",
    "                for candidate_item in candidates_on_date:\n",
    "                    candidate_data = candidate_item['data']\n",
    "                    candidates[candidate_data['data_index']] = candidate_data\n",
    "                    \n",
    "                    if 'candidate_date' in candidate_data:\n",
    "                        candidate_dates.append(candidate_data['candidate_date'])\n",
    "                        candidate_stocks.add(candidate_data.get('candidate_stock', ''))\n",
    "    \n",
    "    candidate_dates.sort()\n",
    "    print(f\"âœ… Total candidates: {len(candidates)}\")\n",
    "    print(f\"Candidate date range: {candidate_dates[0] if candidate_dates else 'None'} to {candidate_dates[-1] if candidate_dates else 'None'}\")\n",
    "    print(f\"Candidate stocks: {len(candidate_stocks)} unique stocks\")\n",
    "    print(f\"Sample candidate stocks: {list(candidate_stocks)[:10]}\")\n",
    "    \n",
    "    # Check for overlap\n",
    "    stock_overlap = test_stocks.intersection(candidate_stocks)\n",
    "    print(f\"Stock overlap: {len(stock_overlap)}/{len(test_stocks)} test stocks also in candidates\")\n",
    "    \n",
    "    # Date overlap check\n",
    "    test_date_set = set(test_dates)\n",
    "    candidate_date_set = set(candidate_dates)\n",
    "    date_overlap = test_date_set.intersection(candidate_date_set)\n",
    "    print(f\"Date overlap: {len(date_overlap)} dates appear in both test and candidate sets\")\n",
    "    \n",
    "    if len(date_overlap) > 0:\n",
    "        print(f\"âš ï¸  WARNING: Overlapping dates detected!\")\n",
    "        print(f\"Sample overlapping dates: {list(date_overlap)[:10]}\")\n",
    "    \n",
    "    # Step 3: Analyze similarity results structure\n",
    "    print(\"\\nðŸ“‚ STEP 3: SIMILARITY RESULTS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    similarity_file = 'similar_candidates/test/FinQuest/test_similarity_results_FinQuest.pkl'\n",
    "    \n",
    "    if not Path(similarity_file).exists():\n",
    "        print(f\"âŒ Similarity file not found: {similarity_file}\")\n",
    "        return\n",
    "    \n",
    "    with open(similarity_file, 'rb') as f:\n",
    "        similarity_results = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ… Similarity results: {len(similarity_results)} queries\")\n",
    "    \n",
    "    # Analyze first few similarity results in detail\n",
    "    print(f\"\\nðŸ” DETAILED ANALYSIS OF FIRST 5 SIMILARITY RESULTS:\")\n",
    "    \n",
    "    for i in range(min(5, len(similarity_results))):\n",
    "        result = similarity_results[i]\n",
    "        query_stock = result.get('query_stock', 'Unknown')\n",
    "        query_date = result.get('query_date', 'Unknown')\n",
    "        similarity_list = result.get('similarity_list', [])\n",
    "        \n",
    "        print(f\"\\n--- Query {i+1}: {query_stock} on {query_date} ---\")\n",
    "        print(f\"Candidates found: {len(similarity_list)}\")\n",
    "        \n",
    "        if similarity_list:\n",
    "            # Analyze top 3 candidates\n",
    "            for j, candidate_info in enumerate(similarity_list[:3]):\n",
    "                candidate_index = candidate_info.get('candidate_index', 'Unknown')\n",
    "                candidate_score = candidate_info.get('candidate_score', 0)\n",
    "                \n",
    "                print(f\"  Candidate {j+1}:\")\n",
    "                print(f\"    Index: {candidate_index}\")\n",
    "                print(f\"    Score: {candidate_score:.10f}\")\n",
    "                \n",
    "                # Get candidate details\n",
    "                if candidate_index in candidates:\n",
    "                    candidate_data = candidates[candidate_index]\n",
    "                    cand_stock = candidate_data.get('candidate_stock', 'Unknown')\n",
    "                    cand_date = candidate_data.get('candidate_date', 'Unknown')\n",
    "                    cand_movement = candidate_data.get('movement', 'Unknown')\n",
    "                    \n",
    "                    print(f\"    Stock: {cand_stock}\")\n",
    "                    print(f\"    Date: {cand_date}\")\n",
    "                    print(f\"    Movement: {cand_movement}\")\n",
    "                    \n",
    "                    # Check if this is an exact match\n",
    "                    if query_stock == cand_stock and query_date == cand_date:\n",
    "                        print(f\"    ðŸš¨ EXACT MATCH: Same stock and date!\")\n",
    "                    \n",
    "                    # Check similarity score patterns\n",
    "                    if abs(candidate_score - 1.0) < 1e-10:\n",
    "                        print(f\"    âš ï¸  PERFECT SCORE: Exactly 1.0\")\n",
    "                else:\n",
    "                    print(f\"    âŒ Candidate data not found for index {candidate_index}\")\n",
    "    \n",
    "    # Step 4: Statistical analysis of similarity scores\n",
    "    print(f\"\\nðŸ“Š STEP 4: SIMILARITY SCORE STATISTICS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    all_scores = []\n",
    "    perfect_scores = 0\n",
    "    exact_matches = 0\n",
    "    score_patterns = {}\n",
    "    \n",
    "    for result in similarity_results:\n",
    "        query_stock = result.get('query_stock', '')\n",
    "        query_date = result.get('query_date', '')\n",
    "        similarity_list = result.get('similarity_list', [])\n",
    "        \n",
    "        for candidate_info in similarity_list:\n",
    "            candidate_index = candidate_info.get('candidate_index', -1)\n",
    "            candidate_score = candidate_info.get('candidate_score', 0)\n",
    "            \n",
    "            all_scores.append(candidate_score)\n",
    "            \n",
    "            # Count perfect scores\n",
    "            if abs(candidate_score - 1.0) < 1e-10:\n",
    "                perfect_scores += 1\n",
    "            \n",
    "            # Count exact matches\n",
    "            if candidate_index in candidates:\n",
    "                candidate_data = candidates[candidate_index]\n",
    "                if (candidate_data.get('candidate_stock', '') == query_stock and \n",
    "                    candidate_data.get('candidate_date', '') == query_date):\n",
    "                    exact_matches += 1\n",
    "            \n",
    "            # Track score patterns\n",
    "            score_rounded = round(candidate_score, 6)\n",
    "            score_patterns[score_rounded] = score_patterns.get(score_rounded, 0) + 1\n",
    "    \n",
    "    print(f\"Total similarity scores analyzed: {len(all_scores)}\")\n",
    "    print(f\"Perfect scores (1.0): {perfect_scores} ({perfect_scores/len(all_scores)*100:.2f}%)\")\n",
    "    print(f\"Exact stock-date matches: {exact_matches}\")\n",
    "    print(f\"Unique score values: {len(score_patterns)}\")\n",
    "    \n",
    "    if len(all_scores) > 0:\n",
    "        print(f\"Score statistics:\")\n",
    "        print(f\"  Min: {min(all_scores):.10f}\")\n",
    "        print(f\"  Max: {max(all_scores):.10f}\")\n",
    "        print(f\"  Mean: {np.mean(all_scores):.10f}\")\n",
    "        print(f\"  Std: {np.std(all_scores):.10f}\")\n",
    "    \n",
    "    # Show most common scores\n",
    "    print(f\"\\nMost common similarity scores:\")\n",
    "    sorted_patterns = sorted(score_patterns.items(), key=lambda x: x[1], reverse=True)\n",
    "    for score, count in sorted_patterns[:10]:\n",
    "        print(f\"  {score:.6f}: {count} times ({count/len(all_scores)*100:.1f}%)\")\n",
    "    \n",
    "    # Step 5: Root cause analysis\n",
    "    print(f\"\\nðŸ” STEP 5: ROOT CAUSE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if perfect_scores > len(all_scores) * 0.5:\n",
    "        print(\"ðŸš¨ CRITICAL ISSUE: >50% of similarity scores are exactly 1.0\")\n",
    "        \n",
    "        possible_causes = [\n",
    "            \"1. Query-candidate data overlap (same stock-date pairs in both sets)\",\n",
    "            \"2. Embedding computation error (all embeddings identical)\",\n",
    "            \"3. Cosine similarity calculation bug (returning 1.0 for everything)\", \n",
    "            \"4. Normalization issue (all embeddings have norm 0 or identical)\",\n",
    "            \"5. Data preprocessing creating duplicate sequences\",\n",
    "            \"6. Similarity search using wrong embedding vectors\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Possible causes:\")\n",
    "        for cause in possible_causes:\n",
    "            print(f\"  {cause}\")\n",
    "    \n",
    "    elif exact_matches > 0:\n",
    "        print(f\"ðŸš¨ DATA LEAK: {exact_matches} queries match candidates with same stock-date\")\n",
    "        print(\"This means your 'training' candidates include your test queries!\")\n",
    "        \n",
    "    elif len(score_patterns) == 1:\n",
    "        print(\"ðŸš¨ COMPUTATION ERROR: All similarity scores are identical\")\n",
    "        print(\"This suggests a bug in similarity computation\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ðŸ¤” UNCLEAR: Similarity scores look normal but something else is wrong\")\n",
    "        print(\"Check your similarity search generation process\")\n",
    "    \n",
    "    # Step 6: Recommendations\n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if exact_matches > 0:\n",
    "        print(\"1. ðŸ”§ IMMEDIATE FIX: Remove exact stock-date matches from candidates\")\n",
    "        print(\"   Add this filter in your similarity search:\")\n",
    "        print(\"\"\"\n",
    "   def filter_overlapping_candidates(query, similarity_list, candidates):\n",
    "       filtered = []\n",
    "       query_stock = query.get('query_stock', '')\n",
    "       query_date = query.get('query_date', '')\n",
    "       \n",
    "       for candidate_info in similarity_list:\n",
    "           candidate_index = candidate_info.get('candidate_index', -1)\n",
    "           if candidate_index in candidates:\n",
    "               candidate_data = candidates[candidate_index]\n",
    "               # Skip if same stock and date\n",
    "               if not (candidate_data.get('candidate_stock', '') == query_stock and \n",
    "                      candidate_data.get('candidate_date', '') == query_date):\n",
    "                   filtered.append(candidate_info)\n",
    "       \n",
    "       return filtered\n",
    "        \"\"\")\n",
    "    \n",
    "    if perfect_scores > len(all_scores) * 0.1:\n",
    "        print(\"2. ðŸ”§ CHECK EMBEDDINGS: Verify embedding computation\")\n",
    "        print(\"   - Check if all embeddings are identical\")\n",
    "        print(\"   - Verify cosine similarity calculation\") \n",
    "        print(\"   - Check embedding normalization\")\n",
    "    \n",
    "    print(\"3. ðŸ”§ VERIFY DATA SPLIT: Ensure temporal separation\")\n",
    "    print(\"   - Training data should be from earlier dates\")\n",
    "    print(\"   - Test data should be from later dates\") \n",
    "    print(\"   - No overlap in stock-date combinations\")\n",
    "    \n",
    "    return {\n",
    "        'test_queries': len(test_queries),\n",
    "        'candidates': len(candidates),\n",
    "        'similarity_results': len(similarity_results),\n",
    "        'perfect_scores': perfect_scores,\n",
    "        'exact_matches': exact_matches,\n",
    "        'total_scores': len(all_scores),\n",
    "        'stock_overlap': len(stock_overlap),\n",
    "        'date_overlap': len(date_overlap)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = trace_finquest_data_pipeline()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b6eff-2fb2-4526-9a6b-6b340d13b824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
