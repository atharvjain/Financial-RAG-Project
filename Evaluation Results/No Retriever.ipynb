{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51e7382-af6f-486a-93ee-47dc42bf1ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 2642 test queries\n",
      "INFO:__main__:Loaded 2642 ground truth entries\n",
      "INFO:__main__: Starting MULTI-LLM NO RETRIEVAL experiment\n",
      "INFO:__main__:Testing LLMs: ['StockLLM', 'Qwen2.5-1.5B', 'Llama3.2-3B']\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: StockLLM\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__: Starting NO RETRIEVAL experiment with StockLLM\n",
      "INFO:__main__:Loading StockLLM: ElsaShaw/StockLLM\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845be1a210d341849860530edee790fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Successfully loaded StockLLM\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy so far: 0.455\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy so far: 0.470\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy so far: 0.472\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy so far: 0.486\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy so far: 0.468\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy so far: 0.481\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy so far: 0.489\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy so far: 0.494\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy so far: 0.495\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy so far: 0.516\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy so far: 0.521\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy so far: 0.523\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:NO RETRIEVAL RESULTS - StockLLM\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.5238\n",
      "INFO:__main__:Rise Accuracy: 1.0000 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.0016 (1260 samples)\n",
      "INFO:__main__:Results saved to: no_retrieval_experiments/StockLLM_no_retrieval_results.csv\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__: Completed StockLLM\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: Qwen2.5-1.5B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__: Starting NO RETRIEVAL experiment with Qwen2.5-1.5B\n",
      "INFO:__main__:Loading Qwen2.5-1.5B: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:__main__:✅ Successfully loaded Qwen2.5-1.5B\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy so far: 0.469\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy so far: 0.535\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy so far: 0.550\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy so far: 0.548\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy so far: 0.538\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy so far: 0.545\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy so far: 0.536\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy so far: 0.526\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy so far: 0.526\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy so far: 0.519\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy so far: 0.519\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy so far: 0.519\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy so far: 0.514\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy so far: 0.494\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy so far: 0.492\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy so far: 0.489\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy so far: 0.487\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy so far: 0.487\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy so far: 0.486\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy so far: 0.487\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy so far: 0.486\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy so far: 0.485\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy so far: 0.486\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy so far: 0.484\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy so far: 0.481\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy so far: 0.481\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy so far: 0.480\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:NO RETRIEVAL RESULTS - Qwen2.5-1.5B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.4803\n",
      "INFO:__main__:Rise Accuracy: 0.1838 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.8056 (1260 samples)\n",
      "INFO:__main__:Results saved to: no_retrieval_experiments/Qwen2.5-1.5B_no_retrieval_results.csv\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__: Completed Qwen2.5-1.5B\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: Llama3.2-3B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__: Starting NO RETRIEVAL experiment with Llama3.2-3B\n",
      "INFO:__main__:Loading Llama3.2-3B: meta-llama/Llama-3.2-3B-Instruct\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8d9ee7568643ec9b39aee9894ff56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Successfully loaded Llama3.2-3B\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy so far: 0.531\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy so far: 0.535\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy so far: 0.534\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy so far: 0.545\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy so far: 0.527\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy so far: 0.526\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy so far: 0.526\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy so far: 0.527\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy so far: 0.521\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy so far: 0.523\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy so far: 0.515\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy so far: 0.515\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy so far: 0.518\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy so far: 0.518\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy so far: 0.515\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy so far: 0.514\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:NO RETRIEVAL RESULTS - Llama3.2-3B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.4996\n",
      "INFO:__main__:Rise Accuracy: 0.2938 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.7254 (1260 samples)\n",
      "INFO:__main__:Results saved to: no_retrieval_experiments/Llama3.2-3B_no_retrieval_results.csv\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__: Completed Llama3.2-3B\n",
      "INFO:__main__:\n",
      "================================================================================\n",
      "INFO:__main__:NO RETRIEVAL - LLM COMPARISON SUMMARY\n",
      "INFO:__main__:================================================================================\n",
      "INFO:__main__:StockLLM        | Accuracy: 0.5238 | Rise: 1.000 | Fall: 0.002\n",
      "INFO:__main__:Qwen2.5-1.5B    | Accuracy: 0.4803 | Rise: 0.184 | Fall: 0.806\n",
      "INFO:__main__:Llama3.2-3B     | Accuracy: 0.4996 | Rise: 0.294 | Fall: 0.725\n",
      "INFO:__main__:\n",
      "Detailed comparison saved to: no_retrieval_experiments/no_retrieval_llm_comparison.csv\n",
      "INFO:__main__:\n",
      " Multi-LLM experiment completed!\n",
      "INFO:__main__:Combined results saved to: no_retrieval_experiments/all_llms_no_retrieval_combined.csv\n",
      "INFO:__main__: NO RETRIEVAL EXPERIMENT COMPLETED SUCCESSFULLY!\n",
      "INFO:__main__: Total results: 7926 predictions across 3 LLMs\n"
     ]
    }
   ],
   "source": [
    "# SECTION 1: NO RETRIEVAL BASELINE EXPERIMENT\n",
    "# Pure LLM prediction without any historical context retrieval\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NoRetrievalExperiment:\n",
    "    \"\"\"\n",
    "    Baseline experiment: LLM prediction with only query context\n",
    "    Tests pure LLM financial reasoning without historical examples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_queries_file: str, ground_truth_file: str):\n",
    "        self.test_queries_file = test_queries_file\n",
    "        self.ground_truth_file = ground_truth_file\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load data\n",
    "        self.test_queries = self._load_json_file(test_queries_file)\n",
    "        self.ground_truth = self._load_json_file(ground_truth_file)\n",
    "        self.gt_lookup = {gt['query_id']: gt for gt in self.ground_truth}\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.test_queries)} test queries\")\n",
    "        logger.info(f\"Loaded {len(self.ground_truth)} ground truth entries\")\n",
    "        \n",
    "        # LLM configurations for testing\n",
    "        self.llm_configs = {\n",
    "            'StockLLM': {\n",
    "                'model_name': 'ElsaShaw/StockLLM',\n",
    "                'description': 'Specialized financial LLM'\n",
    "            },\n",
    "            'Llama3.2-3B': {\n",
    "                'model_name': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "                'description': 'Medium general-purpose LLM'\n",
    "            },\n",
    "            'Qwen2.5-1.5B': {\n",
    "                'model_name': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "                'description': 'Qwen instruction-following model'\n",
    "            },\n",
    "            'Phi3-Mini': {\n",
    "                'model_name': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "                'description': 'Microsoft compact LLM'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Current LLM state\n",
    "        self.current_llm = None\n",
    "        self.current_tokenizer = None\n",
    "        self.current_llm_name = None\n",
    "    \n",
    "    def _load_json_file(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load JSONL file\"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return data\n",
    "    \n",
    "    def load_llm(self, llm_name: str):\n",
    "        \"\"\"Load specified LLM for testing\"\"\"\n",
    "        if llm_name not in self.llm_configs:\n",
    "            raise ValueError(f\"Unknown LLM: {llm_name}\")\n",
    "        \n",
    "        # Clear previous model from memory\n",
    "        if self.current_llm is not None:\n",
    "            del self.current_llm\n",
    "            del self.current_tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        config = self.llm_configs[llm_name]\n",
    "        model_name = config['model_name']\n",
    "        \n",
    "        logger.info(f\"Loading {llm_name}: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.current_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if self.current_tokenizer.pad_token is None:\n",
    "                self.current_tokenizer.pad_token = self.current_tokenizer.eos_token\n",
    "            \n",
    "            # Load model with appropriate settings\n",
    "            self.current_llm = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16 if self.device.type == 'cuda' else torch.float32,\n",
    "                device_map=\"auto\" if self.device.type == 'cuda' else None,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True  # For some models like Phi\n",
    "            )\n",
    "            \n",
    "            self.current_llm_name = llm_name\n",
    "            logger.info(f\"✅ Successfully loaded {llm_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load {llm_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_no_retrieval_prompt(self, query: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate minimal prompt with only query stock context\n",
    "        No historical examples or retrieved candidates\n",
    "        \"\"\"\n",
    "        query_stock = query.get('query_stock', 'Unknown')\n",
    "        query_date = query.get('query_date', 'Unknown')\n",
    "        \n",
    "        # Include only the essential query information\n",
    "        context = {\n",
    "            \"query_stock\": query_stock,\n",
    "            \"recent_date_list\": query.get('recent_date_list', []),\n",
    "            \"adjusted_close_list\": query.get('adjusted_close_list', [])\n",
    "        }\n",
    "        \n",
    "        # Simple, direct prompt without historical context\n",
    "        prompt = (\n",
    "            f\"Stock Context: {str(context)}\\n\\n\"\n",
    "            f\"Based on the stock price history above, predict the stock movement by filling in the [blank] with 'rise' or 'fall'. \"\n",
    "            f\"Just fill in the blank, do not explain.\\n\\n\"\n",
    "            f\"Query: On {query_date}, the movement of ${query_stock} is [blank].\"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def ask_llm(self, prompt: str) -> str:\n",
    "        \"\"\"Get prediction from current LLM\"\"\"\n",
    "        if self.current_llm is None:\n",
    "            raise RuntimeError(\"No LLM loaded\")\n",
    "        \n",
    "        # Format prompt based on LLM architecture\n",
    "        if 'Llama' in self.current_llm_name:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst. Predict stock movements accurately using only the given information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            formatted_prompt = self.current_tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            # For other models, use simple system/user format\n",
    "            formatted_prompt = f\"System: You are a financial analyst.\\nUser: {prompt}\\nAssistant:\"\n",
    "        \n",
    "        # Tokenize\n",
    "        input_ids = self.current_tokenizer.encode(\n",
    "            formatted_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=1024\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate with conservative settings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_llm.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,  # Short response expected\n",
    "                temperature=0.1,    # Low temperature for consistency\n",
    "                do_sample=False,    # Deterministic\n",
    "                pad_token_id=self.current_tokenizer.eos_token_id,\n",
    "                eos_token_id=self.current_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "        response = self.current_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def extract_prediction(self, response: str, reference: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Extract rise/fall prediction from LLM response\"\"\"\n",
    "        response_clean = response.lower().strip()\n",
    "        reference_clean = reference.lower().strip()\n",
    "        \n",
    "        # Extract prediction\n",
    "        if 'rise' in response_clean:\n",
    "            prediction = 'rise'\n",
    "        elif 'fall' in response_clean:\n",
    "            prediction = 'fall'\n",
    "        else:\n",
    "            prediction = 'freeze'  # Unclear response\n",
    "        \n",
    "        # Check correctness\n",
    "        correct = (prediction == reference_clean)\n",
    "        \n",
    "        return prediction, correct\n",
    "    \n",
    "    def run_single_llm_experiment(self, llm_name: str, output_dir: str) -> pd.DataFrame:\n",
    "        \"\"\"Run no-retrieval experiment for a single LLM\"\"\"\n",
    "        logger.info(f\" Starting NO RETRIEVAL experiment with {llm_name}\")\n",
    "        \n",
    "        # Load the specified LLM\n",
    "        self.load_llm(llm_name)\n",
    "        \n",
    "        results = []\n",
    "        processed_count = 0\n",
    "        correct_count = 0\n",
    "        \n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_id = query['query_id']\n",
    "            ground_truth = self.gt_lookup.get(query_id)\n",
    "            \n",
    "            # Skip if no ground truth\n",
    "            if not ground_truth:\n",
    "                continue\n",
    "            \n",
    "            reference_answer = ground_truth['actual_movement']\n",
    "            \n",
    "            # Skip freeze movements (focus on rise/fall)\n",
    "            if reference_answer == 'freeze':\n",
    "                continue\n",
    "            \n",
    "            # Progress logging\n",
    "            if (i + 1) % 50 == 0:\n",
    "                accuracy = correct_count / processed_count if processed_count > 0 else 0\n",
    "                logger.info(f\"Progress: {i+1}/{len(self.test_queries)} | Accuracy so far: {accuracy:.3f}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate prompt (no retrieval context)\n",
    "                prompt = self.generate_no_retrieval_prompt(query)\n",
    "                \n",
    "                # Get LLM prediction\n",
    "                llm_response = self.ask_llm(prompt)\n",
    "                \n",
    "                # Extract and validate prediction\n",
    "                prediction, correct = self.extract_prediction(llm_response, reference_answer)\n",
    "                \n",
    "                # Track accuracy\n",
    "                processed_count += 1\n",
    "                if correct:\n",
    "                    correct_count += 1\n",
    "                \n",
    "                # Store result\n",
    "                result = {\n",
    "                    'llm_name': llm_name,\n",
    "                    'query_id': query_id,\n",
    "                    'query_stock': query.get('query_stock', ''),\n",
    "                    'query_date': query.get('query_date', ''),\n",
    "                    'method': 'no_retrieval',\n",
    "                    'prompt': prompt,\n",
    "                    'llm_response': llm_response,\n",
    "                    'prediction': prediction,\n",
    "                    'reference': reference_answer,\n",
    "                    'correct': correct,\n",
    "                    'candidate_count': 0,  # No candidates used\n",
    "                    'retrieval_context': 'none'\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing query {query_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save detailed results\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f'{llm_name}_no_retrieval_results.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_predictions = len(df)\n",
    "        overall_accuracy = df['correct'].mean() if total_predictions > 0 else 0\n",
    "        \n",
    "        # Class-specific metrics\n",
    "        rise_df = df[df['reference'] == 'rise']\n",
    "        fall_df = df[df['reference'] == 'fall']\n",
    "        \n",
    "        rise_accuracy = rise_df['correct'].mean() if len(rise_df) > 0 else 0\n",
    "        fall_accuracy = fall_df['correct'].mean() if len(fall_df) > 0 else 0\n",
    "        \n",
    "        # Log results\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"NO RETRIEVAL RESULTS - {llm_name}\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Total Predictions: {total_predictions}\")\n",
    "        logger.info(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        logger.info(f\"Rise Accuracy: {rise_accuracy:.4f} ({len(rise_df)} samples)\")\n",
    "        logger.info(f\"Fall Accuracy: {fall_accuracy:.4f} ({len(fall_df)} samples)\")\n",
    "        logger.info(f\"Results saved to: {output_file}\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def run_multi_llm_experiment(self, llm_list: List[str], output_dir: str) -> pd.DataFrame:\n",
    "        \"\"\"Run no-retrieval experiment across multiple LLMs\"\"\"\n",
    "        logger.info(f\" Starting MULTI-LLM NO RETRIEVAL experiment\")\n",
    "        logger.info(f\"Testing LLMs: {llm_list}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for llm_name in llm_list:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"TESTING LLM: {llm_name}\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                # Run experiment for this LLM\n",
    "                llm_results = self.run_single_llm_experiment(llm_name, output_dir)\n",
    "                all_results.append(llm_results)\n",
    "                \n",
    "                logger.info(f\" Completed {llm_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\" Failed {llm_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if all_results:\n",
    "            combined_df = pd.concat(all_results, ignore_index=True)\n",
    "            \n",
    "            # Save combined results\n",
    "            combined_file = os.path.join(output_dir, 'all_llms_no_retrieval_combined.csv')\n",
    "            combined_df.to_csv(combined_file, index=False)\n",
    "            \n",
    "            # Generate comparison report\n",
    "            self._generate_comparison_report(combined_df, output_dir)\n",
    "            \n",
    "            logger.info(f\"\\n Multi-LLM experiment completed!\")\n",
    "            logger.info(f\"Combined results saved to: {combined_file}\")\n",
    "            \n",
    "            return combined_df\n",
    "        else:\n",
    "            logger.error(\" No successful experiments!\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _generate_comparison_report(self, combined_df: pd.DataFrame, output_dir: str):\n",
    "        \"\"\"Generate comparison report across LLMs\"\"\"\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        for llm_name in combined_df['llm_name'].unique():\n",
    "            llm_df = combined_df[combined_df['llm_name'] == llm_name]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total = len(llm_df)\n",
    "            accuracy = llm_df['correct'].mean()\n",
    "            \n",
    "            rise_df = llm_df[llm_df['reference'] == 'rise']\n",
    "            fall_df = llm_df[llm_df['reference'] == 'fall']\n",
    "            \n",
    "            rise_acc = rise_df['correct'].mean() if len(rise_df) > 0 else 0\n",
    "            fall_acc = fall_df['correct'].mean() if len(fall_df) > 0 else 0\n",
    "            \n",
    "            # Prediction distribution\n",
    "            pred_dist = llm_df['prediction'].value_counts()\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'llm_name': llm_name,\n",
    "                'total_predictions': total,\n",
    "                'overall_accuracy': accuracy,\n",
    "                'rise_accuracy': rise_acc,\n",
    "                'fall_accuracy': fall_acc,\n",
    "                'rise_predictions': len(rise_df),\n",
    "                'fall_predictions': len(fall_df),\n",
    "                'predicted_rise': pred_dist.get('rise', 0),\n",
    "                'predicted_fall': pred_dist.get('fall', 0),\n",
    "                'predicted_freeze': pred_dist.get('freeze', 0),\n",
    "                'method': 'no_retrieval'\n",
    "            })\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        comparison_file = os.path.join(output_dir, 'no_retrieval_llm_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_file, index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(\"\\n\" + \"=\"*80)\n",
    "        logger.info(\"NO RETRIEVAL - LLM COMPARISON SUMMARY\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        for result in comparison_results:\n",
    "            logger.info(f\"{result['llm_name']:15} | Accuracy: {result['overall_accuracy']:.4f} | \"\n",
    "                       f\"Rise: {result['rise_accuracy']:.3f} | Fall: {result['fall_accuracy']:.3f}\")\n",
    "        \n",
    "        logger.info(f\"\\nDetailed comparison saved to: {comparison_file}\")\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "def run_no_retrieval_experiment():\n",
    "    \"\"\"Main function to run the no retrieval experiment\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    test_queries_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/test_queries_rise_fall_only.json'\n",
    "    ground_truth_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/ground_truth_rise_fall_only.json'\n",
    "    output_dir = 'no_retrieval_experiments'\n",
    "    \n",
    "    # LLMs to test (start with fewer for testing)\n",
    "    test_llms = [\n",
    "        'StockLLM',      # Specialized financial model\n",
    "        'Qwen2.5-1.5B',   \n",
    "        'Llama3.2-3B'    # Medium general model\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Initialize experiment\n",
    "        experiment = NoRetrievalExperiment(test_queries_file, ground_truth_file)\n",
    "        \n",
    "        # Run multi-LLM experiment\n",
    "        results = experiment.run_multi_llm_experiment(test_llms, output_dir)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            logger.info(\" NO RETRIEVAL EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "            logger.info(f\" Total results: {len(results)} predictions across {len(test_llms)} LLMs\")\n",
    "        else:\n",
    "            logger.error(\" Experiment failed - no results generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\" Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_no_retrieval_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed9a8f-5588-4a65-9809-bae45dc24736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580dad77-69b2-46d2-a9a0-ec61b44e4096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
