{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c961edb9-0558-4155-826f-3dee57acfe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading candidates from 75 files...\n",
      "INFO:__main__:Loaded 2642 test queries\n",
      "INFO:__main__:Loaded 2642 ground truth entries\n",
      "INFO:__main__:Loaded 8151 candidates for random sampling\n",
      "INFO:__main__:🚀 RUNNING MULTI-LLM RANDOM RETRIEVAL EXPERIMENT\n",
      "INFO:__main__:🚀 Starting MULTI-LLM RANDOM RETRIEVAL experiment\n",
      "INFO:__main__:Testing LLMs: ['StockLLM', 'Qwen2.5-1.5B', 'Llama3.2-3B']\n",
      "INFO:__main__:Candidates per query: 5\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: StockLLM\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:🚀 Starting RANDOM RETRIEVAL experiment with StockLLM (k=5)\n",
      "INFO:__main__:Loading StockLLM: ElsaShaw/StockLLM\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f187ae9f6d4b9f8dd8308db59525a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Successfully loaded StockLLM\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy so far: 0.455\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy so far: 0.470\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy so far: 0.472\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy so far: 0.490\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy so far: 0.493\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy so far: 0.507\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy so far: 0.511\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy so far: 0.514\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy so far: 0.514\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy so far: 0.514\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy so far: 0.518\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy so far: 0.523\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy so far: 0.524\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:RANDOM RETRIEVAL RESULTS - StockLLM\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.5257\n",
      "INFO:__main__:Rise Accuracy: 0.9913 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.0151 (1260 samples)\n",
      "INFO:__main__:Avg Candidates Used: 5.0\n",
      "INFO:__main__:Results saved to: random_retrieval_experiments/StockLLM_random_retrieval_k5_results.csv\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:✅ Completed StockLLM\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: Qwen2.5-1.5B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:🚀 Starting RANDOM RETRIEVAL experiment with Qwen2.5-1.5B (k=5)\n",
      "INFO:__main__:Loading Qwen2.5-1.5B: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:__main__:✅ Successfully loaded Qwen2.5-1.5B\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy so far: 0.531\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy so far: 0.508\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy so far: 0.518\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy so far: 0.515\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy so far: 0.494\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy so far: 0.492\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy so far: 0.488\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy so far: 0.488\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy so far: 0.490\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy so far: 0.487\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy so far: 0.481\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy so far: 0.482\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy so far: 0.478\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy so far: 0.480\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy so far: 0.478\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy so far: 0.474\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy so far: 0.478\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy so far: 0.477\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy so far: 0.478\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy so far: 0.478\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy so far: 0.477\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy so far: 0.477\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy so far: 0.474\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy so far: 0.473\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy so far: 0.474\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy so far: 0.474\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy so far: 0.474\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy so far: 0.474\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy so far: 0.473\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy so far: 0.476\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy so far: 0.477\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy so far: 0.478\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy so far: 0.475\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:RANDOM RETRIEVAL RESULTS - Qwen2.5-1.5B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.4769\n",
      "INFO:__main__:Rise Accuracy: 0.2887 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.6833 (1260 samples)\n",
      "INFO:__main__:Avg Candidates Used: 5.0\n",
      "INFO:__main__:Results saved to: random_retrieval_experiments/Qwen2.5-1.5B_random_retrieval_k5_results.csv\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:✅ Completed Qwen2.5-1.5B\n",
      "INFO:__main__:\n",
      "============================================================\n",
      "INFO:__main__:TESTING LLM: Llama3.2-3B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:🚀 Starting RANDOM RETRIEVAL experiment with Llama3.2-3B (k=5)\n",
      "INFO:__main__:Loading Llama3.2-3B: meta-llama/Llama-3.2-3B-Instruct\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e689e06a83f3410ba1f21a1709fcbe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Successfully loaded Llama3.2-3B\n",
      "INFO:__main__:Progress: 50/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 100/2642 | Accuracy so far: 0.455\n",
      "INFO:__main__:Progress: 150/2642 | Accuracy so far: 0.470\n",
      "INFO:__main__:Progress: 200/2642 | Accuracy so far: 0.472\n",
      "INFO:__main__:Progress: 250/2642 | Accuracy so far: 0.486\n",
      "INFO:__main__:Progress: 300/2642 | Accuracy so far: 0.468\n",
      "INFO:__main__:Progress: 350/2642 | Accuracy so far: 0.481\n",
      "INFO:__main__:Progress: 400/2642 | Accuracy so far: 0.489\n",
      "INFO:__main__:Progress: 450/2642 | Accuracy so far: 0.494\n",
      "INFO:__main__:Progress: 500/2642 | Accuracy so far: 0.505\n",
      "INFO:__main__:Progress: 550/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:Progress: 600/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 650/2642 | Accuracy so far: 0.495\n",
      "INFO:__main__:Progress: 700/2642 | Accuracy so far: 0.496\n",
      "INFO:__main__:Progress: 750/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 800/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 850/2642 | Accuracy so far: 0.498\n",
      "INFO:__main__:Progress: 900/2642 | Accuracy so far: 0.501\n",
      "INFO:__main__:Progress: 950/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1000/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 1050/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1100/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 1150/2642 | Accuracy so far: 0.495\n",
      "INFO:__main__:Progress: 1200/2642 | Accuracy so far: 0.497\n",
      "INFO:__main__:Progress: 1250/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 1300/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 1350/2642 | Accuracy so far: 0.499\n",
      "INFO:__main__:Progress: 1400/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1450/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1500/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1550/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 1600/2642 | Accuracy so far: 0.500\n",
      "INFO:__main__:Progress: 1650/2642 | Accuracy so far: 0.503\n",
      "INFO:__main__:Progress: 1700/2642 | Accuracy so far: 0.502\n",
      "INFO:__main__:Progress: 1750/2642 | Accuracy so far: 0.504\n",
      "INFO:__main__:Progress: 1800/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 1850/2642 | Accuracy so far: 0.506\n",
      "INFO:__main__:Progress: 1900/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 1950/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 2000/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 2050/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 2100/2642 | Accuracy so far: 0.509\n",
      "INFO:__main__:Progress: 2150/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 2200/2642 | Accuracy so far: 0.510\n",
      "INFO:__main__:Progress: 2250/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2300/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2350/2642 | Accuracy so far: 0.512\n",
      "INFO:__main__:Progress: 2400/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 2450/2642 | Accuracy so far: 0.513\n",
      "INFO:__main__:Progress: 2500/2642 | Accuracy so far: 0.517\n",
      "INFO:__main__:Progress: 2550/2642 | Accuracy so far: 0.521\n",
      "INFO:__main__:Progress: 2600/2642 | Accuracy so far: 0.523\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:RANDOM RETRIEVAL RESULTS - Llama3.2-3B\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Total Predictions: 2642\n",
      "INFO:__main__:Overall Accuracy: 0.5250\n",
      "INFO:__main__:Rise Accuracy: 0.9949 (1382 samples)\n",
      "INFO:__main__:Fall Accuracy: 0.0095 (1260 samples)\n",
      "INFO:__main__:Avg Candidates Used: 5.0\n",
      "INFO:__main__:Results saved to: random_retrieval_experiments/Llama3.2-3B_random_retrieval_k5_results.csv\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:✅ Completed Llama3.2-3B\n",
      "INFO:__main__:\n",
      "================================================================================\n",
      "INFO:__main__:RANDOM RETRIEVAL (k=5) - LLM COMPARISON SUMMARY\n",
      "INFO:__main__:================================================================================\n",
      "INFO:__main__:StockLLM        | Accuracy: 0.5257 | Rise: 0.991 | Fall: 0.015 | Candidates: 5.0\n",
      "INFO:__main__:Qwen2.5-1.5B    | Accuracy: 0.4769 | Rise: 0.289 | Fall: 0.683 | Candidates: 5.0\n",
      "INFO:__main__:Llama3.2-3B     | Accuracy: 0.5250 | Rise: 0.995 | Fall: 0.010 | Candidates: 5.0\n",
      "INFO:__main__:\n",
      "Detailed comparison saved to: random_retrieval_experiments/random_retrieval_k5_llm_comparison.csv\n",
      "INFO:__main__:\n",
      "✅ Multi-LLM random retrieval experiment completed!\n",
      "INFO:__main__:Combined results saved to: random_retrieval_experiments/all_llms_random_retrieval_k5_combined.csv\n",
      "INFO:__main__:🎉 RANDOM RETRIEVAL EXPERIMENT COMPLETED SUCCESSFULLY!\n",
      "INFO:__main__:📊 Total results: 7926 predictions across 3 LLMs\n"
     ]
    }
   ],
   "source": [
    "# SECTION 2: RANDOM RETRIEVAL EXPERIMENT\n",
    "# LLM prediction with randomly sampled historical candidates\n",
    "# Tests whether ANY historical context helps vs. SMART retrieval\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RandomRetrievalExperiment:\n",
    "    \"\"\"\n",
    "    Random retrieval experiment: LLM + randomly sampled historical candidates\n",
    "    Controls for the effect of having ANY historical context vs. smart retrieval\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_queries_file: str, ground_truth_file: str, embeddings_dir: str):\n",
    "        self.test_queries_file = test_queries_file\n",
    "        self.ground_truth_file = ground_truth_file\n",
    "        self.embeddings_dir = embeddings_dir\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load data\n",
    "        self.test_queries = self._load_json_file(test_queries_file)\n",
    "        self.ground_truth = self._load_json_file(ground_truth_file)\n",
    "        self.gt_lookup = {gt['query_id']: gt for gt in self.ground_truth}\n",
    "        \n",
    "        # Load candidates from embeddings\n",
    "        self.candidates = self._load_candidates()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.test_queries)} test queries\")\n",
    "        logger.info(f\"Loaded {len(self.ground_truth)} ground truth entries\")\n",
    "        logger.info(f\"Loaded {len(self.candidates)} candidates for random sampling\")\n",
    "        \n",
    "        # LLM configurations\n",
    "        self.llm_configs = {\n",
    "            'StockLLM': {\n",
    "                'model_name': 'ElsaShaw/StockLLM',\n",
    "                'description': 'Specialized financial LLM'\n",
    "            },\n",
    "            'Llama3.2-3B': {\n",
    "                'model_name': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "                'description': 'Medium general-purpose LLM'\n",
    "            },\n",
    "            'Qwen2.5-1.5B': {\n",
    "                'model_name': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "                'description': 'Qwen instruction-following model'\n",
    "            },\n",
    "            'Phi3-Mini': {\n",
    "                'model_name': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "                'description': 'Microsoft compact LLM'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Current LLM state\n",
    "        self.current_llm = None\n",
    "        self.current_tokenizer = None\n",
    "        self.current_llm_name = None\n",
    "    \n",
    "    def _load_json_file(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load JSONL file\"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return data\n",
    "    \n",
    "    def _load_candidates(self) -> Dict:\n",
    "        \"\"\"Load all candidates from embedding files\"\"\"\n",
    "        candidates = {}\n",
    "        embedding_files = list(Path(self.embeddings_dir).glob(\"c_*_FinQuest_embeddings_*.pkl\"))\n",
    "        \n",
    "        logger.info(f\"Loading candidates from {len(embedding_files)} files...\")\n",
    "        \n",
    "        for file_path in embedding_files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                embedding_data = pickle.load(f)\n",
    "            \n",
    "            for date_group in embedding_data:\n",
    "                for date, candidates_on_date in date_group.items():\n",
    "                    for candidate_item in candidates_on_date:\n",
    "                        candidate_data = candidate_item['data']\n",
    "                        candidates[candidate_data['data_index']] = candidate_data\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def load_llm(self, llm_name: str):\n",
    "        \"\"\"Load specified LLM for testing\"\"\"\n",
    "        if llm_name not in self.llm_configs:\n",
    "            raise ValueError(f\"Unknown LLM: {llm_name}\")\n",
    "        \n",
    "        # Clear previous model\n",
    "        if self.current_llm is not None:\n",
    "            del self.current_llm\n",
    "            del self.current_tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        config = self.llm_configs[llm_name]\n",
    "        model_name = config['model_name']\n",
    "        \n",
    "        logger.info(f\"Loading {llm_name}: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            self.current_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if self.current_tokenizer.pad_token is None:\n",
    "                self.current_tokenizer.pad_token = self.current_tokenizer.eos_token\n",
    "            \n",
    "            self.current_llm = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16 if self.device.type == 'cuda' else torch.float32,\n",
    "                device_map=\"auto\" if self.device.type == 'cuda' else None,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.current_llm_name = llm_name\n",
    "            logger.info(f\"✅ Successfully loaded {llm_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load {llm_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_qualified_candidates(self, query_date: str) -> List[Dict]:\n",
    "        \"\"\"Get candidates that occur before the query date (temporal safety)\"\"\"\n",
    "        query_dt = datetime.strptime(query_date, \"%Y-%m-%d\")\n",
    "        qualified = []\n",
    "        \n",
    "        for candidate in self.candidates.values():\n",
    "            try:\n",
    "                candidate_dt = datetime.strptime(candidate['candidate_date'], \"%Y-%m-%d\")\n",
    "                if query_dt > candidate_dt:  # Only past candidates\n",
    "                    qualified.append(candidate)\n",
    "            except (ValueError, KeyError):\n",
    "                continue  # Skip malformed dates\n",
    "        \n",
    "        return qualified\n",
    "    \n",
    "    def sample_random_candidates(self, qualified_candidates: List[Dict], k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Randomly sample k candidates from qualified pool\"\"\"\n",
    "        if len(qualified_candidates) <= k:\n",
    "            return qualified_candidates\n",
    "        else:\n",
    "            return random.sample(qualified_candidates, k)\n",
    "    \n",
    "    def generate_random_retrieval_prompt(self, query: Dict, random_candidates: List[Dict]) -> Tuple[str, List[int]]:\n",
    "        \"\"\"\n",
    "        Generate prompt with randomly retrieved historical candidates\n",
    "        Same format as FinQuest retrieval but with random selection\n",
    "        \"\"\"\n",
    "        query_stock = query.get('query_stock', 'Unknown')\n",
    "        query_date = query.get('query_date', 'Unknown')\n",
    "        \n",
    "        instruction = (\n",
    "            \"Based on the following information, predict stock movement by filling in the [blank] with 'rise' or 'fall'. \"\n",
    "            \"Just fill in the blank, do not explain.\\n\"\n",
    "        )\n",
    "        \n",
    "        retrieve_prompt = 'These are randomly selected sequences that may affect this stock\\'s price recently:\\n'\n",
    "        \n",
    "        # Format random candidates\n",
    "        candidate_text = \"\"\n",
    "        candidate_movement_count = [0, 0]  # [rise, fall]\n",
    "        \n",
    "        for candidate in random_candidates:\n",
    "            # Track movement distribution\n",
    "            movement = candidate.get('movement', 'unknown')\n",
    "            if movement == 'rise':\n",
    "                candidate_movement_count[0] += 1\n",
    "            elif movement == 'fall':\n",
    "                candidate_movement_count[1] += 1\n",
    "            \n",
    "            # Format candidate sequence\n",
    "            candidate_sequence = {\n",
    "                'candidate_stock': candidate.get('candidate_stock', 'Unknown'),\n",
    "                'candidate_date': candidate.get('candidate_date', 'Unknown'),\n",
    "                'recent_date_list': candidate.get('recent_date_list', []),\n",
    "                'adjusted_close_list': candidate.get('adjusted_close_list', [])\n",
    "            }\n",
    "            \n",
    "            candidate_text += str({'candidate_sequence': candidate_sequence}) + '\\n'\n",
    "        \n",
    "        # Query section\n",
    "        query_prompt = 'This is the query sequence:\\n'\n",
    "        query_sequence = {\n",
    "            'query_stock': query_stock,\n",
    "            'query_date': query_date,\n",
    "            'recent_date_list': query.get('recent_date_list', []),\n",
    "            'adjusted_close_list': query.get('adjusted_close_list', [])\n",
    "        }\n",
    "        \n",
    "        query_instruction = f'\\nQuery: On {query_date}, the movement of ${query_stock} is [blank].\\n'\n",
    "        \n",
    "        # Combine all parts\n",
    "        full_prompt = instruction + retrieve_prompt + candidate_text + '\\n' + query_prompt + str(query_sequence) + '\\n' + query_instruction\n",
    "        \n",
    "        return full_prompt, candidate_movement_count\n",
    "    \n",
    "    def ask_llm(self, prompt: str) -> str:\n",
    "        \"\"\"Get prediction from current LLM\"\"\"\n",
    "        if self.current_llm is None:\n",
    "            raise RuntimeError(\"No LLM loaded\")\n",
    "        \n",
    "        # Format prompt based on LLM architecture\n",
    "        if 'Llama' in self.current_llm_name:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst. Use the historical examples to predict stock movements accurately.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            formatted_prompt = self.current_tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt = f\"System: You are a financial analyst.\\nUser: {prompt}\\nAssistant:\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        input_ids = self.current_tokenizer.encode(\n",
    "            formatted_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=1500  # Longer for retrieval context\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_llm.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.current_tokenizer.eos_token_id,\n",
    "                eos_token_id=self.current_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "        response = self.current_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def extract_prediction(self, response: str, reference: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Extract rise/fall prediction from LLM response\"\"\"\n",
    "        response_clean = response.lower().strip()\n",
    "        reference_clean = reference.lower().strip()\n",
    "        \n",
    "        if 'rise' in response_clean:\n",
    "            prediction = 'rise'\n",
    "        elif 'fall' in response_clean:\n",
    "            prediction = 'fall'\n",
    "        else:\n",
    "            prediction = 'freeze'\n",
    "        \n",
    "        correct = (prediction == reference_clean)\n",
    "        return prediction, correct\n",
    "    \n",
    "    def run_single_llm_experiment(self, llm_name: str, output_dir: str, k_candidates: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Run random retrieval experiment for a single LLM\"\"\"\n",
    "        logger.info(f\"🚀 Starting RANDOM RETRIEVAL experiment with {llm_name} (k={k_candidates})\")\n",
    "        \n",
    "        # Load the specified LLM\n",
    "        self.load_llm(llm_name)\n",
    "        \n",
    "        results = []\n",
    "        processed_count = 0\n",
    "        correct_count = 0\n",
    "        \n",
    "        for i, query in enumerate(self.test_queries):\n",
    "            query_id = query['query_id']\n",
    "            ground_truth = self.gt_lookup.get(query_id)\n",
    "            \n",
    "            if not ground_truth:\n",
    "                continue\n",
    "            \n",
    "            reference_answer = ground_truth['actual_movement']\n",
    "            \n",
    "            # Skip freeze movements\n",
    "            if reference_answer == 'freeze':\n",
    "                continue\n",
    "            \n",
    "            # Progress logging\n",
    "            if (i + 1) % 50 == 0:\n",
    "                accuracy = correct_count / processed_count if processed_count > 0 else 0\n",
    "                logger.info(f\"Progress: {i+1}/{len(self.test_queries)} | Accuracy so far: {accuracy:.3f}\")\n",
    "            \n",
    "            try:\n",
    "                # Get qualified candidates (temporal safety)\n",
    "                qualified_candidates = self.get_qualified_candidates(query['query_date'])\n",
    "                \n",
    "                if len(qualified_candidates) == 0:\n",
    "                    logger.warning(f\"No qualified candidates for query {query_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Randomly sample k candidates\n",
    "                random_candidates = self.sample_random_candidates(qualified_candidates, k_candidates)\n",
    "                \n",
    "                # Generate prompt with random candidates\n",
    "                prompt, candidate_movement_count = self.generate_random_retrieval_prompt(query, random_candidates)\n",
    "                \n",
    "                # Get LLM prediction\n",
    "                llm_response = self.ask_llm(prompt)\n",
    "                \n",
    "                # Extract and validate prediction\n",
    "                prediction, correct = self.extract_prediction(llm_response, reference_answer)\n",
    "                \n",
    "                # Track accuracy\n",
    "                processed_count += 1\n",
    "                if correct:\n",
    "                    correct_count += 1\n",
    "                \n",
    "                # Store result\n",
    "                result = {\n",
    "                    'llm_name': llm_name,\n",
    "                    'query_id': query_id,\n",
    "                    'query_stock': query.get('query_stock', ''),\n",
    "                    'query_date': query.get('query_date', ''),\n",
    "                    'method': 'random_retrieval',\n",
    "                    'prompt': prompt,\n",
    "                    'llm_response': llm_response,\n",
    "                    'prediction': prediction,\n",
    "                    'reference': reference_answer,\n",
    "                    'correct': correct,\n",
    "                    'candidate_count': len(random_candidates),\n",
    "                    'candidate_movement_dist': str(candidate_movement_count),\n",
    "                    'k_candidates': k_candidates,\n",
    "                    'total_qualified_candidates': len(qualified_candidates)\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing query {query_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save detailed results\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f'{llm_name}_random_retrieval_k{k_candidates}_results.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_predictions = len(df)\n",
    "        overall_accuracy = df['correct'].mean() if total_predictions > 0 else 0\n",
    "        \n",
    "        # Class-specific metrics\n",
    "        rise_df = df[df['reference'] == 'rise']\n",
    "        fall_df = df[df['reference'] == 'fall']\n",
    "        \n",
    "        rise_accuracy = rise_df['correct'].mean() if len(rise_df) > 0 else 0\n",
    "        fall_accuracy = fall_df['correct'].mean() if len(fall_df) > 0 else 0\n",
    "        \n",
    "        # Candidate statistics\n",
    "        avg_candidates_used = df['candidate_count'].mean() if total_predictions > 0 else 0\n",
    "        \n",
    "        # Log results\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"RANDOM RETRIEVAL RESULTS - {llm_name}\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Total Predictions: {total_predictions}\")\n",
    "        logger.info(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        logger.info(f\"Rise Accuracy: {rise_accuracy:.4f} ({len(rise_df)} samples)\")\n",
    "        logger.info(f\"Fall Accuracy: {fall_accuracy:.4f} ({len(fall_df)} samples)\")\n",
    "        logger.info(f\"Avg Candidates Used: {avg_candidates_used:.1f}\")\n",
    "        logger.info(f\"Results saved to: {output_file}\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def run_multi_llm_experiment(self, llm_list: List[str], output_dir: str, k_candidates: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Run random retrieval experiment across multiple LLMs\"\"\"\n",
    "        logger.info(f\"🚀 Starting MULTI-LLM RANDOM RETRIEVAL experiment\")\n",
    "        logger.info(f\"Testing LLMs: {llm_list}\")\n",
    "        logger.info(f\"Candidates per query: {k_candidates}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for llm_name in llm_list:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"TESTING LLM: {llm_name}\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                # Run experiment for this LLM\n",
    "                llm_results = self.run_single_llm_experiment(llm_name, output_dir, k_candidates)\n",
    "                all_results.append(llm_results)\n",
    "                \n",
    "                logger.info(f\"✅ Completed {llm_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed {llm_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        if all_results:\n",
    "            combined_df = pd.concat(all_results, ignore_index=True)\n",
    "            \n",
    "            # Save combined results\n",
    "            combined_file = os.path.join(output_dir, f'all_llms_random_retrieval_k{k_candidates}_combined.csv')\n",
    "            combined_df.to_csv(combined_file, index=False)\n",
    "            \n",
    "            # Generate comparison report\n",
    "            self._generate_comparison_report(combined_df, output_dir, k_candidates)\n",
    "            \n",
    "            logger.info(f\"\\n✅ Multi-LLM random retrieval experiment completed!\")\n",
    "            logger.info(f\"Combined results saved to: {combined_file}\")\n",
    "            \n",
    "            return combined_df\n",
    "        else:\n",
    "            logger.error(\"❌ No successful experiments!\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def run_k_ablation_study(self, llm_name: str, output_dir: str, k_values: List[int] = [1, 3, 5, 10]) -> pd.DataFrame:\n",
    "        \"\"\"Run ablation study on number of random candidates\"\"\"\n",
    "        logger.info(f\"🔬 Starting K-ABLATION STUDY for {llm_name}\")\n",
    "        logger.info(f\"Testing k values: {k_values}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for k in k_values:\n",
    "            logger.info(f\"\\n{'='*40}\")\n",
    "            logger.info(f\"TESTING k={k} candidates\")\n",
    "            logger.info(f\"{'='*40}\")\n",
    "            \n",
    "            try:\n",
    "                # Run experiment with k candidates\n",
    "                k_results = self.run_single_llm_experiment(llm_name, output_dir, k)\n",
    "                all_results.append(k_results)\n",
    "                \n",
    "                logger.info(f\"✅ Completed k={k}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed k={k}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Analyze k-ablation results\n",
    "        if all_results:\n",
    "            self._analyze_k_ablation(all_results, llm_name, output_dir, k_values)\n",
    "            \n",
    "            combined_df = pd.concat(all_results, ignore_index=True)\n",
    "            return combined_df\n",
    "        else:\n",
    "            logger.error(\"❌ K-ablation study failed!\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _analyze_k_ablation(self, results_list: List[pd.DataFrame], llm_name: str, output_dir: str, k_values: List[int]):\n",
    "        \"\"\"Analyze effect of different k values\"\"\"\n",
    "        \n",
    "        k_analysis = []\n",
    "        \n",
    "        for i, df in enumerate(results_list):\n",
    "            k = k_values[i]\n",
    "            \n",
    "            accuracy = df['correct'].mean()\n",
    "            rise_acc = df[df['reference'] == 'rise']['correct'].mean()\n",
    "            fall_acc = df[df['reference'] == 'fall']['correct'].mean()\n",
    "            \n",
    "            k_analysis.append({\n",
    "                'k_candidates': k,\n",
    "                'overall_accuracy': accuracy,\n",
    "                'rise_accuracy': rise_acc,\n",
    "                'fall_accuracy': fall_acc,\n",
    "                'total_predictions': len(df)\n",
    "            })\n",
    "        \n",
    "        # Save k-ablation analysis\n",
    "        k_df = pd.DataFrame(k_analysis)\n",
    "        k_file = os.path.join(output_dir, f'{llm_name}_k_ablation_analysis.csv')\n",
    "        k_df.to_csv(k_file, index=False)\n",
    "        \n",
    "        # Print analysis\n",
    "        logger.info(f\"\\nK-ABLATION ANALYSIS - {llm_name}\")\n",
    "        logger.info(\"=\"*50)\n",
    "        for result in k_analysis:\n",
    "            logger.info(f\"k={result['k_candidates']:2d} | Accuracy: {result['overall_accuracy']:.4f} | \"\n",
    "                       f\"Rise: {result['rise_accuracy']:.3f} | Fall: {result['fall_accuracy']:.3f}\")\n",
    "        \n",
    "        logger.info(f\"\\nK-ablation analysis saved to: {k_file}\")\n",
    "    \n",
    "    def _generate_comparison_report(self, combined_df: pd.DataFrame, output_dir: str, k_candidates: int):\n",
    "        \"\"\"Generate comparison report across LLMs\"\"\"\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        for llm_name in combined_df['llm_name'].unique():\n",
    "            llm_df = combined_df[combined_df['llm_name'] == llm_name]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total = len(llm_df)\n",
    "            accuracy = llm_df['correct'].mean()\n",
    "            \n",
    "            rise_df = llm_df[llm_df['reference'] == 'rise']\n",
    "            fall_df = llm_df[llm_df['reference'] == 'fall']\n",
    "            \n",
    "            rise_acc = rise_df['correct'].mean() if len(rise_df) > 0 else 0\n",
    "            fall_acc = fall_df['correct'].mean() if len(fall_df) > 0 else 0\n",
    "            \n",
    "            # Prediction distribution\n",
    "            pred_dist = llm_df['prediction'].value_counts()\n",
    "            \n",
    "            # Candidate statistics\n",
    "            avg_candidates = llm_df['candidate_count'].mean()\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'llm_name': llm_name,\n",
    "                'total_predictions': total,\n",
    "                'overall_accuracy': accuracy,\n",
    "                'rise_accuracy': rise_acc,\n",
    "                'fall_accuracy': fall_acc,\n",
    "                'rise_predictions': len(rise_df),\n",
    "                'fall_predictions': len(fall_df),\n",
    "                'predicted_rise': pred_dist.get('rise', 0),\n",
    "                'predicted_fall': pred_dist.get('fall', 0),\n",
    "                'predicted_freeze': pred_dist.get('freeze', 0),\n",
    "                'avg_candidates_used': avg_candidates,\n",
    "                'k_candidates': k_candidates,\n",
    "                'method': 'random_retrieval'\n",
    "            })\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        comparison_file = os.path.join(output_dir, f'random_retrieval_k{k_candidates}_llm_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_file, index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(\"\\n\" + \"=\"*80)\n",
    "        logger.info(f\"RANDOM RETRIEVAL (k={k_candidates}) - LLM COMPARISON SUMMARY\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        for result in comparison_results:\n",
    "            logger.info(f\"{result['llm_name']:15} | Accuracy: {result['overall_accuracy']:.4f} | \"\n",
    "                       f\"Rise: {result['rise_accuracy']:.3f} | Fall: {result['fall_accuracy']:.3f} | \"\n",
    "                       f\"Candidates: {result['avg_candidates_used']:.1f}\")\n",
    "        \n",
    "        logger.info(f\"\\nDetailed comparison saved to: {comparison_file}\")\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "def run_random_retrieval_experiment():\n",
    "    \"\"\"Main function to run the random retrieval experiment\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    test_queries_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/test_queries_rise_fall_only.json'\n",
    "    ground_truth_file = '/root/nfs/AJ FinRag/Evaluation Results/Test Queries/ground_truth_rise_fall_only.json'\n",
    "    embeddings_dir = '/root/nfs/AJ FinRag/Embeddings/embeddings/test/FinQuest'\n",
    "    output_dir = 'random_retrieval_experiments'\n",
    "    \n",
    "    # LLMs to test\n",
    "    test_llms = [\n",
    "        'StockLLM',      # Specialized financial model\n",
    "        'Qwen2.5-1.5B',   # Small general model\n",
    "        'Llama3.2-3B'    # Medium general model\n",
    "    ]\n",
    "    \n",
    "    # Experimental parameters\n",
    "    k_candidates = 5  # Number of random candidates to retrieve\n",
    "    \n",
    "    try:\n",
    "        # Initialize experiment\n",
    "        experiment = RandomRetrievalExperiment(test_queries_file, ground_truth_file, embeddings_dir)\n",
    "        \n",
    "        # Option 1: Run multi-LLM experiment with fixed k\n",
    "        logger.info(\"🚀 RUNNING MULTI-LLM RANDOM RETRIEVAL EXPERIMENT\")\n",
    "        results = experiment.run_multi_llm_experiment(test_llms, output_dir, k_candidates)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            logger.info(\"🎉 RANDOM RETRIEVAL EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "            logger.info(f\"📊 Total results: {len(results)} predictions across {len(test_llms)} LLMs\")\n",
    "        \n",
    "        # Option 2: Run k-ablation study (uncomment to run)\n",
    "        # logger.info(\"🔬 RUNNING K-ABLATION STUDY\")\n",
    "        # ablation_results = experiment.run_k_ablation_study('StockLLM', output_dir, [1, 3, 5, 10])\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_random_retrieval_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108373d7-2638-4da8-b1c5-ed9429200229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
