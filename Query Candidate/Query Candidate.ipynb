{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44946679-5233-4623-88d7-761d94bd1705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating LLM Training Data for All Companies ===\n",
      "Loading processed data from: /root/nfs/AJ FinRag/Company Processed Data/all_companies_processed.json\n",
      "Loaded 18800 records for 25 companies\n",
      "Date range: 2022-01-03 00:00:00 to 2024-12-30 00:00:00\n",
      "\n",
      "Dataset shape: (18800, 19)\n",
      "Date range: 2022-01-03 00:00:00 to 2024-12-30 00:00:00\n",
      "Companies: 25\n",
      "Sample companies: ['AAPL', 'ADBE', 'AMD', 'AMZN', 'BAC', 'CRM', 'CVX', 'GOOGL', 'HD', 'INTC']\n",
      "\n",
      "Available columns:\n",
      "  date: 18800/18800 non-null values\n",
      "  ticker: 18800/18800 non-null values\n",
      "  open: 18800/18800 non-null values\n",
      "  high: 18800/18800 non-null values\n",
      "  low: 18800/18800 non-null values\n",
      "  close: 18800/18800 non-null values\n",
      "  volume: 18800/18800 non-null values\n",
      "  adj_close: 18800/18800 non-null values\n",
      "  MACD_Histogram: 18800/18800 non-null values\n",
      "  macd_crossover: 18800/18800 non-null values\n",
      "  bollinger_bands: 1938/18800 non-null values\n",
      "  exceeding_upper: 1036/18800 non-null values\n",
      "  exceeding_lower: 902/18800 non-null values\n",
      "  overbought_and_oversold_conditions: 5944/18800 non-null values\n",
      "  kdj_crossover: 7521/18800 non-null values\n",
      "  Returns: 18775/18800 non-null values\n",
      "  VWAP: 18800/18800 non-null values\n",
      "  alpha_smr: 18325/18800 non-null values\n",
      "  alpha_mom: 18325/18800 non-null values\n",
      "\n",
      "=== Generating Individual Company Files ===\n",
      "Adding movement labels...\n",
      "  adj_close: 18800 non-null values\n",
      "  close: 18800 non-null values\n",
      "  high: 18800 non-null values\n",
      "  low: 18800 non-null values\n",
      "  open: 18800 non-null values\n",
      "  volume: 18800 non-null values\n",
      "  MACD_Histogram: 18800 non-null values\n",
      "  VWAP: 18800 non-null values\n",
      "  alpha_smr: 18325 non-null values\n",
      "  alpha_mom: 18325 non-null values\n",
      "  Returns: 18775 non-null values\n",
      "Available indicators: ['adj_close', 'close', 'high', 'low', 'open', 'volume', 'MACD_Histogram', 'VWAP', 'alpha_smr', 'alpha_mom', 'Returns']\n",
      "Processing 25 companies individually...\n",
      "Processing AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189/880546261.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_with_movement = df.groupby('ticker').apply(label_movement).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AAPL: 742 queries, 8131 candidates\n",
      "Processing ADBE...\n",
      "  ADBE: 742 queries, 8131 candidates\n",
      "Processing AMD...\n",
      "  AMD: 742 queries, 8131 candidates\n",
      "Processing AMZN...\n",
      "  AMZN: 742 queries, 8131 candidates\n",
      "Processing BAC...\n",
      "  BAC: 742 queries, 8131 candidates\n",
      "Processing CRM...\n",
      "  CRM: 742 queries, 8131 candidates\n",
      "Processing CVX...\n",
      "  CVX: 742 queries, 8131 candidates\n",
      "Processing GOOGL...\n",
      "  GOOGL: 742 queries, 8131 candidates\n",
      "Processing HD...\n",
      "  HD: 742 queries, 8131 candidates\n",
      "Processing INTC...\n",
      "  INTC: 742 queries, 8131 candidates\n",
      "Processing JNJ...\n",
      "  JNJ: 742 queries, 8131 candidates\n",
      "Processing JPM...\n",
      "  JPM: 742 queries, 8131 candidates\n",
      "Processing MA...\n",
      "  MA: 742 queries, 8131 candidates\n",
      "Processing MCD...\n",
      "  MCD: 742 queries, 8131 candidates\n",
      "Processing META...\n",
      "  META: 742 queries, 8131 candidates\n",
      "Processing MSFT...\n",
      "  MSFT: 742 queries, 8131 candidates\n",
      "Processing NFLX...\n",
      "  NFLX: 742 queries, 8131 candidates\n",
      "Processing NKE...\n",
      "  NKE: 742 queries, 8131 candidates\n",
      "Processing NVDA...\n",
      "  NVDA: 742 queries, 8131 candidates\n",
      "Processing PFE...\n",
      "  PFE: 742 queries, 8131 candidates\n",
      "Processing TSLA...\n",
      "  TSLA: 742 queries, 8131 candidates\n",
      "Processing UNH...\n",
      "  UNH: 742 queries, 8131 candidates\n",
      "Processing V...\n",
      "  V: 742 queries, 8131 candidates\n",
      "Processing WMT...\n",
      "  WMT: 742 queries, 8131 candidates\n",
      "Processing XOM...\n",
      "  XOM: 742 queries, 8131 candidates\n",
      "\n",
      "✅ Individual company files saved to: company_data\n",
      "Summary statistics saved to: company_data/company_data_summary.csv\n",
      "\n",
      "=== Generating Combined Dataset ===\n",
      "Adding movement labels...\n",
      "Available indicators: ['adj_close', 'close', 'high', 'low', 'open', 'volume', 'MACD_Histogram', 'VWAP', 'alpha_smr', 'alpha_mom', 'Returns']\n",
      "Processing 25 companies for combined dataset...\n",
      "Processing AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189/880546261.py:201: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_with_movement = df.groupby('ticker').apply(label_movement).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AAPL: 742 queries, 8131 candidates\n",
      "Processing ADBE...\n",
      "  ADBE: 742 queries, 8131 candidates\n",
      "Processing AMD...\n",
      "  AMD: 742 queries, 8131 candidates\n",
      "Processing AMZN...\n",
      "  AMZN: 742 queries, 8131 candidates\n",
      "Processing BAC...\n",
      "  BAC: 742 queries, 8131 candidates\n",
      "Processing CRM...\n",
      "  CRM: 742 queries, 8131 candidates\n",
      "Processing CVX...\n",
      "  CVX: 742 queries, 8131 candidates\n",
      "Processing GOOGL...\n",
      "  GOOGL: 742 queries, 8131 candidates\n",
      "Processing HD...\n",
      "  HD: 742 queries, 8131 candidates\n",
      "Processing INTC...\n",
      "  INTC: 742 queries, 8131 candidates\n",
      "Processing JNJ...\n",
      "  JNJ: 742 queries, 8131 candidates\n",
      "Processing JPM...\n",
      "  JPM: 742 queries, 8131 candidates\n",
      "Processing MA...\n",
      "  MA: 742 queries, 8131 candidates\n",
      "Processing MCD...\n",
      "  MCD: 742 queries, 8131 candidates\n",
      "Processing META...\n",
      "  META: 742 queries, 8131 candidates\n",
      "Processing MSFT...\n",
      "  MSFT: 742 queries, 8131 candidates\n",
      "Processing NFLX...\n",
      "  NFLX: 742 queries, 8131 candidates\n",
      "Processing NKE...\n",
      "  NKE: 742 queries, 8131 candidates\n",
      "Processing NVDA...\n",
      "  NVDA: 742 queries, 8131 candidates\n",
      "Processing PFE...\n",
      "  PFE: 742 queries, 8131 candidates\n",
      "Processing TSLA...\n",
      "  TSLA: 742 queries, 8131 candidates\n",
      "Processing UNH...\n",
      "  UNH: 742 queries, 8131 candidates\n",
      "Processing V...\n",
      "  V: 742 queries, 8131 candidates\n",
      "Processing WMT...\n",
      "  WMT: 742 queries, 8131 candidates\n",
      "Processing XOM...\n",
      "  XOM: 742 queries, 8131 candidates\n",
      "\n",
      "Total: 18550 queries, 203275 candidates\n",
      "✅ Combined data saved:\n",
      "  Queries: llm_data/all_companies_train_qlist.json\n",
      "  Candidates: llm_data/all_companies_train_clist.json\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total companies processed: 25\n",
      "Total queries across all companies: 18550\n",
      "Total candidates across all companies: 203275\n",
      "Average queries per company: 742.0\n",
      "Average candidates per company: 8131.0\n",
      "\n",
      "Top 10 companies by query count:\n",
      "  AAPL: 742 queries, 8131 candidates\n",
      "  ADBE: 742 queries, 8131 candidates\n",
      "  AMD: 742 queries, 8131 candidates\n",
      "  AMZN: 742 queries, 8131 candidates\n",
      "  BAC: 742 queries, 8131 candidates\n",
      "  CRM: 742 queries, 8131 candidates\n",
      "  CVX: 742 queries, 8131 candidates\n",
      "  GOOGL: 742 queries, 8131 candidates\n",
      "  HD: 742 queries, 8131 candidates\n",
      "  INTC: 742 queries, 8131 candidates\n",
      "\n",
      "Sample query:\n",
      "{\n",
      "  \"data_index\": 10,\n",
      "  \"query_stock\": \"AAPL\",\n",
      "  \"query_date\": \"2022-01-18\",\n",
      "  \"recent_date_list\": [\n",
      "    \"2022-01-03\",\n",
      "    \"2022-01-04\",\n",
      "    \"2022-01-05\",\n",
      "    \"2022-01-06\",\n",
      "    \"2022-01-07\",\n",
      "    \"2022-01-10\",\n",
      "    \"2022-01-11\",\n",
      "    \"2022-01-12\",\n",
      "    \"2022-01-13\",\n",
      "    \"2022-01-14\"\n",
      "  ],\n",
      "  \"adjusted_close_list\": [\n",
      "    178.4431,\n",
      "    176.1784,\n",
      "    171.4921,\n",
      "    168.6293,\n",
      "    168.796,\n",
      "    168.8156,\n",
      "    171.6489,\n",
      "    172.0901,\n",
      "    168.8156,\n",
      "    169.6783\n",
      "  ],\n",
      "  \"movement\": \"fall\"\n",
      "}\n",
      "\n",
      "Sample candidate:\n",
      "{\n",
      "  \"data_index\": 0,\n",
      "  \"candidate_stock\": \"AAPL\",\n",
      "  \"candidate_date\": \"2022-01-18\",\n",
      "  \"candidate_movement\": \"fall\",\n",
      "  \"recent_date_list\": [\n",
      "    \"2022-01-03\",\n",
      "    \"2022-01-04\",\n",
      "    \"2022-01-05\",\n",
      "    \"2022-01-06\",\n",
      "    \"2022-01-07\",\n",
      "    \"2022-01-10\",\n",
      "    \"2022-01-11\",\n",
      "    \"2022-01-12\",\n",
      "    \"2022-01-13\",\n",
      "    \"2022-01-14\"\n",
      "  ],\n",
      "  \"adj_close_list\": [\n",
      "    178.4431,\n",
      "    176.1784,\n",
      "    171.4921,\n",
      "    168.6293,\n",
      "    168.796,\n",
      "    168.8156,\n",
      "    171.6489,\n",
      "    172.0901,\n",
      "    168.8156,\n",
      "    169.6783\n",
      "  ],\n",
      "  \"indicator_name\": \"adj_close\"\n",
      "}\n",
      "\n",
      "✅ All data preparation complete!\n",
      "Files generated:\n",
      "  - Individual company files in 'company_data/' folder\n",
      "  - Combined dataset in 'llm_data/' folder\n",
      "  - Summary statistics in 'company_data/company_data_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths for processed data\n",
    "processed_data_path = \"/root/nfs/AJ FinRag/Company Processed Data/all_companies_processed.json\"\n",
    "company_data_dir = \"company_data\"\n",
    "llm_data_dir = \"llm_data\"\n",
    "\n",
    "def load_processed_data(file_path):\n",
    "    \"\"\"Load processed data from JSON file and convert timestamps\"\"\"\n",
    "    print(f\"Loading processed data from: {file_path}\")\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            # Convert Unix timestamp (milliseconds) to datetime\n",
    "            record['date'] = pd.to_datetime(record['date'], unit='ms')\n",
    "            data.append(record)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort by ticker and date to ensure proper order\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Loaded {len(df)} records for {df['ticker'].nunique()} companies\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def label_movement(df):\n",
    "    \"\"\"Label stock movements as rise, fall, or freeze based on returns\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate daily returns if not present or use existing Returns column\n",
    "    if 'Returns' in df.columns and df['Returns'].notna().any():\n",
    "        df['return'] = df['Returns'] * 100  # Convert to percentage\n",
    "    else:\n",
    "        df['return'] = df['adj_close'].pct_change() * 100\n",
    "\n",
    "    def classify(r):\n",
    "        if pd.isna(r):  # Handle NaN values\n",
    "            return 'freeze'\n",
    "        if r > 0.55:\n",
    "            return 'rise'\n",
    "        elif r < -0.5:\n",
    "            return 'fall'\n",
    "        return 'freeze'\n",
    "\n",
    "    df['movement'] = df['return'].apply(classify)\n",
    "    return df\n",
    "\n",
    "def generate_queries(df, stock_name, start_date, end_date):\n",
    "    \"\"\"Generate query sequences for the LLM\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Filter by stock and date range\n",
    "    df = df[(df['ticker'] == stock_name) &\n",
    "            (df['date'] >= start_date) &\n",
    "            (df['date'] <= end_date)].reset_index(drop=True)\n",
    "\n",
    "    if len(df) <= 5:\n",
    "        print(f\"Warning: {stock_name} has insufficient data ({len(df)} records)\")\n",
    "        return []\n",
    "\n",
    "    queries = []\n",
    "    for i in range(10, len(df)):\n",
    "        query_date = df.loc[i, \"date\"]\n",
    "        prev_window = df.loc[i-10:i-1]\n",
    "\n",
    "        queries.append({\n",
    "            \"data_index\": i,\n",
    "            \"query_stock\": stock_name,\n",
    "            \"query_date\": query_date.strftime('%Y-%m-%d'),\n",
    "            \"recent_date_list\": [d.strftime('%Y-%m-%d') for d in prev_window[\"date\"]],\n",
    "            \"adjusted_close_list\": [round(float(v), 4) if pd.notna(v) else 0.0 for v in prev_window[\"adj_close\"]],\n",
    "            \"movement\": df.loc[i, \"movement\"]\n",
    "        })\n",
    "\n",
    "    return queries\n",
    "\n",
    "def generate_candidates(df, stock_name, indicators):\n",
    "    \"\"\"Generate candidate sequences for the LLM\"\"\"\n",
    "    # Filter by stock\n",
    "    df = df[df['ticker'] == stock_name].copy().reset_index(drop=True)\n",
    "\n",
    "    if len(df) <= 6:  # Need at least 6 records for candidates\n",
    "        print(f\"Warning: {stock_name} has insufficient data for candidates ({len(df)} records)\")\n",
    "        return []\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(10, len(df) - 1):\n",
    "        candidate_date = df.loc[i, \"date\"]\n",
    "        movement = df.loc[i + 1, \"movement\"]\n",
    "        recent_dates = df.loc[i-10:i-1, \"date\"].tolist()\n",
    "\n",
    "        for ind in indicators:\n",
    "            if ind in df.columns and pd.notna(df.loc[i-5:i-1, ind]).any():  # Check if indicator exists and has data\n",
    "                values = df.loc[i-10:i-1, ind].tolist()\n",
    "                # Handle NaN values in the list\n",
    "                clean_values = [round(float(v), 4) if pd.notna(v) else 0.0 for v in values]\n",
    "\n",
    "                candidates.append({\n",
    "                    \"data_index\": len(candidates),\n",
    "                    \"candidate_stock\": stock_name,\n",
    "                    \"candidate_date\": candidate_date.strftime('%Y-%m-%d'),\n",
    "                    \"candidate_movement\": movement,\n",
    "                    \"recent_date_list\": [d.strftime('%Y-%m-%d') for d in recent_dates],\n",
    "                    f\"{ind}_list\": clean_values,\n",
    "                    \"indicator_name\": ind\n",
    "                })\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def save_jsonl(path, data):\n",
    "    \"\"\"Save data as JSON lines format\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def generate_individual_company_files(df, start_date, end_date, output_dir=\"company_data\"):\n",
    "    \"\"\"Generate separate query and candidate files for each company\"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Add movement labels to the entire dataset\n",
    "    print(\"Adding movement labels...\")\n",
    "    df_with_movement = df.groupby('ticker').apply(label_movement).reset_index(drop=True)\n",
    "\n",
    "    # Available indicators from your data structure\n",
    "    indicators = [\n",
    "        \"adj_close\", \"close\", \"high\", \"low\", \"open\", \"volume\",\n",
    "        \"MACD_Histogram\", \"VWAP\", \"alpha_smr\", \"alpha_mom\", \"Returns\"\n",
    "    ]\n",
    "\n",
    "    # Filter indicators that actually exist in the dataframe and have non-null values\n",
    "    available_indicators = []\n",
    "    for ind in indicators:\n",
    "        if ind in df_with_movement.columns:\n",
    "            non_null_count = df_with_movement[ind].notna().sum()\n",
    "            if non_null_count > 0:\n",
    "                available_indicators.append(ind)\n",
    "                print(f\"  {ind}: {non_null_count} non-null values\")\n",
    "\n",
    "    print(f\"Available indicators: {available_indicators}\")\n",
    "\n",
    "    tickers = df_with_movement['ticker'].unique()\n",
    "    print(f\"Processing {len(tickers)} companies individually...\")\n",
    "\n",
    "    company_stats = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing {ticker}...\")\n",
    "\n",
    "        # Generate queries for this ticker\n",
    "        ticker_queries = generate_queries(df_with_movement, ticker, start_date, end_date)\n",
    "\n",
    "        # Generate candidates for this ticker\n",
    "        ticker_candidates = generate_candidates(df_with_movement, ticker, available_indicators)\n",
    "\n",
    "        if len(ticker_queries) == 0 or len(ticker_candidates) == 0:\n",
    "            print(f\"  Warning: {ticker} has no queries or candidates, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Save individual company files\n",
    "        query_file = os.path.join(output_dir, f\"{ticker}_train_qlist.json\")\n",
    "        candidate_file = os.path.join(output_dir, f\"{ticker}_train_clist.json\")\n",
    "\n",
    "        save_jsonl(query_file, ticker_queries)\n",
    "        save_jsonl(candidate_file, ticker_candidates)\n",
    "\n",
    "        company_stats.append({\n",
    "            'ticker': ticker,\n",
    "            'queries': len(ticker_queries),\n",
    "            'candidates': len(ticker_candidates)\n",
    "        })\n",
    "\n",
    "        print(f\"  {ticker}: {len(ticker_queries)} queries, {len(ticker_candidates)} candidates\")\n",
    "\n",
    "    # Save summary statistics\n",
    "    if company_stats:\n",
    "        stats_df = pd.DataFrame(company_stats)\n",
    "        stats_df.to_csv(os.path.join(output_dir, \"company_data_summary.csv\"), index=False)\n",
    "\n",
    "    print(f\"\\n✅ Individual company files saved to: {output_dir}\")\n",
    "    print(f\"Summary statistics saved to: {os.path.join(output_dir, 'company_data_summary.csv')}\")\n",
    "\n",
    "    return company_stats\n",
    "\n",
    "def generate_combined_company_data(df, start_date, end_date, output_dir=\"llm_data\"):\n",
    "    \"\"\"Generate combined query and candidate data for all companies\"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Add movement labels to the entire dataset\n",
    "    print(\"Adding movement labels...\")\n",
    "    df_with_movement = df.groupby('ticker').apply(label_movement).reset_index(drop=True)\n",
    "\n",
    "    # Available indicators from your data structure\n",
    "    indicators = [\n",
    "        \"adj_close\", \"close\", \"high\", \"low\", \"open\", \"volume\",\n",
    "        \"MACD_Histogram\", \"VWAP\", \"alpha_smr\", \"alpha_mom\", \"Returns\"\n",
    "    ]\n",
    "\n",
    "    # Filter indicators that actually exist in the dataframe and have non-null values\n",
    "    available_indicators = []\n",
    "    for ind in indicators:\n",
    "        if ind in df_with_movement.columns:\n",
    "            non_null_count = df_with_movement[ind].notna().sum()\n",
    "            if non_null_count > 0:\n",
    "                available_indicators.append(ind)\n",
    "\n",
    "    print(f\"Available indicators: {available_indicators}\")\n",
    "\n",
    "    all_queries = []\n",
    "    all_candidates = []\n",
    "\n",
    "    tickers = df_with_movement['ticker'].unique()\n",
    "    print(f\"Processing {len(tickers)} companies for combined dataset...\")\n",
    "\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing {ticker}...\")\n",
    "\n",
    "        # Generate queries for this ticker\n",
    "        ticker_queries = generate_queries(df_with_movement, ticker, start_date, end_date)\n",
    "        all_queries.extend(ticker_queries)\n",
    "\n",
    "        # Generate candidates for this ticker\n",
    "        ticker_candidates = generate_candidates(df_with_movement, ticker, available_indicators)\n",
    "        all_candidates.extend(ticker_candidates)\n",
    "\n",
    "        print(f\"  {ticker}: {len(ticker_queries)} queries, {len(ticker_candidates)} candidates\")\n",
    "\n",
    "    print(f\"\\nTotal: {len(all_queries)} queries, {len(all_candidates)} candidates\")\n",
    "\n",
    "    # Save combined data\n",
    "    queries_path = os.path.join(output_dir, \"all_companies_train_qlist.json\")\n",
    "    candidates_path = os.path.join(output_dir, \"all_companies_train_clist.json\")\n",
    "\n",
    "    save_jsonl(queries_path, all_queries)\n",
    "    save_jsonl(candidates_path, all_candidates)\n",
    "\n",
    "    print(f\"✅ Combined data saved:\")\n",
    "    print(f\"  Queries: {queries_path}\")\n",
    "    print(f\"  Candidates: {candidates_path}\")\n",
    "\n",
    "    return all_queries, all_candidates\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Generating LLM Training Data for All Companies ===\")\n",
    "\n",
    "    # Load processed data (timestamps will be converted automatically)\n",
    "    processed_df = load_processed_data(processed_data_path)\n",
    "\n",
    "    # Check your data\n",
    "    print(f\"\\nDataset shape: {processed_df.shape}\")\n",
    "    print(f\"Date range: {processed_df['date'].min()} to {processed_df['date'].max()}\")\n",
    "    print(f\"Companies: {processed_df['ticker'].nunique()}\")\n",
    "    print(f\"Sample companies: {sorted(processed_df['ticker'].unique())[:10]}\")  # Show first 10\n",
    "\n",
    "    # Show column info\n",
    "    print(f\"\\nAvailable columns:\")\n",
    "    for col in processed_df.columns:\n",
    "        non_null_count = processed_df[col].notna().sum()\n",
    "        print(f\"  {col}: {non_null_count}/{len(processed_df)} non-null values\")\n",
    "\n",
    "    # Generate individual company files\n",
    "    print(\"\\n=== Generating Individual Company Files ===\")\n",
    "    company_stats = generate_individual_company_files(\n",
    "        processed_df,\n",
    "        pd.to_datetime(\"2022-01-03\"),\n",
    "        pd.to_datetime(\"2024-12-30\"),\n",
    "        company_data_dir\n",
    "    )\n",
    "\n",
    "    # Generate combined dataset\n",
    "    print(\"\\n=== Generating Combined Dataset ===\")\n",
    "    all_queries, all_candidates = generate_combined_company_data(\n",
    "        processed_df,\n",
    "        pd.to_datetime(\"2022-01-03\"),\n",
    "        pd.to_datetime(\"2024-12-30\"),\n",
    "        llm_data_dir\n",
    "    )\n",
    "\n",
    "    # Display summary statistics\n",
    "    if company_stats:\n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        stats_df = pd.DataFrame(company_stats)\n",
    "        print(f\"Total companies processed: {len(stats_df)}\")\n",
    "        print(f\"Total queries across all companies: {stats_df['queries'].sum()}\")\n",
    "        print(f\"Total candidates across all companies: {stats_df['candidates'].sum()}\")\n",
    "        print(f\"Average queries per company: {stats_df['queries'].mean():.1f}\")\n",
    "        print(f\"Average candidates per company: {stats_df['candidates'].mean():.1f}\")\n",
    "\n",
    "        # Show top 10 companies by data volume\n",
    "        print(\"\\nTop 10 companies by query count:\")\n",
    "        top_companies = stats_df.nlargest(10, 'queries')\n",
    "        for _, row in top_companies.iterrows():\n",
    "            print(f\"  {row['ticker']}: {row['queries']} queries, {row['candidates']} candidates\")\n",
    "\n",
    "    # Show sample data\n",
    "    if all_queries:\n",
    "        print(\"\\nSample query:\")\n",
    "        print(json.dumps(all_queries[0], indent=2))\n",
    "\n",
    "    if all_candidates:\n",
    "        print(\"\\nSample candidate:\")\n",
    "        print(json.dumps(all_candidates[0], indent=2))\n",
    "\n",
    "    print(\"\\n✅ All data preparation complete!\")\n",
    "    print(\"Files generated:\")\n",
    "    print(f\"  - Individual company files in '{company_data_dir}/' folder\")\n",
    "    print(f\"  - Combined dataset in '{llm_data_dir}/' folder\")\n",
    "    if company_stats:\n",
    "        print(f\"  - Summary statistics in '{company_data_dir}/company_data_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563269d2-da4a-4a21-92d2-f6bebd509c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
