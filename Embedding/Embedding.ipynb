{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f1a643-d813-43b6-892a-a9a955dd977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: FIXED FINQUEST EMBEDDING GENERATION\n",
      "INFO:__main__:======================================================================\n",
      "INFO:__main__:Dataset: your_dataset\n",
      "INFO:__main__:Embedder: FinQuest\n",
      "INFO:__main__:Device: cuda\n",
      "INFO:__main__:======================================================================\n",
      "INFO:__main__: Starting FIXED FinQuest embedding generation (both query and candidate)\n",
      "INFO:__main__:Step 0: Testing embedding quality...\n",
      "INFO:__main__:ðŸ§ª Testing embedding quality...\n",
      "INFO:__main__:Loading trained FinQuest model...\n",
      "INFO:__main__: FinQuest model loaded and validated successfully\n",
      "INFO:__main__:Embedding test results:\n",
      "INFO:__main__:  Min similarity: 0.999875\n",
      "INFO:__main__:  Max similarity: 0.999902\n",
      "INFO:__main__:  Mean similarity: 0.999885\n",
      "ERROR:__main__: EMBEDDING COLLAPSE DETECTED in generated embeddings!\n",
      "ERROR:__main__: Embedding quality test failed - your model has collapsed embeddings!\n",
      "ERROR:__main__:   You need to retrain your FinQuest model before proceeding.\n",
      "ERROR:__main__: FAILED: Embedding generation failed!\n",
      "ERROR:__main__:   Please check the error messages above and fix the issues.\n",
      "INFO:__main__:\n",
      " TROUBLESHOOTING GUIDE:\n",
      "INFO:__main__:1. If embedding quality test failed:\n",
      "INFO:__main__:   â†’ Your FinQuest model has collapsed embeddings\n",
      "INFO:__main__:   â†’ You need to retrain the model with better hyperparameters\n",
      "INFO:__main__:2. If file loading failed:\n",
      "INFO:__main__:   â†’ Check if your data files exist at the specified paths\n",
      "INFO:__main__:   â†’ Verify JSON format in your data files\n",
      "INFO:__main__:3. If model loading failed:\n",
      "INFO:__main__:   â†’ Check if your model checkpoint exists\n",
      "INFO:__main__:   â†’ Verify the model was trained successfully\n"
     ]
    }
   ],
   "source": [
    "# FIXED FinQuest Embedding Generation\n",
    "# Addresses potential issues causing embedding collapse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Disable warnings\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class FinQuestRetrieverFixed(torch.nn.Module):\n",
    "    \"\"\"FIXED: FinQuest model with better embedding generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2', hidden_size=384, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        encoder_dim = self.encoder.config.hidden_size\n",
    "        self.projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoder_dim, hidden_size * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(hidden_size * 2, hidden_size),\n",
    "            torch.nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.financial_attention = torch.nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def encode_sequence(self, sequences):\n",
    "        \"\"\"FIXED: Encode sequences with better error handling and validation\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        if not sequences or all(not seq.strip() for seq in sequences):\n",
    "            logger.warning(\"Empty sequences provided!\")\n",
    "            return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "        \n",
    "        # FIXED: Better tokenization with error handling\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                sequences,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenization error: {e}\")\n",
    "            return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "        \n",
    "        # FIXED: Ensure model is in eval mode during inference\n",
    "        was_training = self.training\n",
    "        if was_training:\n",
    "            self.eval()\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():  # FIXED: Ensure no gradients\n",
    "                # Encoder forward pass\n",
    "                outputs = self.encoder(**inputs)\n",
    "                embeddings = self.mean_pooling(outputs, inputs['attention_mask'])\n",
    "                \n",
    "                # FIXED: Check for NaN or extreme values\n",
    "                if torch.isnan(embeddings).any():\n",
    "                    logger.error(\"NaN detected in encoder embeddings!\")\n",
    "                    return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "                \n",
    "                if torch.abs(embeddings).max() > 100:\n",
    "                    logger.warning(f\"Extreme values in embeddings: {torch.abs(embeddings).max().item()}\")\n",
    "                \n",
    "                # Projection layer\n",
    "                projected = self.projection(embeddings.float())\n",
    "                \n",
    "                # FIXED: Check projection output\n",
    "                if torch.isnan(projected).any():\n",
    "                    logger.error(\"NaN detected in projection!\")\n",
    "                    return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "                \n",
    "                # Attention mechanism  \n",
    "                attended, _ = self.financial_attention(\n",
    "                    projected.unsqueeze(1),\n",
    "                    projected.unsqueeze(1), \n",
    "                    projected.unsqueeze(1)\n",
    "                )\n",
    "                attended = attended.squeeze(1)\n",
    "                \n",
    "                # FIXED: Check attention output\n",
    "                if torch.isnan(attended).any():\n",
    "                    logger.error(\"NaN detected in attention!\")\n",
    "                    return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "                \n",
    "                # FIXED: Apply dropout only during training, not inference\n",
    "                if self.training:\n",
    "                    attended = self.dropout(attended)\n",
    "                \n",
    "                # FIXED: L2 normalization with epsilon to prevent division by zero\n",
    "                final_embeddings = torch.nn.functional.normalize(attended, p=2, dim=1, eps=1e-8)\n",
    "                \n",
    "                # FIXED: Final validation\n",
    "                if torch.isnan(final_embeddings).any():\n",
    "                    logger.error(\"NaN detected in final embeddings!\")\n",
    "                    return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "                \n",
    "                # FIXED: Ensure embeddings are properly normalized\n",
    "                norms = torch.norm(final_embeddings, dim=1)\n",
    "                if (norms < 0.9).any() or (norms > 1.1).any():\n",
    "                    logger.warning(f\"Embedding normalization issue. Norms range: {norms.min().item():.6f} to {norms.max().item():.6f}\")\n",
    "                \n",
    "                return final_embeddings\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in encode_sequence: {e}\")\n",
    "            return torch.zeros(len(sequences), self.hidden_size).to(device)\n",
    "        \n",
    "        finally:\n",
    "            # Restore training mode\n",
    "            if was_training:\n",
    "                self.train()\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"FIXED: Mean pooling with better numerical stability\"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        # FIXED: Better numerical stability\n",
    "        masked_embeddings = token_embeddings * input_mask_expanded\n",
    "        summed_embeddings = torch.sum(masked_embeddings, 1)\n",
    "        summed_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        return summed_embeddings / summed_mask\n",
    "\n",
    "class EmbedderFixed:\n",
    "    \"\"\"FIXED: Embedding class with better error handling and validation\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder_name, device='cuda'):\n",
    "        self.embedder_name = embedder_name\n",
    "        self.device = device\n",
    "        self.root = '/root/nfs/AJ FinRag/Models/'\n",
    "        \n",
    "        if self.embedder_name == 'FinQuest':\n",
    "            logger.info('Loading trained FinQuest model...')\n",
    "            model_path = os.path.join(self.root, 'finquest_models/finquest_retriever_best.pth')\n",
    "            self.model = self._load_finquest_model(model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            raise ValueError(f\"Embedder {embedder_name} not supported. Only 'FinQuest' is available.\")\n",
    "    \n",
    "    def _load_finquest_model(self, model_path):\n",
    "        \"\"\"FIXED: Load model with better error handling and validation\"\"\"\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Validate checkpoint\n",
    "        if 'model_state_dict' not in checkpoint:\n",
    "            raise KeyError(\"No 'model_state_dict' found in checkpoint\")\n",
    "        \n",
    "        # Check for NaN parameters\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        nan_params = []\n",
    "        for name, param in state_dict.items():\n",
    "            if torch.isnan(param).any():\n",
    "                nan_params.append(name)\n",
    "        \n",
    "        if nan_params:\n",
    "            logger.error(f\"NaN parameters found in checkpoint: {nan_params}\")\n",
    "            raise ValueError(\"Checkpoint contains NaN parameters - model is corrupted\")\n",
    "        \n",
    "        # Create and load model\n",
    "        model = FinQuestRetrieverFixed().to(self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        # Attach tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        model.tokenizer = tokenizer\n",
    "        \n",
    "        # Set to eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        # FIXED: Test model with dummy input to ensure it works\n",
    "        test_input = \"Test sequence for validation\"\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                test_embedding = model.encode_sequence([test_input])\n",
    "                if torch.isnan(test_embedding).any():\n",
    "                    raise ValueError(\"Model produces NaN embeddings\")\n",
    "                if torch.norm(test_embedding) < 1e-6:\n",
    "                    logger.warning(\"Model produces very small embeddings - possible collapse\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model validation failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        logger.info(f\" FinQuest model loaded and validated successfully\")\n",
    "        return model\n",
    "    \n",
    "    def embed_sequences_batch(self, sequences, batch_size=32):\n",
    "        \"\"\"FIXED: Batch processing with better memory management\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = sequences[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = self.model.encode_sequence(batch)\n",
    "                    \n",
    "                    # Validate batch results\n",
    "                    if torch.isnan(batch_embeddings).any():\n",
    "                        logger.error(f\"NaN embeddings in batch {i//batch_size + 1}\")\n",
    "                        # Replace NaN with zeros\n",
    "                        batch_embeddings = torch.where(\n",
    "                            torch.isnan(batch_embeddings),\n",
    "                            torch.zeros_like(batch_embeddings),\n",
    "                            batch_embeddings\n",
    "                        )\n",
    "                    \n",
    "                    # Move to CPU to save GPU memory\n",
    "                    batch_embeddings = batch_embeddings.cpu().numpy()\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "                # Add zero embeddings for failed batch\n",
    "                zero_embeddings = np.zeros((len(batch), self.model.hidden_size))\n",
    "                all_embeddings.extend(zero_embeddings)\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def embed_queries_in_parallel(self, queries_on_date):\n",
    "        \"\"\"FIXED: Embed queries with better error handling\"\"\"\n",
    "        query_strs = []\n",
    "        \n",
    "        for query in queries_on_date:\n",
    "            try:\n",
    "                query_str = self._get_query_str(query)\n",
    "                query_strs.append(query_str)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting query to string: {e}\")\n",
    "                # Use a default string for failed queries\n",
    "                query_strs.append(\"{'query_stock': 'Unknown', 'query_date': 'Unknown', 'recent_date_list': [], 'adjusted_close_list': []}\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embed_sequences_batch(query_strs)\n",
    "        \n",
    "        # Create results\n",
    "        results = []\n",
    "        for i, query in enumerate(queries_on_date):\n",
    "            results.append({\n",
    "                'data': query,\n",
    "                'embedding': embeddings[i]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def embed_candidates_in_parallel(self, candidates_on_date):\n",
    "        \"\"\"FIXED: Embed candidates with better error handling\"\"\"\n",
    "        candidate_strs = []\n",
    "        \n",
    "        for candidate in candidates_on_date:\n",
    "            try:\n",
    "                candidate_str = self._get_candidate_str(candidate)\n",
    "                candidate_strs.append(candidate_str)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting candidate to string: {e}\")\n",
    "                # Use a default string for failed candidates\n",
    "                candidate_strs.append(\"{'candidate_stock': 'Unknown', 'candidate_date': 'Unknown', 'recent_date_list': [], 'values_list': []}\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embed_sequences_batch(candidate_strs)\n",
    "        \n",
    "        # Create results\n",
    "        results = []\n",
    "        for i, candidate in enumerate(candidates_on_date):\n",
    "            results.append({\n",
    "                'data': candidate,\n",
    "                'embedding': embeddings[i]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_query_str(self, query):\n",
    "        \"\"\"FIXED: Convert query to string with better error handling\"\"\"\n",
    "        if 'query_str' in query and query['query_str']:\n",
    "            return query['query_str']\n",
    "        \n",
    "        # FIXED: Ensure all fields have defaults and are properly formatted\n",
    "        seq_dict = {\n",
    "            'query_stock': str(query.get('query_stock', 'Unknown')),\n",
    "            'query_date': str(query.get('query_date', 'Unknown')),\n",
    "            'recent_date_list': list(query.get('recent_date_list', [])),\n",
    "            'adjusted_close_list': [float(x) if x is not None else 0.0 for x in query.get('adjusted_close_list', [])]\n",
    "        }\n",
    "        \n",
    "        return str(seq_dict)\n",
    "    \n",
    "    def _get_candidate_str(self, candidate):\n",
    "        \"\"\"FIXED: Convert candidate to string with better error handling\"\"\"\n",
    "        if 'candidate_str' in candidate and candidate['candidate_str']:\n",
    "            return candidate['candidate_str']\n",
    "        \n",
    "        # Find the indicator key\n",
    "        indicator_key = None\n",
    "        for key in candidate.keys():\n",
    "            if key.endswith('_list') and key != 'recent_date_list':\n",
    "                indicator_key = key\n",
    "                break\n",
    "        \n",
    "        if indicator_key is None:\n",
    "            indicator_key = 'values_list'\n",
    "            candidate[indicator_key] = []\n",
    "        \n",
    "        # FIXED: Ensure all fields are properly formatted\n",
    "        seq_dict = {\n",
    "            'candidate_stock': str(candidate.get('candidate_stock', 'Unknown')),\n",
    "            'candidate_date': str(candidate.get('candidate_date', 'Unknown')),\n",
    "            'recent_date_list': list(candidate.get('recent_date_list', [])),\n",
    "            indicator_key: [float(x) if x is not None else 0.0 for x in candidate.get(indicator_key, [])]\n",
    "        }\n",
    "        \n",
    "        return str(seq_dict)\n",
    "\n",
    "class DatastoreFixed:\n",
    "    \"\"\"FIXED: Datastore with better error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, test_dataset, mode='test'):\n",
    "        self.test_dataset = test_dataset\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Data paths\n",
    "        if mode == 'test':\n",
    "            self.query_path = '/root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_qlist.json'\n",
    "            self.candidate_path = '/root/nfs/AJ FinRag/Query Candidate/llm_data/all_companies_train_clist.json'\n",
    "        \n",
    "        # Load data with error handling\n",
    "        self.queries = self._load_json_file_safe(self.query_path)\n",
    "        self.candidates = self._load_json_file_safe(self.candidate_path)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.queries)} queries and {len(self.candidates)} candidates\")\n",
    "        \n",
    "        # Validate data\n",
    "        self._validate_data()\n",
    "    \n",
    "    def _load_json_file_safe(self, file_path):\n",
    "        \"\"\"FIXED: Load JSON file with better error handling\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return []\n",
    "        \n",
    "        data = []\n",
    "        line_num = 0\n",
    "        errors = 0\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line_num += 1\n",
    "                    line = line.strip()\n",
    "                    \n",
    "                    if not line:  # Skip empty lines\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        item = json.loads(line)\n",
    "                        data.append(item)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        errors += 1\n",
    "                        if errors <= 10:  # Log first 10 errors\n",
    "                            logger.warning(f\"JSON decode error on line {line_num}: {e}\")\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        errors += 1\n",
    "                        if errors <= 10:\n",
    "                            logger.warning(f\"Error on line {line_num}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "            return []\n",
    "        \n",
    "        if errors > 0:\n",
    "            logger.warning(f\"Encountered {errors} errors while loading {file_path}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        \"\"\"FIXED: Validate loaded data\"\"\"\n",
    "        # Check queries\n",
    "        valid_queries = 0\n",
    "        for query in self.queries:\n",
    "            if self._is_valid_query(query):\n",
    "                valid_queries += 1\n",
    "        \n",
    "        # Check candidates\n",
    "        valid_candidates = 0\n",
    "        for candidate in self.candidates:\n",
    "            if self._is_valid_candidate(candidate):\n",
    "                valid_candidates += 1\n",
    "        \n",
    "        logger.info(f\"Valid queries: {valid_queries}/{len(self.queries)}\")\n",
    "        logger.info(f\"Valid candidates: {valid_candidates}/{len(self.candidates)}\")\n",
    "        \n",
    "        if valid_queries < len(self.queries) * 0.9:\n",
    "            logger.warning(\"More than 10% of queries are invalid!\")\n",
    "        \n",
    "        if valid_candidates < len(self.candidates) * 0.9:\n",
    "            logger.warning(\"More than 10% of candidates are invalid!\")\n",
    "    \n",
    "    def _is_valid_query(self, query):\n",
    "        \"\"\"Check if query has required fields\"\"\"\n",
    "        required_fields = ['query_stock', 'query_date']\n",
    "        return all(field in query and query[field] for field in required_fields)\n",
    "    \n",
    "    def _is_valid_candidate(self, candidate):\n",
    "        \"\"\"Check if candidate has required fields\"\"\"\n",
    "        required_fields = ['candidate_stock', 'candidate_date']\n",
    "        return all(field in candidate and candidate[field] for field in required_fields)\n",
    "    \n",
    "    def group_no_freeze_query_str_by_date(self):\n",
    "        \"\"\"FIXED: Group queries by date with better filtering\"\"\"\n",
    "        queries_by_date = {}\n",
    "        skipped = 0\n",
    "        \n",
    "        for query in self.queries:\n",
    "            # Skip invalid queries\n",
    "            if not self._is_valid_query(query):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Filter out freeze movements\n",
    "            movement = query.get('movement', 'freeze')\n",
    "            if movement == 'freeze':\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            date = query.get('query_date', 'Unknown')\n",
    "            if date == 'Unknown':\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            if date not in queries_by_date:\n",
    "                queries_by_date[date] = []\n",
    "            queries_by_date[date].append(query)\n",
    "        \n",
    "        logger.info(f\"Grouped queries by date: {len(queries_by_date)} dates, skipped {skipped} invalid/freeze queries\")\n",
    "        return queries_by_date\n",
    "    \n",
    "    def group_candidate_str_by_date(self):\n",
    "        \"\"\"FIXED: Group candidates by date with better filtering\"\"\"\n",
    "        candidates_by_date = {}\n",
    "        skipped = 0\n",
    "        \n",
    "        for candidate in self.candidates:\n",
    "            # Skip invalid candidates\n",
    "            if not self._is_valid_candidate(candidate):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            date = candidate.get('candidate_date', 'Unknown')\n",
    "            if date == 'Unknown':\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            if date not in candidates_by_date:\n",
    "                candidates_by_date[date] = []\n",
    "            candidates_by_date[date].append(candidate)\n",
    "        \n",
    "        logger.info(f\"Grouped candidates by date: {len(candidates_by_date)} dates, skipped {skipped} invalid candidates\")\n",
    "        return candidates_by_date\n",
    "\n",
    "def generate_embeddings_fixed(test_dataset, embedder_name, q_or_c, device='cuda'):\n",
    "    \"\"\"FIXED: Main function with better error handling and validation\"\"\"\n",
    "    logger.info(f\"Starting {q_or_c} embedding generation for {embedder_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize components\n",
    "        datastore = DatastoreFixed(test_dataset, 'test')\n",
    "        embedder = EmbedderFixed(embedder_name, device)\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = f'/root/nfs/AJ FinRag/Embeddings/embeddings/test/{embedder_name}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logger.info(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        if q_or_c == 'query':\n",
    "            logger.info(\"Generating query embeddings...\")\n",
    "            \n",
    "            # Get grouped queries\n",
    "            queries_by_date = datastore.group_no_freeze_query_str_by_date()\n",
    "            \n",
    "            if not queries_by_date:\n",
    "                logger.error(\"No valid queries found!\")\n",
    "                return 1\n",
    "            \n",
    "            # Generate embeddings\n",
    "            query_embedding_list = []\n",
    "            total_queries = 0\n",
    "            \n",
    "            for date, queries_on_date in tqdm(queries_by_date.items(), desc=\"Processing query dates\"):\n",
    "                try:\n",
    "                    query_embeddings_on_date = embedder.embed_queries_in_parallel(queries_on_date)\n",
    "                    query_embedding_list.append({date: query_embeddings_on_date})\n",
    "                    total_queries += len(queries_on_date)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing queries for date {date}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save results\n",
    "            output_file = os.path.join(output_dir, f'q_{test_dataset}_{embedder_name}_embeddings.pkl')\n",
    "            \n",
    "            try:\n",
    "                with open(output_file, 'wb') as f:\n",
    "                    pickle.dump(query_embedding_list, f)\n",
    "                logger.info(f\" Query embeddings saved: {output_file}\")\n",
    "                logger.info(f\"Total queries embedded: {total_queries}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving query embeddings: {e}\")\n",
    "                return 1\n",
    "        \n",
    "        elif q_or_c == 'candidate':\n",
    "            logger.info(\"Generating candidate embeddings...\")\n",
    "            \n",
    "            # Get grouped candidates\n",
    "            candidates_by_date = datastore.group_candidate_str_by_date()\n",
    "            \n",
    "            if not candidates_by_date:\n",
    "                logger.error(\"No valid candidates found!\")\n",
    "                return 1\n",
    "            \n",
    "            # Generate embeddings in groups\n",
    "            candidate_embedding_list = []\n",
    "            total_candidates = 0\n",
    "            group = 1\n",
    "            dates_processed = 0\n",
    "            \n",
    "            for date, candidates_on_date in tqdm(candidates_by_date.items(), desc=\"Processing candidate dates\"):\n",
    "                try:\n",
    "                    candidate_embeddings_on_date = embedder.embed_candidates_in_parallel(candidates_on_date)\n",
    "                    candidate_embedding_list.append({date: candidate_embeddings_on_date})\n",
    "                    total_candidates += len(candidates_on_date)\n",
    "                    dates_processed += 1\n",
    "                    \n",
    "                    # Save every 10 dates\n",
    "                    if dates_processed % 10 == 0:\n",
    "                        output_file = os.path.join(output_dir, f'c_{test_dataset}_{embedder_name}_embeddings_{group}.pkl')\n",
    "                        \n",
    "                        try:\n",
    "                            with open(output_file, 'wb') as f:\n",
    "                                pickle.dump(candidate_embedding_list, f)\n",
    "                            logger.info(f\" Saved candidate group {group}: {output_file}\")\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Error saving candidate group {group}: {e}\")\n",
    "                        \n",
    "                        candidate_embedding_list = []\n",
    "                        group += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing candidates for date {date}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save final group\n",
    "            if candidate_embedding_list:\n",
    "                output_file = os.path.join(output_dir, f'c_{test_dataset}_{embedder_name}_embeddings_{group}.pkl')\n",
    "                \n",
    "                try:\n",
    "                    with open(output_file, 'wb') as f:\n",
    "                        pickle.dump(candidate_embedding_list, f)\n",
    "                    logger.info(f\" Saved final candidate group {group}: {output_file}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error saving final candidate group: {e}\")\n",
    "            \n",
    "            logger.info(f\"Total candidates embedded: {total_candidates}\")\n",
    "            logger.info(f\"Total groups saved: {group}\")\n",
    "        \n",
    "        else:\n",
    "            logger.error(f\"Invalid q_or_c parameter: {q_or_c}\")\n",
    "            return 1\n",
    "        \n",
    "        logger.info(\" Embedding generation completed successfully!\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in embedding generation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1\n",
    "\n",
    "def test_embeddings_quality():\n",
    "    \"\"\"Test the quality of generated embeddings\"\"\"\n",
    "    logger.info(\"ðŸ§ª Testing embedding quality...\")\n",
    "    \n",
    "    try:\n",
    "        embedder = EmbedderFixed('FinQuest')\n",
    "        \n",
    "        # Test with diverse sequences\n",
    "        test_sequences = [\n",
    "            \"{'query_stock': 'AAPL', 'query_date': '2024-01-15', 'recent_date_list': ['2024-01-01'], 'adjusted_close_list': [150.0]}\",\n",
    "            \"{'query_stock': 'TSLA', 'query_date': '2024-01-15', 'recent_date_list': ['2024-01-01'], 'adjusted_close_list': [200.0]}\",\n",
    "            \"{'query_stock': 'XOM', 'query_date': '2024-01-15', 'recent_date_list': ['2024-01-01'], 'adjusted_close_list': [80.0]}\"\n",
    "        ]\n",
    "        \n",
    "        embeddings = embedder.embed_sequences_batch(test_sequences)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                sim = np.dot(embeddings[i], embeddings[j])\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        logger.info(f\"Embedding test results:\")\n",
    "        logger.info(f\"  Min similarity: {min(similarities):.6f}\")\n",
    "        logger.info(f\"  Max similarity: {max(similarities):.6f}\")\n",
    "        logger.info(f\"  Mean similarity: {np.mean(similarities):.6f}\")\n",
    "        \n",
    "        if min(similarities) > 0.95:\n",
    "            logger.error(\" EMBEDDING COLLAPSE DETECTED in generated embeddings!\")\n",
    "            return False\n",
    "        else:\n",
    "            logger.info(\" Embeddings show reasonable diversity\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error testing embeddings: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_both_embeddings_fixed(test_dataset='your_dataset', embedder_name='FinQuest', device='cuda'):\n",
    "    \"\"\"Generate both query and candidate embeddings with the fixed version\"\"\"\n",
    "    logger.info(\" Starting FIXED FinQuest embedding generation (both query and candidate)\")\n",
    "    \n",
    "    # Test embedding quality first\n",
    "    logger.info(\"Step 0: Testing embedding quality...\")\n",
    "    if not test_embeddings_quality():\n",
    "        logger.error(\" Embedding quality test failed - your model has collapsed embeddings!\")\n",
    "        logger.error(\"   You need to retrain your FinQuest model before proceeding.\")\n",
    "        return 1\n",
    "    \n",
    "    # Generate query embeddings\n",
    "    logger.info(\"Step 1: Generating query embeddings...\")\n",
    "    result = generate_embeddings_fixed(test_dataset, embedder_name, 'query', device)\n",
    "    \n",
    "    if result != 0:\n",
    "        logger.error(\" Query embedding generation failed!\")\n",
    "        return 1\n",
    "    \n",
    "    # Generate candidate embeddings\n",
    "    logger.info(\"Step 2: Generating candidate embeddings...\")\n",
    "    result = generate_embeddings_fixed(test_dataset, embedder_name, 'candidate', device)\n",
    "    \n",
    "    if result != 0:\n",
    "        logger.error(\" Candidate embedding generation failed!\")\n",
    "        return 1\n",
    "    \n",
    "    logger.info(\" All embedding generation completed successfully!\")\n",
    "    \n",
    "    # Verify output files\n",
    "    output_dir = f'/root/nfs/AJ FinRag/Embeddings/embeddings/test/{embedder_name}'\n",
    "    query_file = os.path.join(output_dir, f'q_{test_dataset}_{embedder_name}_embeddings.pkl')\n",
    "    candidate_file = os.path.join(output_dir, f'c_{test_dataset}_{embedder_name}_embeddings_1.pkl')\n",
    "    \n",
    "    if os.path.exists(query_file) and os.path.exists(candidate_file):\n",
    "        logger.info(\" All embedding files created successfully!\")\n",
    "        logger.info(f\" Query embeddings: {query_file}\")\n",
    "        logger.info(f\" Candidate embeddings: {output_dir}\")\n",
    "        \n",
    "        # Quick validation of generated files\n",
    "        try:\n",
    "            with open(query_file, 'rb') as f:\n",
    "                query_data = pickle.load(f)\n",
    "            with open(candidate_file, 'rb') as f:\n",
    "                candidate_data = pickle.load(f)\n",
    "                \n",
    "            logger.info(f\" Query file contains {len(query_data)} date groups\")\n",
    "            logger.info(f\" Candidate file contains {len(candidate_data)} date groups\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Error validating generated files: {e}\")\n",
    "        \n",
    "    else:\n",
    "        logger.error(\" Some embedding files missing!\")\n",
    "        if not os.path.exists(query_file):\n",
    "            logger.error(f\"   Missing: {query_file}\")\n",
    "        if not os.path.exists(candidate_file):\n",
    "            logger.error(f\"   Missing: {candidate_file}\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set multiprocessing method\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "    \n",
    "    # Configuration\n",
    "    test_dataset = 'your_dataset'\n",
    "    embedder_name = 'FinQuest'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    logger.info(\" FIXED FINQUEST EMBEDDING GENERATION\")\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(f\"Dataset: {test_dataset}\")\n",
    "    logger.info(f\"Embedder: {embedder_name}\")\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    logger.info(\"=\"*70)\n",
    "    \n",
    "    # Generate both embeddings\n",
    "    result = generate_both_embeddings_fixed(test_dataset, embedder_name, device)\n",
    "    \n",
    "    if result == 0:\n",
    "        logger.info(\" SUCCESS: All embeddings generated successfully!\")\n",
    "        logger.info(\"   You can now run your similarity search and experiments.\")\n",
    "    else:\n",
    "        logger.error(\" FAILED: Embedding generation failed!\")\n",
    "        logger.error(\"   Please check the error messages above and fix the issues.\")\n",
    "        \n",
    "        # Provide troubleshooting guidance\n",
    "        logger.info(\"\\n TROUBLESHOOTING GUIDE:\")\n",
    "        logger.info(\"1. If embedding quality test failed:\")\n",
    "        logger.info(\"   â†’ Your FinQuest model has collapsed embeddings\")\n",
    "        logger.info(\"   â†’ You need to retrain the model with better hyperparameters\")\n",
    "        logger.info(\"2. If file loading failed:\")\n",
    "        logger.info(\"   â†’ Check if your data files exist at the specified paths\")\n",
    "        logger.info(\"   â†’ Verify JSON format in your data files\")\n",
    "        logger.info(\"3. If model loading failed:\")\n",
    "        logger.info(\"   â†’ Check if your model checkpoint exists\")\n",
    "        logger.info(\"   â†’ Verify the model was trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb9027-34af-4c5c-a3a9-e50733ad3a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6471422-8b27-4c4c-833e-6d3ab3e10b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
